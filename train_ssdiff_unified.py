"""
SSDiffç»Ÿä¸€è®­ç»ƒè„šæœ¬
æ”¯æŒä¸‰ç§è®­ç»ƒæ¨¡å¼ï¼š
1. L1æ¨¡å¼ï¼šç›´æ¥ç›‘ç£å­¦ä¹ 
2. VSDæ¨¡å¼ï¼šçŸ¥è¯†è’¸é¦
3. Mixedæ¨¡å¼ï¼šL1 + VSDæ··åˆè®­ç»ƒ
"""
import os
import sys
import argparse
import datetime
import socket
import numpy as np
import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
from pathlib import Path
import wandb

sys.path.append('/data2/user/zelilin/ARConv_SSDiff/SSDiff_main')

from accelerate import Accelerator
from accelerate.utils import set_seed, ProjectConfiguration
from accelerate import DistributedDataParallelKwargs
from diffusers.optimization import get_scheduler

from ssdiff_unified import SSDiff_Unified_gen, switch_conv2d_to_arconv_with_interpolation
from configs.option_DPM_pansharpening import parser_args as ssdiff_parser_args
from pancollection.common.psdata import PansharpeningSession as DataSession
from train_utils_progressive import (
    get_training_stage, 
    print_stage_transition,
)


def parse_args(input_args=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Train SSDiff Unified Model')
    
    # åŸºç¡€è·¯å¾„
    parser.add_argument("--pretrained_ssdiff_path", type=str, required=True)
    parser.add_argument("--lora_checkpoint_path", type=str, default=None)
    parser.add_argument("--data_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, default="experiments/ssdiff_unified")
    
    # è®­ç»ƒæ¨¡å¼
    parser.add_argument("--loss_mode", type=str, default="mixed",
                       choices=["l1", "vsd", "mixed"],
                       help="è®­ç»ƒæ¨¡å¼: l1(ç›´æ¥ç›‘ç£), vsd(è’¸é¦), mixed(æ··åˆ)")
    parser.add_argument("--lambda_l1", type=float, default=1.0,
                       help="L1æŸå¤±æƒé‡ï¼ˆmixedæ¨¡å¼ï¼‰")
    parser.add_argument("--lambda_vsd", type=float, default=1.0,
                       help="VSDæŸå¤±æƒé‡ï¼ˆmixedæ¨¡å¼ï¼‰")
    parser.add_argument("--lambda_distribution", type=float, default=1.0,
                       help="åˆ†å¸ƒåŒ¹é…æŸå¤±æƒé‡ï¼ˆè’¸é¦ä¸€è‡´æ€§ï¼‰")
    parser.add_argument("--lambda_diff", type=float, default=1.0,
                       help="æ‰©æ•£ä¸€è‡´æ€§æŸå¤±æƒé‡ï¼ˆå•æ­¥è’¸é¦ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--seed", type=int, default=123)
    parser.add_argument("--train_batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--max_train_steps", type=int, default=50000)
    parser.add_argument("--num_training_epochs", type=int, default=10000)
    parser.add_argument("--checkpointing_steps", type=int, default=200)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    parser.add_argument("--lr_scheduler", type=str, default="constant")
    parser.add_argument("--lr_warmup_steps", type=int, default=500)
    
    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument("--adam_beta1", type=float, default=0.9)
    parser.add_argument("--adam_beta2", type=float, default=0.999)
    parser.add_argument("--adam_weight_decay", type=float, default=1e-2)
    parser.add_argument("--adam_epsilon", type=float, default=1e-08)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    
    # LoRAé…ç½®
    parser.add_argument("--lora_rank", type=int, default=4)
    parser.add_argument("--train_lora", type=lambda x: str(x).lower() == 'true', default=True)
    
    # VAEé…ç½®
    parser.add_argument("--use_vae", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--vae_latent_dim", type=int, default=256)
    parser.add_argument("--train_vae", type=lambda x: str(x).lower() == 'true', default=True)
    parser.add_argument("--vae_lr", type=float, default=1e-5)
    parser.add_argument("--use_kl_loss", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--lambda_kl", type=float, default=0.001)
    parser.add_argument("--use_perceptual_loss", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--lambda_perceptual", type=float, default=0.1)
    
    # ControlNeté…ç½®
    parser.add_argument("--use_controlnet", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--ms_channels", type=int, default=8)
    parser.add_argument("--train_controlnet", type=lambda x: str(x).lower() == 'true', default=True)
    parser.add_argument("--controlnet_lr", type=float, default=5e-5)
    
    # CLIPé…ç½®
    parser.add_argument("--use_clip", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--clip_model_path", type=str, default=None)
    parser.add_argument("--prompt", type=str, default="A high resolution satellite image")
    parser.add_argument("--train_clip", type=lambda x: str(x).lower() == 'true', default=False)
    
    # RAMé…ç½®
    parser.add_argument("--use_ram", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--ram_model_path", type=str, default=None)
    
    # ARConvé…ç½®
    parser.add_argument("--use_arconv", type=str, default="True")
    parser.add_argument("--arconv_hw_range", type=str, default="[1,5]")
    parser.add_argument("--arconv_warmup_steps", type=int, default=1000)
    parser.add_argument("--arconv_activate_steps", type=int, default=3000)
    parser.add_argument("--arconv_fixstep", type=int, default=4000)
    
    # æ‰©æ•£æ¨¡å‹å‚æ•°
    parser.add_argument("--predict_xstart", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--use_ddim", type=lambda x: str(x).lower() == 'true', default=False)
    parser.add_argument("--timestep_respacing", type=str, default="1000")
    parser.add_argument("--diffusion_steps", type=int, default=1000)
    parser.add_argument("--noise_schedule", type=str, default="linear")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--mixed_precision", type=str, default="fp16")
    parser.add_argument("--logging_dir", type=str, default="logs")
    parser.add_argument("--report_to", type=str, default="tensorboard")
    parser.add_argument("--tracker_project_name", type=str, default="train_ssdiff_unified")
    parser.add_argument("--dataloader_num_workers", type=int, default=0)
    parser.add_argument("--set_grads_to_none", action="store_true")
    
    # SSDiffç‰¹å®šå‚æ•°
    parser.add_argument("--ms_dim", type=int, default=8)
    parser.add_argument("--pan_dim", type=int, default=1)
    parser.add_argument("--image_size", type=int, default=64)
    
    # æ¢å¤è®­ç»ƒ
    parser.add_argument("--resume_from_checkpoint", type=str, default=None)
    parser.add_argument("--resume_step", type=int, default=0)
    
    if input_args is not None:
        args = parser.parse_args(input_args)
    else:
        args = parser.parse_args()
    
    return args


def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # åˆå§‹åŒ–Accelerator
    logging_dir = Path(args.output_dir, args.logging_dir)
    accelerator_project_config = ProjectConfiguration(
        project_dir=args.output_dir, 
        logging_dir=logging_dir
    )
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        kwargs_handlers=[ddp_kwargs],
    )
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_main_process:
        os.makedirs(os.path.join(args.output_dir, "checkpoints"), exist_ok=True)
    
    # è·å–SSDiffé…ç½®
    ssdiff_args = ssdiff_parser_args()
    for key, value in vars(args).items():
        setattr(ssdiff_args, key, value)
    
    if hasattr(args, 'train_batch_size'):
        ssdiff_args.samples_per_gpu = args.train_batch_size
    
    # å‚æ•°ç±»å‹è½¬æ¢
    if hasattr(ssdiff_args, 'use_arconv'):
        ssdiff_args.use_arconv = ssdiff_args.use_arconv.lower() == 'true'
    if hasattr(ssdiff_args, 'arconv_hw_range'):
        if isinstance(ssdiff_args.arconv_hw_range, str):
            ssdiff_args.arconv_hw_range = eval(ssdiff_args.arconv_hw_range)
    if hasattr(args, 'use_arconv'):
        args.use_arconv = args.use_arconv.lower() == 'true'
    
    # åˆ›å»ºæ¨¡å‹
    print("\n" + "="*70)
    print(f"åˆ›å»ºSSDiffç»Ÿä¸€è®­ç»ƒæ¨¡å‹ - æ¨¡å¼: {args.loss_mode.upper()}")
    print("="*70)
    print(f"  VAEç¼–ç å™¨: {'å¯ç”¨' if ssdiff_args.use_vae else 'ç¦ç”¨'}")
    print(f"  ControlNet: {'å¯ç”¨' if ssdiff_args.use_controlnet else 'ç¦ç”¨'}")
    print(f"  CLIPæ–‡æœ¬ç¼–ç å™¨: {'å¯ç”¨' if ssdiff_args.use_clip else 'ç¦ç”¨'}")
    print(f"  RAM Captionç”Ÿæˆå™¨: {'å¯ç”¨' if ssdiff_args.use_ram else 'ç¦ç”¨'}")
    print(f"  ARConv: {'å¯ç”¨' if ssdiff_args.use_arconv else 'ç¦ç”¨'}")
    if args.loss_mode == "mixed":
        print(f"  æŸå¤±æƒé‡: L1={args.lambda_l1}, VSD={args.lambda_vsd}")
    print("="*70)
    
    # ARConvæ¸è¿›å¼ç­–ç•¥
    original_use_arconv = ssdiff_args.use_arconv
    if original_use_arconv:
        ssdiff_args.use_arconv = False
    
    model_gen = SSDiff_Unified_gen(ssdiff_args)
    model_gen.set_train(
        enable_arconv=False,
        train_vae=args.train_vae,
        train_lora=args.train_lora,
        train_controlnet=args.train_controlnet,
        train_clip=args.train_clip if hasattr(args, 'train_clip') else False
    )
    
    ssdiff_args.use_arconv = original_use_arconv
    print("æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # æ”¶é›†å¯è®­ç»ƒå‚æ•°
    lora_params = []
    vae_params = []
    controlnet_params = []
    arconv_params = []
    gate_params = []
    proj_params = []
    
    for n, p in model_gen.unet.named_parameters():
        if p.requires_grad:
            if "lora_down" in n or "lora_up" in n:
                lora_params.append(p)
            elif any(key in n.lower() for key in ['arconv', 'adaptive', 'offset', 'modulation', 'kernel_gen']):
                arconv_params.append(p)
            elif 'gate' in n and 'attn_gate' not in n:
                gate_params.append(p)
            elif 'text_proj' in n:
                proj_params.append(p)
    
    if model_gen.vae_encoder is not None:
        for p in model_gen.vae_encoder.parameters():
            if p.requires_grad:
                vae_params.append(p)
    
    if model_gen.controlnet_full is not None:
        for p in model_gen.controlnet_full.parameters():
            if p.requires_grad:
                controlnet_params.append(p)
    
    print(f"\nå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡:")
    print(f"  LoRA: {len(lora_params)}")
    print(f"  VAE: {len(vae_params)}")
    print(f"  ControlNet: {len(controlnet_params)}")
    print(f"  ARConv: {len(arconv_params)}")
    print(f"  Gate: {len(gate_params)}")
    print(f"  Projection: {len(proj_params)}")
    
    # åˆ›å»ºå‚æ•°ç»„
    param_groups = []
    if lora_params:
        param_groups.append({'params': lora_params, 'lr': args.learning_rate, 'name': 'lora'})
    if vae_params:
        vae_lr = getattr(args, 'vae_lr', args.learning_rate * 0.2)
        param_groups.append({'params': vae_params, 'lr': vae_lr, 'name': 'vae'})
    if controlnet_params:
        controlnet_lr = getattr(args, 'controlnet_lr', args.learning_rate)
        param_groups.append({'params': controlnet_params, 'lr': controlnet_lr, 'name': 'controlnet'})
    if arconv_params:
        param_groups.append({'params': arconv_params, 'lr': args.learning_rate * 0.1, 'name': 'arconv'})
    if gate_params:
        gate_lr = args.learning_rate * 2.0
        param_groups.append({'params': gate_params, 'lr': gate_lr, 'name': 'gate'})
    if proj_params:
        param_groups.append({'params': proj_params, 'lr': args.learning_rate, 'name': 'projection'})
    
    optimizer = torch.optim.AdamW(
        param_groups,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )
    
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        num_training_steps=args.max_train_steps * accelerator.num_processes,
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    session = DataSession(ssdiff_args)
    train_dataloader, _, _ = session.get_dataloader(ssdiff_args.dataset['train'], False, None)
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_dataloader)}")
    
    # Prepare
    model_gen, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model_gen, optimizer, train_dataloader, lr_scheduler
    )
    
    # åˆå§‹åŒ–trackers
    if accelerator.is_main_process:
        tracker_config = dict(vars(args))
        accelerator.init_trackers(args.tracker_project_name, config=tracker_config)
    
    # æ¢å¤checkpoint
    global_step = 0
    if args.resume_from_checkpoint is not None:
        print(f"ä»checkpointæ¢å¤: {args.resume_from_checkpoint}")
        global_step = args.resume_step
    
    # è®­ç»ƒå¾ªç¯
    arconv_switched = False
    current_stage = "warmup"
    
    progress_bar = tqdm(
        range(global_step, args.max_train_steps),
        initial=global_step,
        desc="Steps",
        disable=not accelerator.is_local_main_process,
    )
    
    for epoch in range(args.num_training_epochs):
        for step, batch in enumerate(train_dataloader):
            # æ›´æ–°è®­ç»ƒé˜¶æ®µ
            prev_stage = current_stage
            current_stage = get_training_stage(global_step, args)
            
            if prev_stage != current_stage:
                print_stage_transition(global_step, prev_stage, current_stage, args)
            
            # ARConvåˆ‡æ¢
            if args.use_arconv and not arconv_switched and global_step >= args.arconv_warmup_steps:
                print(f"\nåˆ‡æ¢åˆ°ARConvï¼ˆæ­¥æ•°: {global_step}ï¼‰")
                unwrapped_model_gen = accelerator.unwrap_model(model_gen)
                switch_conv2d_to_arconv_with_interpolation(unwrapped_model_gen.unet, args)
                arconv_switched = True
                model_gen = accelerator.prepare(unwrapped_model_gen)
                
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆç•¥ï¼Œé€»è¾‘åŒ train_ssdiff_vae.pyï¼‰
                print("ARConvåˆ‡æ¢å®Œæˆ")
            
            # è·å–åŸå§‹æ¨¡å‹ï¼ˆå¤„ç†DDPåŒ…è£…ï¼‰
            unwrapped_model = accelerator.unwrap_model(model_gen)
            
            # æ›´æ–°å½“å‰æ­¥æ•°
            unwrapped_model.set_step(global_step)
            
            # è®¾ç½®è®­ç»ƒæ¨¡å¼
            enable_arconv = current_stage != "warmup"
            unwrapped_model.set_train(
                enable_arconv=enable_arconv,
                train_vae=args.train_vae,
                train_lora=args.train_lora,
                train_controlnet=args.train_controlnet,
                train_clip=args.train_clip if hasattr(args, 'train_clip') else False
            )
            
            with accelerator.accumulate(model_gen):
                # è·å–æ•°æ®
                pan = batch['pan']
                lms = batch['lms']
                ms = batch['ms']
                gt = batch['gt']
                
                if gt.shape[1] > gt.shape[2]:
                    import einops
                    gt = einops.rearrange(gt, 'b h w c -> b c h w')
                
                # ğŸ“Š æ•°æ®èŒƒå›´æ£€æŸ¥ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªbatchæ‰“å°ï¼‰
                if global_step == 0 and accelerator.is_main_process:
                    print("\n" + "="*70)
                    print("ğŸ“Š æ•°æ®èŒƒå›´æ£€æŸ¥")
                    print("="*70)
                    print(f"PAN   - min: {pan.min():.4f}, max: {pan.max():.4f}, mean: {pan.mean():.4f}")
                    print(f"LMS   - min: {lms.min():.4f}, max: {lms.max():.4f}, mean: {lms.mean():.4f}")
                    print(f"MS    - min: {ms.min():.4f}, max: {ms.max():.4f}, mean: {ms.mean():.4f}")
                    print(f"GT    - min: {gt.min():.4f}, max: {gt.max():.4f}, mean: {gt.mean():.4f}")
                    residual_check = gt - lms
                    print(f"GT-LMS - min: {residual_check.min():.4f}, max: {residual_check.max():.4f}, mean: {residual_check.mean():.4f}, std: {residual_check.std():.4f}")
                    print("="*70)
                    if gt.max() > 1.5 or gt.min() < -0.5:
                        print("âš ï¸  è­¦å‘Šï¼šæ•°æ®èŒƒå›´å¼‚å¸¸ï¼åº”è¯¥åœ¨[0,1]èŒƒå›´å†…")
                        print("âš ï¸  è¿™ä¼šå¯¼è‡´losså¼‚å¸¸å¤§ï¼")
                    else:
                        print("âœ… æ•°æ®èŒƒå›´æ­£å¸¸")
                    print("="*70 + "\n")
                
                # è®¡ç®—æŸå¤±
                total_loss = 0.0
                loss_dict = {}
                
                if args.loss_mode == "l1":
                    # ä»…L1æŸå¤±
                    loss, _, loss_dict = unwrapped_model.l1_loss(lms, pan, ms, gt)
                    total_loss = loss
                
                elif args.loss_mode == "vsd":
                    # ä»…VSDæŸå¤±
                    loss, _, loss_dict = unwrapped_model.vsd_loss(lms, pan, ms, gt)
                    total_loss = loss
                
                elif args.loss_mode == "mixed":
                    # æ··åˆæŸå¤±
                    loss_l1, _, loss_dict_l1 = unwrapped_model.l1_loss(lms, pan, ms, gt)
                    loss_vsd, _, loss_dict_vsd = unwrapped_model.vsd_loss(lms, pan, ms, gt)
                    
                    total_loss = args.lambda_l1 * loss_l1 + args.lambda_vsd * loss_vsd
                    
                    loss_dict = {
                        'l1_loss': loss_dict_l1.get('l1_loss', 0.0),
                        'vsd_loss': loss_dict_vsd.get('vsd_loss', 0.0),
                    }
                    if 'kl_loss' in loss_dict_l1:
                        loss_dict['kl_loss'] = loss_dict_l1['kl_loss']
                    if 'perceptual_loss' in loss_dict_l1:
                        loss_dict['perceptual_loss'] = loss_dict_l1['perceptual_loss']

                # è’¸é¦ç›¸å…³çš„é™„åŠ æŸå¤±
                use_distribution_loss = args.loss_mode in ("vsd", "mixed") and args.lambda_distribution > 0
                if use_distribution_loss:
                    loss_dist, _, loss_dict_dist = unwrapped_model.distribution_matching_loss(lms, pan, ms, gt)
                    total_loss = total_loss + args.lambda_distribution * loss_dist
                    loss_dict['distribution_matching_loss'] = loss_dict_dist.get(
                        'distribution_matching_loss', loss_dist.detach().item()
                    )
                    loss_dict['distribution_matching_loss_weighted'] = (
                        args.lambda_distribution * loss_dist
                    ).detach().item()

                use_diffusion_loss = args.loss_mode in ("vsd", "mixed") and args.lambda_diff > 0
                if use_diffusion_loss:
                    loss_diff, _, loss_dict_diff = unwrapped_model.diff_loss(lms, pan, ms, gt)
                    total_loss = total_loss + args.lambda_diff * loss_diff
                    loss_dict['diff_loss'] = loss_dict_diff.get(
                        'diff_loss', loss_diff.detach().item()
                    )
                    loss_dict['diff_loss_weighted'] = (
                        args.lambda_diff * loss_diff
                    ).detach().item()
                
                loss_dict['total_loss'] = total_loss.detach().item()
                
                # åå‘ä¼ æ’­
                accelerator.backward(total_loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model_gen.parameters(), args.max_grad_norm)
                
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=args.set_grads_to_none)
            
            # æ›´æ–°è¿›åº¦
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                
                if accelerator.is_main_process:
                    # è®°å½•æ—¥å¿—
                    progress_logs = {"loss": total_loss.detach().item()}
                    progress_bar.set_postfix(**progress_logs)
                    
                    # ä¿å­˜checkpoint
                    if global_step % args.checkpointing_steps == 0:
                        outf = os.path.join(
                            args.output_dir, 
                            "checkpoints", 
                            f"model_{global_step}.pkl"
                        )
                        accelerator.unwrap_model(model_gen).save_model(outf)
                        print(f"Checkpointä¿å­˜åœ¨æ­¥æ•° {global_step}")
                    
                    # WandBæ—¥å¿—
                    if global_step % 10 == 0:
                        wandb_logs = {
                            "train/total_loss": total_loss.item(),
                            "train/step": global_step,
                        }
                        for key, value in loss_dict.items():
                            wandb_logs[f"train/{key}"] = value
                        
                        wandb.log(wandb_logs, step=global_step)
                    
                    accelerator.log(loss_dict, step=global_step)
                
                if global_step >= args.max_train_steps:
                    print(f"\nè®­ç»ƒå®Œæˆ! è¾¾åˆ°æœ€å¤§æ­¥æ•°: {args.max_train_steps}")
                    return
        
        if global_step >= args.max_train_steps:
            wandb.finish()
            break
    
    print("è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    args = parse_args()
    
    # åˆå§‹åŒ–wandb
    run_dir = os.path.join("runs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    os.makedirs(run_dir, exist_ok=True)
    
    wandb.init(
        config=vars(args),
        project="ssdiff-unified-training",
        entity="tszharry-xi-an-jiaotong-university-",
        notes=socket.gethostname(),
        name=f"unified_{args.loss_mode}_{args.lora_rank}",
        dir=run_dir,
        job_type="training",
        mode="offline",
        reinit=True
    )
    
    main(args)
