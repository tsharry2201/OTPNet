"""
SSDiff ControlNetäºŒæ¬¡è®­ç»ƒæ¨¡å‹
åŸºäºä¸€æ­¥è’¸é¦åçš„é¢„è®­ç»ƒæ¨¡å‹å’ŒLoRAå±‚ï¼Œä½¿ç”¨å®Œæ•´ControlNetè¿›è¡ŒäºŒæ¬¡è®­ç»ƒ
"""
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
sys.path.append('/home/zelilin/data/pansharpening/SSDiff_main')
from utils.script_util import create_model_and_diffusion, args_to_dict, model_and_diffusion_defaults
from model.ARConv import ARConv
from model.fusformer import Fusformer
from model.SSNet import ResBlock, Down, Up
from model.nn import (
    SiLU,
    conv_nd,
    linear,
    zero_module,
    normalization,
    timestep_embedding,
)
from transformers import AutoTokenizer, CLIPTextModel
import torchvision.transforms as T
from torchvision import models
from torchvision.models.feature_extraction import create_feature_extractor
from PIL import Image
from ram.models.ram_lora import ram
from ram import inference_ram as inference

class RAMCaptionGenerator:
    
    def __init__(self, model_path, device='cuda'):
        """
        åˆå§‹åŒ–RAMæ¨¡å‹
        Args:
            model_path: é¢„è®­ç»ƒRAMæƒé‡è·¯å¾„ï¼Œä¾‹å¦‚ 'ram_swin_large_14m.pth'
            device: è¿è¡Œè®¾å¤‡
        """
        self.device = device
        self.model_path = model_path

        # åŠ è½½æ¨¡å‹
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½ RAM æ¨¡å‹æƒé‡: {model_path}")
        self.model = ram(pretrained=model_path, image_size=384, vit='swin_l').to(device)
        self.model.eval()

        # å›¾åƒé¢„å¤„ç†
        self.transform = T.Compose([
            T.Resize((384, 384)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])
        print("âœ“ RAM Caption Generator å·²åˆå§‹åŒ–ï¼ˆå®Œæ•´ç‰ˆï¼‰")
        print("  - æ¨¡å‹ç»“æ„: Swin-Large")
        print("  - æƒé‡æ–‡ä»¶:", model_path)
        print("  - åŠŸèƒ½: è‡ªåŠ¨ä»PANå›¾åƒç”Ÿæˆåœºæ™¯caption")

    def preprocess_image(self, image_tensor):
        """
        å°†è¾“å…¥çš„PANå›¾åƒå¼ é‡è½¬ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼
        Args:
            image_tensor: [1, H, W] æˆ– [B, 1, H, W]
        Returns:
            PIL.Image æ ¼å¼
        """
        if len(image_tensor.shape) == 4:
            image_tensor = image_tensor[0]
        if image_tensor.shape[0] == 1:
            zeros = torch.zeros_like(image_tensor)
            img = image_tensor.repeat(3, 1, 1)
            #print("zero")
        elif image_tensor.shape[0] > 3:
            img = image_tensor[[1,2,4],:,:]  # å¤šé€šé“å–å¹³å‡

        img = img.clamp(0, 1)
        img_pil = T.ToPILImage()(img.cpu())
        return img_pil

    @torch.no_grad()
    def generate_caption(self, image_tensor):
        """
        ä»PANå›¾åƒç”Ÿæˆè¯­ä¹‰caption
        Args:
            image_tensor: [B, 1, H, W] æˆ– [1, H, W]
        Returns:
            caption: ç”Ÿæˆçš„æ–‡æœ¬æè¿°å­—ç¬¦ä¸²
        """
        img_pil = self.preprocess_image(image_tensor)
        img_input = self.transform(img_pil).unsqueeze(0).to(self.device)

        # æ¨¡å‹å‰å‘æ¨ç†ï¼Œå¾—åˆ°æ ‡ç­¾
        tags = self.model.generate_tag(img_input)
        if isinstance(tags, list):
        # å¦‚æœæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå°±å±•å¼€æˆä¸€ç»´
            flat_tags = []
            for t in tags:
                if isinstance(t, list):
                    flat_tags.extend(t)
                elif isinstance(t, str):
                    flat_tags.append(t)
            tags = flat_tags
        elif isinstance(tags, str):
            tags = [tags]
        else:
            tags = [str(tags)]
        caption = ", ".join(tags)
        return caption

def _extract_effective_weight(conv_layer):
    """æå–Convå±‚çš„æœ‰æ•ˆæƒé‡"""
    if hasattr(conv_layer, 'base_layer') and hasattr(conv_layer, 'lora_down'):
        base_weight = conv_layer.base_layer.weight.data.clone()
        lora_down_weight = conv_layer.lora_down.weight.data
        lora_up_weight = conv_layer.lora_up.weight.data
        scaling = conv_layer.scaling
        
        rank, inc, kh, kw = lora_down_weight.shape
        outc, rank2, _, _ = lora_up_weight.shape
        
        lora_down_flat = lora_down_weight.reshape(rank, inc * kh * kw)
        lora_up_flat = lora_up_weight.squeeze(-1).squeeze(-1)
        lora_delta_flat = torch.matmul(lora_up_flat, lora_down_flat) * scaling
        lora_delta = lora_delta_flat.reshape(outc, inc, kh, kw)
        
        fused_weight = base_weight + lora_delta
        return fused_weight
    else:
        return conv_layer.weight.data.clone()


def switch_conv2d_to_arconv_with_interpolation(model, args):
    """åŠ¨æ€åˆ‡æ¢ï¼šå°†æ¨¡å‹ä¸­çš„Conv2dæ›¿æ¢ä¸ºARConv"""
    def replace_conv_in_module(module, module_name=""):
        from model.SSNet import ResBlock
        
        if isinstance(module, ResBlock):
            if isinstance(module.conv0, ARConv):
                return
            
            conv0_weight = _extract_effective_weight(module.conv0)
            conv1_weight = _extract_effective_weight(module.conv1)
            
            in_channels = conv0_weight.shape[1]
            hidden_channels = conv0_weight.shape[0]
            out_channels = conv1_weight.shape[0]
            
            new_conv0 = ARConv(in_channels, hidden_channels, 3, 1, 1)
            new_conv1 = ARConv(hidden_channels, out_channels, 3, 1, 1)
            
            _init_arconv_from_conv2d_interpolation(new_conv0, conv0_weight)
            _init_arconv_from_conv2d_interpolation(new_conv1, conv1_weight)
            
            module.conv0 = new_conv0
            module.conv1 = new_conv1
            module.use_arconv = True
            
            if hasattr(args, 'arconv_hw_range'):
                hw_range = args.arconv_hw_range
                if isinstance(hw_range, str):
                    hw_range = eval(hw_range)
                module.arconv_hw_range = hw_range
            else:
                module.arconv_hw_range = [1, 9]
    
        for name, child in module.named_children():
            replace_conv_in_module(child, f"{module_name}.{name}" if module_name else name)
    
    replace_conv_in_module(model, "unet")


def _init_arconv_from_conv2d_interpolation(arconv_model, conv2d_weight):
    """ARConvæ’ç­‰èµ·æ­¥åˆå§‹åŒ–"""
    kernel_sizes = [(3,3), (3,5), (5,3), (3,7), (7,3), (5,5), (5,7), (7,5), (7,7)]
    
    for i, (h, w) in enumerate(kernel_sizes):
        if h == 3 and w == 3:
            arconv_model.convs[i].weight.data = conv2d_weight.clone()
        else:
            expanded_weight = F.interpolate(
                conv2d_weight, 
                size=(h, w), 
                mode='bilinear', 
                align_corners=True
            )
            arconv_model.convs[i].weight.data = expanded_weight
    
    nn.init.zeros_(arconv_model.m_conv[6].weight)
    if arconv_model.m_conv[6].bias is not None:
        nn.init.constant_(arconv_model.m_conv[6].bias, 3.0)
    
    nn.init.zeros_(arconv_model.b_conv[6].weight)
    if arconv_model.b_conv[6].bias is not None:
        nn.init.zeros_(arconv_model.b_conv[6].bias)
    
    nn.init.zeros_(arconv_model.p_conv[4].weight)
    if arconv_model.p_conv[4].bias is not None:
        nn.init.zeros_(arconv_model.p_conv[4].bias)
    
    nn.init.zeros_(arconv_model.l_conv[4].weight)
    if arconv_model.l_conv[4].bias is not None:
        nn.init.constant_(arconv_model.l_conv[4].bias, 0.0)
    
    nn.init.zeros_(arconv_model.w_conv[4].weight)
    if arconv_model.w_conv[4].bias is not None:
        nn.init.constant_(arconv_model.w_conv[4].bias, 0.0)


def initialize_ssdiff_unet_with_lora(args, pretrained_path=None):
    """åˆå§‹åŒ–SSDiffçš„UNetå¹¶æ·»åŠ LoRAå±‚åˆ°æŒ‡å®šçš„Conv2då’ŒLinearå±‚
    
    Args:
        args: å‚æ•°é…ç½®
        pretrained_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„
        
    argsä¸­çš„lora_targeté€‰é¡¹:
        'all': å¯¹æ‰€æœ‰Conv2då’ŒLinearå±‚åº”ç”¨LoRAï¼ˆé»˜è®¤ï¼‰
        'resblock': ä»…å¯¹ResBlockåº”ç”¨LoRA
        'fusformer': ä»…å¯¹Fusformeråº”ç”¨LoRA
        'resblock+fusformer': å¯¹ResBlockå’ŒFusformeråº”ç”¨LoRA
    """
    student_args_dict = args_to_dict(args, model_and_diffusion_defaults().keys())
    # åªè®¾ç½®åˆæ³•çš„å‚æ•°ï¼Œä½¿ç”¨getattré¿å…AttributeError
    if 'use_arconv' in student_args_dict:
        student_args_dict['use_arconv'] = getattr(args, 'use_arconv', False)
    if 'use_scene_token' in student_args_dict:
        student_args_dict['use_scene_token'] = getattr(args, 'use_scene_token', False)
    print(student_args_dict['use_scene_token'])
    model, _ = create_model_and_diffusion(**student_args_dict)
    
    if pretrained_path is not None and os.path.exists(pretrained_path):
        state_dict = torch.load(pretrained_path, map_location='cpu')
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"Missing keys: {len(missing_keys)}")
    
    model.requires_grad_(False)
    model.train()
    
    # è·å–LoRAç›®æ ‡é…ç½®
    lora_target = getattr(args, 'lora_target', 'all')
    
    # æ‰©å±•çš„LoRAåº”ç”¨ç­–ç•¥ï¼šæ ¹æ®lora_targeté€‰æ‹©åº”ç”¨èŒƒå›´
    lora_target_modules = []
    excluded_patterns = [
        'time_embed',      # æ—¶é—´åµŒå…¥å±‚ï¼ˆä¿æŒå†»ç»“ï¼‰
        'label_emb',       # æ ‡ç­¾åµŒå…¥å±‚
        'scene_embed',     # Scene tokenåµŒå…¥å±‚ï¼ˆç‹¬ç«‹è®­ç»ƒï¼‰
        'cross_attn.to_out.0',  # CrossAttentionçš„è¾“å‡ºå±‚ï¼ˆä½¿ç”¨Linearä½†å·²åŒ…å«åœ¨to_outä¸­ï¼‰
    ]
    
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤
            should_exclude = any(pattern in name for pattern in excluded_patterns)
            if should_exclude:
                continue
            
            # æ ¹æ®lora_targetè¿‡æ»¤æ¨¡å—
            if lora_target == 'all':
                # æ‰€æœ‰å±‚éƒ½æ·»åŠ LoRA
                lora_target_modules.append(name)
            elif lora_target == 'resblock':
                # ä»…ResBlock
                if 'resblock' in name:
                    lora_target_modules.append(name)
            elif lora_target == 'fusformer':
                # ä»…Fusformer
                if 'fusformer' in name:
                    lora_target_modules.append(name)
            elif lora_target == 'resblock+fusformer':
                # ResBlockå’ŒFusformer
                if 'resblock' in name or 'fusformer' in name:
                    lora_target_modules.append(name)
            else:
                # é»˜è®¤æ‰€æœ‰å±‚
                lora_target_modules.append(name)
    
    # ä½¿ç”¨æ‰€æœ‰å€™é€‰æ¨¡å—ï¼ˆä¸å†è¿‡æ»¤ï¼‰
    filtered_modules = lora_target_modules
    
    print(f"\n{'='*70}")
    print(f"LoRAé…ç½®:")
    print(f"  LoRAç›®æ ‡èŒƒå›´: {lora_target}")
    print(f"  å€™é€‰æ¨¡å—æ€»æ•°: {len(lora_target_modules)}")
    print(f"  å°†æ·»åŠ LoRAçš„æ¨¡å—: {len(filtered_modules)}")
    print(f"  LoRA Rank: {args.lora_rank}")
    print(f"{'='*70}\n")
    
    class LoRALayer(nn.Module):
        def __init__(self, base_layer, rank=4, alpha=8):
            super().__init__()
            self.base_layer = base_layer
            self.rank = rank
            self.alpha = alpha
            self.scaling = alpha / rank
            
            if isinstance(base_layer, nn.Conv2d):
                self.lora_down = nn.Conv2d(
                    base_layer.in_channels, 
                    rank, 
                    kernel_size=base_layer.kernel_size,
                    stride=base_layer.stride,
                    padding=base_layer.padding,
                    bias=False
                )
                self.lora_up = nn.Conv2d(
                    rank, 
                    base_layer.out_channels, 
                    kernel_size=1,
                    stride=1,
                    padding=0,
                    bias=False
                )
                nn.init.kaiming_uniform_(self.lora_down.weight, a=1)
                nn.init.zeros_(self.lora_up.weight)
            elif isinstance(base_layer, nn.Linear):
                self.lora_down = nn.Linear(
                    base_layer.in_features,
                    rank,
                    bias=False
                )
                self.lora_up = nn.Linear(
                    rank,
                    base_layer.out_features,
                    bias=False
                )
                nn.init.kaiming_uniform_(self.lora_down.weight, a=1)
                nn.init.zeros_(self.lora_up.weight)
        
        def forward(self, x):
            base_out = self.base_layer(x)
            if hasattr(self, 'lora_down'):
                x_dtype = x.dtype
                lora_down_out = self.lora_down(x.to(self.lora_down.weight.dtype))
                lora_out = self.lora_up(lora_down_out) * self.scaling
                lora_out = lora_out.to(x_dtype)
                return base_out + lora_out
            return base_out
    
    lora_count = 0
    lora_stats = {'resblock': 0, 'fusformer': 0, 'down': 0, 'up': 0, 'other': 0}
    
    for name, module in model.named_modules():
        if name in filtered_modules and isinstance(module, (nn.Conv2d, nn.Linear)):
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]
            parent = dict(model.named_modules())[parent_name] if parent_name else model
            
            lora_layer = LoRALayer(module, rank=args.lora_rank, alpha=args.lora_rank * 2)
            setattr(parent, child_name, lora_layer)
            lora_count += 1
            
            # ç»Ÿè®¡å„æ¨¡å—çš„LoRAæ•°é‡
            if 'resblock' in name:
                lora_stats['resblock'] += 1
            elif 'fusformer' in name:
                lora_stats['fusformer'] += 1
            elif 'down' in name:
                lora_stats['down'] += 1
            elif 'up' in name:
                lora_stats['up'] += 1
            else:
                lora_stats['other'] += 1
    
    print(f"âœ… æˆåŠŸä¸º {lora_count} ä¸ªå±‚æ·»åŠ LoRA")
    print(f"   æ¨¡å—åˆ†å¸ƒ:")
    print(f"     - ResBlock:  {lora_stats['resblock']} å±‚")
    print(f"     - Fusformer: {lora_stats['fusformer']} å±‚")
    print(f"     - Down/Up:   {lora_stats['down'] + lora_stats['up']} å±‚")
    print(f"     - å…¶ä»–:      {lora_stats['other']} å±‚\n")
    
    return model, filtered_modules


class PerceptualLoss(nn.Module):
    """æ„ŸçŸ¥æŸå¤±ï¼Œä½¿ç”¨VGG16æå–ç‰¹å¾ï¼Œæ›´åŠ ç¨³å®šçš„ç‰ˆæœ¬"""
    def __init__(self, style_weight=0):
        super().__init__()
        vgg = models.vgg16(pretrained=True).eval()
        self.feature_extractor = create_feature_extractor(
            vgg, 
            return_nodes={
                'features.4': 'relu1_2',  # æµ…å±‚ç‰¹å¾
                'features.9': 'relu2_2',  # ä¸­å±‚ç‰¹å¾
                # å‡å°‘æ·±å±‚ç‰¹å¾ï¼Œå®ƒä»¬å¯èƒ½å¯¹äºé¥æ„Ÿå›¾åƒä¸å¤ªé€‚ç”¨
                # 'features.16': 'relu3_3',
                # 'features.23': 'relu4_3'
            }
        )
        self.feature_extractor.requires_grad_(False)
        
        # è°ƒæ•´ç‰¹å¾å±‚æƒé‡ï¼Œæ›´å…³æ³¨æµ…å±‚ç‰¹å¾
        self.content_weights = {
            'relu1_2': 0.7,  # å¢åŠ æµ…å±‚ç‰¹å¾æƒé‡
            'relu2_2': 0.3,  # å‡å°‘ä¸­å±‚ç‰¹å¾æƒé‡
        }
        
        # æ³¨å†Œç¼“å†²åŒºä»¥ä¿å­˜å‡å€¼å’Œæ ‡å‡†å·®
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        
    def _normalize(self, x):
        """å°†å›¾åƒå½’ä¸€åŒ–åˆ°VGGé¢„æœŸçš„èŒƒå›´ï¼Œå¢å¼ºç¨³å®šæ€§"""
        # ç¡®ä¿è¾“å…¥æ˜¯3é€šé“çš„ï¼Œå¦‚æœä¸æ˜¯åˆ™å¤åˆ¶é€šé“
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1)
        elif x.shape[1] > 3:
            # å¦‚æœæ˜¯å¤šå…‰è°±å›¾åƒï¼Œå–å‰3ä¸ªé€šé“
            x = x[:, :3, :, :]
        
        # å¼ºåˆ¶å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
        x = x.clamp(0, 1)
        
        # åº”ç”¨ImageNetå½’ä¸€åŒ–
        return (x - self.mean) / self.std
    
    def forward(self, x, y):
        """è®¡ç®—xå’Œyä¹‹é—´çš„æ„ŸçŸ¥æŸå¤±ï¼Œæ·»åŠ æ¢¯åº¦è£å‰ªä»¥å¢å¼ºç¨³å®šæ€§"""
        # ç¡®ä¿è¾“å…¥åœ¨åˆç†èŒƒå›´å†…
        x = x.clamp(0, 1)
        y = y.clamp(0, 1)
        
        x = self._normalize(x)
        y = self._normalize(y)
        
        x_features = self.feature_extractor(x)
        y_features = self.feature_extractor(y)
        
        content_loss = 0.0
        for layer, weight in self.content_weights.items():
            # ä½¿ç”¨L1æŸå¤±ä»£æ›¿MSEï¼Œå¯èƒ½æ›´ç¨³å®š
            layer_loss = weight * F.l1_loss(x_features[layer], y_features[layer])
            content_loss += layer_loss
        
        # è£å‰ªå¼‚å¸¸å¤§çš„æŸå¤±å€¼
        if content_loss > 10.0:
            content_loss = torch.log(content_loss + 1.0)
        
        return content_loss


class VAEEncoder(nn.Module):
    """VAEç¼–ç å™¨ï¼Œç”¨äºæå–PANå›¾åƒçš„ç‰¹å¾è¡¨ç¤ºï¼ŒåŒæ—¶æ”¯æŒControlNetæ¨¡å¼"""
    def __init__(self, input_channels=1, latent_dim=256):
        super().__init__()
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨ä¸»å¹²ç½‘ç»œ
        self.encoder = nn.Sequential(
            nn.Conv2d(input_channels, 64, 4, 2, 1),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(256, 512, 4, 2, 1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),
        )
        
        # å…¨å±€ç‰¹å¾æå–ï¼ˆç”¨äºVAEæ½œåœ¨å‘é‡ï¼‰
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.flatten = nn.Flatten()
        self.fc_mu = nn.Linear(512, latent_dim)
        self.fc_logvar = nn.Linear(512, latent_dim)
        
        # å­˜å‚¨ä¸­é—´ç‰¹å¾å›¾ï¼ˆç”¨äºControlNetï¼‰
        self.feature_maps = {}
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x, return_features=False):
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒè¿”å›ä¸­é—´ç‰¹å¾å›¾
        Args:
            x: è¾“å…¥å›¾åƒ
            return_features: æ˜¯å¦è¿”å›ä¸­é—´ç‰¹å¾å›¾ï¼ˆç”¨äºControlNetï¼‰
        """
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„ç‰¹å¾å›¾
        self.feature_maps = {}
        
        # åˆ†é˜¶æ®µæå–ç‰¹å¾
        features = []
        x_input = x
        
        # æå–æ¯å±‚ç‰¹å¾
        for i, layer in enumerate(self.encoder):
            x_input = layer(x_input)
            if i in [1, 4, 7, 10]:  # LeakyReLUå±‚åçš„ç‰¹å¾
                features.append(x_input)
                self.feature_maps[f'level_{len(features)}'] = x_input
        
        # æå–å…¨å±€ç‰¹å¾ç”¨äºVAE
        h = self.global_pool(x_input)
        h = self.flatten(h)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        
        if return_features:
            return mu, logvar, features
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def get_feature_maps(self):
        """è·å–ä¸­é—´ç‰¹å¾å›¾ï¼Œç”¨äºControlNet"""
        return self.feature_maps


class ZeroConv(nn.Module):
    """é›¶åˆå§‹åŒ–çš„1x1å·ç§¯å±‚ï¼Œç”¨äºControlNetè¾“å‡º"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)
        nn.init.zeros_(self.conv.weight)
        if self.conv.bias is not None:
            nn.init.zeros_(self.conv.bias)
    
    def forward(self, x):
        return self.conv(x)


class ControlNet(nn.Module):
    """
    å®Œæ•´çš„ControlNetå®ç°ï¼Œå®Œå…¨å¤åˆ»SSNetç¼–ç å™¨ç»“æ„å¹¶å¤åˆ¶å…¶æƒé‡
    ç”¨äºä»PANå›¾åƒä¸­æå–å¤šå°ºåº¦ç©ºé—´æ§åˆ¶ç‰¹å¾

    1. ç»“æ„ä¸SSNetçš„PANç¼–ç å™¨å®Œå…¨ä¸€è‡´ï¼ˆtrainable copyï¼‰
    2. ä»SSNetå¤åˆ¶é¢„è®­ç»ƒæƒé‡ï¼ˆlocked copy -> trainable copyï¼‰
    3. ä½¿ç”¨zero-convolutionç¡®ä¿è®­ç»ƒåˆæœŸä¸å½±å“ä¸»æ¨¡å‹
    """
    def __init__(self, unet_model, ms_dim=8, pan_dim=1, copy_weights=True, verbose=True):
        super().__init__()
        self.ms_dim = ms_dim
        self.pan_dim = pan_dim
        self.verbose = verbose  # æ§åˆ¶æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
        # ä»UNetä¸­æå–å¿…è¦çš„é…ç½®
        self.model_channels = unet_model.model_channels
        self.use_scale_shift_norm = unet_model.use_scale_shift_norm
        dim = 32  # åŸºç¡€ç»´åº¦

        
        # æ—¶é—´åµŒå…¥å±‚ï¼ˆå¤åˆ¶è‡ªSSNetï¼‰
        time_embed_dim = self.model_channels * 4
        self.time_embed = nn.Sequential(
            linear(self.model_channels, time_embed_dim),
            SiLU(),
            linear(time_embed_dim, time_embed_dim),
        )
        
        # è¾“å…¥ç«¯zero-convï¼ˆåŸå§‹ControlNetè®¾è®¡ï¼‰
        # å°†PAN+x_tæ˜ å°„åˆ°åˆå§‹ç‰¹å¾ï¼Œä½†è®­ç»ƒåˆæœŸè¾“å‡ºä¸º0
        self.input_hint_block = nn.Sequential(
            nn.Conv2d(ms_dim + pan_dim, 16, 3, 1, 1),
            SiLU(),
            nn.Conv2d(16, 32, 3, 1, 1),
            SiLU(),
            zero_module(nn.Conv2d(32, dim, 3, 1, 1))  # é›¶åˆå§‹åŒ–æœ€åä¸€å±‚
        )
        
        # æ—¶é—´åµŒå…¥æŠ•å½±å±‚ï¼ˆå¤åˆ¶è‡ªSSNetï¼‰
        self.out_layers_pan = nn.Sequential(
            nn.GroupNorm(32, dim),
            SiLU(),
            nn.Dropout(p=0.0),
            zero_module(conv_nd(2, dim, dim, 3, padding=1)),
        )
        
        self.emb_layers_pan = nn.Sequential(
            SiLU(),
            linear(time_embed_dim, 2 * dim if self.use_scale_shift_norm else dim),
        )
        
        # å®šä¹‰å„å±‚ç»´åº¦ï¼ˆä¸SSNetå®Œå…¨ä¸€è‡´ï¼‰
        dim0 = dim       # 32
        dim1 = dim * 2   # 64
        dim2 = dim * 4   # 128
        dim3 = dim * 2   # 64
        dim4 = dim       # 32
        
        dim_head = 16
        se_ratio_mlp = 0.5
        se_ratio_rb = 0.5
        
        # Level 0: 64x64 åˆ†è¾¨ç‡ï¼ˆä¸SSNetå®Œå…¨ä¸€è‡´ï¼‰
        self.fusformer0 = Fusformer(dim0, dim0//dim_head, dim_head, int(dim0*se_ratio_mlp))
        self.resblock0 = ResBlock(
            dim0, int(se_ratio_rb*dim0), dim0,
            model_channels=self.model_channels,
            use_scale_shift_norm=self.use_scale_shift_norm,
            use_arconv=False,  # ControlNetä¸ä½¿ç”¨ARConv
        )
        self.down0 = Down(dim0, dim1)
        
        # Level 1: 32x32 åˆ†è¾¨ç‡
        self.fusformer1 = Fusformer(dim1, dim1//dim_head, dim_head, int(dim1*se_ratio_mlp))
        self.resblock1 = ResBlock(
            dim1, int(se_ratio_rb*dim1), dim1,
            use_scale_shift_norm=self.use_scale_shift_norm,
            use_arconv=False,
        )
        self.down1 = Down(dim1, dim2)
        
        # Level 2: 16x16 åˆ†è¾¨ç‡ï¼ˆæœ€æ·±å±‚ï¼‰
        self.fusformer2 = Fusformer(dim2, dim2//dim_head, dim_head, int(dim2*se_ratio_mlp))
        self.resblock2 = ResBlock(
            dim2, int(se_ratio_rb*dim2), dim2,
            use_scale_shift_norm=self.use_scale_shift_norm,
            use_arconv=False,
        )
        self.up0 = Up(dim2, dim3)
        
        # Level 3: 32x32 åˆ†è¾¨ç‡ï¼ˆä¸Šé‡‡æ ·ï¼‰
        self.fusformer3 = Fusformer(dim3, dim3//dim_head, dim_head, int(dim3*se_ratio_mlp))
        self.resblock3 = ResBlock(
            dim3, int(se_ratio_rb*dim3), dim3,
            use_scale_shift_norm=self.use_scale_shift_norm,
            use_arconv=False,
        )
        self.up1 = Up(dim3, dim4)
        
        self.fusformer4 = Fusformer(dim4, dim4//dim_head, dim_head, int(dim4*se_ratio_mlp))
        
        # ========================================
        # ä»UNetå¤åˆ¶æƒé‡ä½œä¸ºåˆå§‹åŒ–ï¼ˆè®­ç»ƒæ—¶å¿…è¦ï¼Œæµ‹è¯•æ—¶ä¼šè¢«checkpointè¦†ç›–ï¼‰
        if copy_weights:
            self._copy_weights_from_unet(unet_model)
        
        self.zero_convs = nn.ModuleDict({
            'level_0': ZeroConv(dim0, dim0),
            'level_1': ZeroConv(dim1, dim1),
            'level_2': ZeroConv(dim2, dim2),
            'level_3': ZeroConv(dim3, dim3),
            'level_4': ZeroConv(dim4, dim4),
        })
        
        # é—¨æ§å‚æ•°
        self.control_gate = nn.ParameterDict({
            'level_0': nn.Parameter(torch.tensor(-0.5)),
            'level_1': nn.Parameter(torch.tensor(-0.5)),
            'level_2': nn.Parameter(torch.tensor(-0.5)),
            'level_3': nn.Parameter(torch.tensor(-0.5)),
            'level_4': nn.Parameter(torch.tensor(-0.5))
        })
        
        print(f"\n{'='*70}")
        print(f"ControlNetåˆå§‹åŒ–å®Œæˆ")
    
    def _copy_weights_from_unet(self, unet_model):
        """
        ä»SSNetå¤åˆ¶å¯¹åº”æ¨¡å—çš„é¢„è®­ç»ƒæƒé‡ä½œä¸ºåˆå§‹åŒ–
        """
        if self.verbose:
            print(f"\nğŸ“¦ æ­£åœ¨ä»UNetå¤åˆ¶æƒé‡è¿›è¡Œåˆå§‹åŒ–...")
            print(f"   (æ³¨ï¼šæµ‹è¯•æ—¶æ­¤æƒé‡ä¼šè¢«checkpointè¦†ç›–)")
        
        # æ˜ å°„è¡¨ï¼šControlNetæ¨¡å— -> SSNetæ¨¡å—
        module_mapping = [
            # æ—¶é—´åµŒå…¥
            ('time_embed', 'time_embed'),
            ('emb_layers_pan', 'emb_layers_pan'),
            ('out_layers_pan', 'out_layers_pan'),
            # Level 0
            ('fusformer0', 'fusformer0'),
            ('resblock0', 'resblock0'),
            ('down0', 'down0'),
            # Level 1
            ('fusformer1', 'fusformer1'),
            ('resblock1', 'resblock1'),
            ('down1', 'down1'),
            # Level 2
            ('fusformer2', 'fusformer2'),
            ('resblock2', 'resblock2'),
            ('up0', 'up0'),
            # Level 3
            ('fusformer3', 'fusformer3'),
            ('resblock3', 'resblock3'),
            ('up1', 'up1'),
            # Level 4
            ('fusformer4', 'fusformer4'),
        ]
        
        copied_modules = 0
        total_params = 0
        
        for ctrl_name, unet_name in module_mapping:
            if hasattr(self, ctrl_name) and hasattr(unet_model, unet_name):
                ctrl_module = getattr(self, ctrl_name)
                unet_module = getattr(unet_model, unet_name)
                
                # å¤åˆ¶æƒé‡
                try:
                    ctrl_state = ctrl_module.state_dict()
                    unet_state = unet_module.state_dict()
                    
                    # åªå¤åˆ¶å½¢çŠ¶åŒ¹é…çš„å‚æ•°
                    for key in ctrl_state.keys():
                        if key in unet_state and ctrl_state[key].shape == unet_state[key].shape:
                            ctrl_state[key].copy_(unet_state[key])
                            total_params += ctrl_state[key].numel()
                    
                    ctrl_module.load_state_dict(ctrl_state)
                    copied_modules += 1
                except Exception as e:
                    if self.verbose:
                        print(f"  âš  è·³è¿‡ {ctrl_name}: {e}")
        
        if self.verbose:
            print(f"âœ“ æˆåŠŸå¤åˆ¶ {copied_modules}/{len(module_mapping)} ä¸ªæ¨¡å—")
            print(f"âœ“ ControlNetæƒé‡åˆå§‹åŒ–å®Œæˆ")
    
    def time_emb_pan(self, h, emb):
        """åº”ç”¨æ—¶é—´åµŒå…¥åˆ°PANç‰¹å¾"""
        emb_out = self.emb_layers_pan(emb).type(h.dtype)
        while len(emb_out.shape) < len(h.shape):
            emb_out = emb_out[..., None]
        
        if self.use_scale_shift_norm:
            out_norm, out_rest = self.out_layers_pan[0], self.out_layers_pan[1:]
            scale, shift = torch.chunk(emb_out, 2, dim=1)
            h = out_norm(h) * (1 + scale) + shift
            h = out_rest(h)
        else:
            h = h + emb_out
            h = self.out_layers_pan(h)
        return h
    
    def forward(self, pan, x_t, timesteps):
        """
        å‰å‘ä¼ æ’­ï¼Œæå–å¤šå°ºåº¦æ§åˆ¶ç‰¹å¾ï¼ˆå®Œå…¨éµå¾ªåŸå§‹ControlNetè®¾è®¡ï¼‰
        
        Args:
            pan: PANå›¾åƒ [B, 1, H, W]
            x_t: å™ªå£°æ®‹å·® [B, 8, H, W]
            timesteps: æ—¶é—´æ­¥ [B]
        
        Returns:
            control_features: å­—å…¸ï¼ŒåŒ…å«5ä¸ªå±‚çº§çš„æ§åˆ¶ç‰¹å¾
                - 'level_0': [B, 32, 64, 64]
                - 'level_1': [B, 64, 32, 32]
                - 'level_2': [B, 128, 16, 16]
                - 'level_3': [B, 64, 32, 32]
                - 'level_4': [B, 32, 64, 64]
        """
        # æ—¶é—´åµŒå…¥
        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))
        
        # åŸå§‹ControlNetï¼šè¾“å…¥æ¡ä»¶é€šè¿‡hint blockå¤„ç†
        # è¾“å…¥: PAN+x_t -> hint block -> åˆå§‹ç‰¹å¾
        # è®­ç»ƒåˆæœŸï¼šhint blockçš„æœ€åä¸€å±‚æ˜¯é›¶åˆå§‹åŒ–ï¼Œè¾“å‡ºâ‰ˆ0
        pan_xt = torch.cat([pan, x_t], dim=1)  # [B, 9, H, W]
        y = self.input_hint_block(pan_xt)  # [B, 32, H, W]
        
        # æ—¶é—´åµŒå…¥æ³¨å…¥
        y = self.time_emb_pan(y, emb)
        
        control_features = {}
        
        # Level 0: 64x64
        y = self.fusformer0(y, y)  # è‡ªæ³¨æ„åŠ›
        y = self.resblock0(y, emb, epoch=0)
        # åŸå§‹ControlNetï¼šè¾“å‡ºé€šè¿‡zero-conv + é—¨æ§
        # zero_conv: è®­ç»ƒåˆæœŸè¾“å‡ºä¸º0
        # control_gate: sigmoid(-3.0) â‰ˆ 0.047ï¼Œè¿›ä¸€æ­¥æŠ‘åˆ¶ ç°åœ¨ä¸ç”¨äº†
        control_features['level_0'] = self.zero_convs['level_0'](y) 
        skip_0 = y
        y = self.down0(y)  # ä¸‹é‡‡æ ·åˆ°32x32
        
        # Level 1: 32x32
        y = self.fusformer1(y, y)
        y = self.resblock1(y, emb, epoch=0)
        control_features['level_1'] = self.zero_convs['level_1'](y) 
        skip_1 = y
        y = self.down1(y)  # ä¸‹é‡‡æ ·åˆ°16x16
        
        # Level 2: 16x16ï¼ˆæœ€æ·±å±‚ï¼‰
        y = self.fusformer2(y, y)
        y = self.resblock2(y, emb, epoch=0)
        control_features['level_2'] = self.zero_convs['level_2'](y)
        y = self.up0(y, skip_1)  # ä¸Šé‡‡æ ·åˆ°32x32
        
        # Level 3: 32x32ï¼ˆä¸Šé‡‡æ ·ï¼‰
        y = self.fusformer3(y, y)
        y = self.resblock3(y, emb, epoch=0)
        control_features['level_3'] = self.zero_convs['level_3'](y) 
        y = self.up1(y, skip_0)  # ä¸Šé‡‡æ ·åˆ°64x64
        
        # Level 4: 64x64ï¼ˆæœ€ç»ˆå±‚ï¼‰
        y = self.fusformer4(y, y)
        control_features['level_4'] = self.zero_convs['level_4'](y) 
        
        return control_features


class SSDiff_VAE_gen(nn.Module):
    """åŸºäºVAEçš„å•æ­¥ç”Ÿæˆå™¨æ¨¡å‹ï¼Œæ”¯æŒControlNet"""
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.current_step = 0
        
        # åŠ è½½é¢„è®­ç»ƒçš„è’¸é¦æ¨¡å‹ï¼ˆå¸¦LoRAï¼‰- éœ€è¦å…ˆåˆå§‹åŒ–UNet
        self.unet, self.lora_target_modules = initialize_ssdiff_unet_with_lora(
            args, 
            pretrained_path=args.pretrained_ssdiff_path
        )
        
        # åŠ è½½é¢„è®­ç»ƒçš„LoRAæƒé‡
        if hasattr(args, 'lora_checkpoint_path') and args.lora_checkpoint_path:
            lora_state = torch.load(args.lora_checkpoint_path, map_location='cpu')
            if 'unet_state_dict' in lora_state:
                for name, param in self.unet.named_parameters():
                    if 'lora' in name and name in lora_state['unet_state_dict']:
                        param.data.copy_(lora_state['unet_state_dict'][name])
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—ï¼ˆäº’ç›¸ç‹¬ç«‹ï¼‰
        self.vae_encoder = None
        self.controlnet_full = None
        self.tokenizer = None
        self.text_encoder = None
        self.use_perceptual_loss = False
        self.use_kl_loss = False
        self.use_clip = False
        self.use_ram = False  
        self.ram_caption_generator = None
        
        # é€‰é¡¹1ï¼šVAEç¼–ç å™¨ï¼ˆç”¨äºæå–å…¨å±€åœºæ™¯ç‰¹å¾ï¼Œä½œä¸ºscene_tokenï¼‰
        if hasattr(args, 'use_vae') and args.use_vae:
            latent_dim = getattr(args, 'vae_latent_dim', 256)
            self.vae_encoder = VAEEncoder(
                input_channels=1,
                latent_dim=latent_dim
            )
            print(f"âœ“ å·²å¯ç”¨VAEç¼–ç å™¨ï¼ˆå…¨å±€åœºæ™¯ç‰¹å¾ï¼‰")
            
            # VAEç›¸å…³çš„æŸå¤±å‡½æ•°
            self.use_perceptual_loss = getattr(args, 'use_perceptual_loss', False)
            if self.use_perceptual_loss:
                self.perceptual_loss = PerceptualLoss()
                self.lambda_perceptual = getattr(args, 'lambda_perceptual', 0.1)
                print(f"  - æ„ŸçŸ¥æŸå¤±: å¯ç”¨ (Î»={self.lambda_perceptual})")
            
            self.use_kl_loss = getattr(args, 'use_kl_loss', False)
            self.lambda_kl = getattr(args, 'lambda_kl', 0.001)
            if self.use_kl_loss:
                print(f"  - KLæ•£åº¦æŸå¤±: å¯ç”¨ (Î»={self.lambda_kl})")
        
        # é€‰é¡¹2ï¼šå®Œæ•´ControlNetï¼ˆç”¨äºæå–å¤šå°ºåº¦ç©ºé—´ç‰¹å¾ï¼Œä½œä¸ºcontrol_featuresï¼‰
        if hasattr(args, 'use_controlnet') and args.use_controlnet:
            ms_dim = getattr(args, 'ms_channels', 8)
            pan_dim = 1
            self.controlnet_full = ControlNet(
                unet_model=self.unet,
                ms_dim=ms_dim,
                pan_dim=pan_dim
            )
            print(f"âœ“ å·²å¯ç”¨å®Œæ•´ControlNetï¼ˆå¤šå°ºåº¦ç©ºé—´ç‰¹å¾ï¼‰")
        
        # é€‰é¡¹3ï¼šCLIPæ–‡æœ¬ç¼–ç å™¨ï¼ˆç”¨äºä»æ–‡æœ¬æç¤ºä¸­æå–è¯­ä¹‰ç‰¹å¾ï¼‰
        if hasattr(args, 'use_clip') and args.use_clip:
            # ä½¿ç”¨SD21Baseä½œä¸ºCLIPåŸºç¡€æ¨¡å‹è·¯å¾„
            clip_base_path = getattr(args, 'clip_model_path', None)
            if clip_base_path is None:
                # é»˜è®¤ä½¿ç”¨SD21Baseè·¯å¾„
                import os
                script_dir = os.path.dirname(os.path.abspath(__file__))
                clip_base_path = os.path.join(script_dir, "model", "SD21Base")
            
            print(f"try to use clip from: {clip_base_path}")
            try:
                # ä»SD21BaseåŠ è½½tokenizerå’Œtext_encoderåŸºç¡€ç»“æ„
                self.tokenizer = AutoTokenizer.from_pretrained(
                    clip_base_path, 
                    subfolder="tokenizer"
                )
                self.text_encoder = CLIPTextModel.from_pretrained(
                    clip_base_path, 
                    subfolder="text_encoder"
                ).to(next(self.unet.parameters()).device)
                
                # å†»ç»“CLIPæ¨¡å‹ï¼ˆè®­ç»ƒæ—¶ä¸æ›´æ–°CLIPæƒé‡ï¼‰
                self.text_encoder.requires_grad_(False)
                
                self.use_clip = True
                self.default_prompt = getattr(args, 'prompt', "A high resolution satellite image")
                
                print(f"âœ“ å·²ä»SD21BaseåŠ è½½CLIPæ–‡æœ¬ç¼–ç å™¨")
                print(f"  - åŸºç¡€æ¨¡å‹è·¯å¾„: {clip_base_path}")
                print(f"  - é»˜è®¤æç¤º: \"{self.default_prompt}\"")
                print(f"  - CLIPæ¨¡å‹å·²å†»ç»“ï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰")
                
            except Exception as e:
                print(f"âš  CLIPæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                self.use_clip = False
                self.tokenizer = None
                self.text_encoder = None
        
        #  é€‰é¡¹4ï¼šRAM Captionç”Ÿæˆå™¨ï¼ˆç”¨äºä»PANå›¾åƒè‡ªåŠ¨ç”Ÿæˆcaptionï¼‰
        if hasattr(args, 'use_ram') and args.use_ram:
            self.use_ram = True
            ram_model_path = getattr(args, 'ram_model_path', None)

            try:
                # =============================
                # åˆå§‹åŒ– RAM caption ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰å°è£…ï¼‰
                # =============================
                device = next(self.unet.parameters()).device

                # åˆå§‹åŒ–ç”Ÿæˆå™¨
                self.ram_caption_generator = RAMCaptionGenerator(
                    model_path=ram_model_path,
                    device=device
                )

                print(f"âœ“ å·²å¯ç”¨RAM Captionç”Ÿæˆå™¨ï¼ˆSwin-Large æ¨¡å‹ï¼‰")
                print(f"  - è‡ªåŠ¨ä»PANå›¾åƒç”Ÿæˆcaption")
                print(f"  - æ¨¡å‹è·¯å¾„: {ram_model_path}")

            except Exception as e:
                print(f"âš  RAMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.use_ram = False
                self.ram_caption_generator = None
        
        # æ‰“å°æœ€ç»ˆé…ç½®
        if self.vae_encoder is None and self.controlnet_full is None and not self.use_clip and not self.use_ram:
            print(f"âš  æœªå¯ç”¨ä»»ä½•æ¡ä»¶æ¨¡å—ï¼Œä½¿ç”¨åŸå§‹UNet+LoRA")
        
        _, self.diffusion = create_model_and_diffusion(
            **args_to_dict(args, model_and_diffusion_defaults().keys())
        )
        
        self.inference_timestep = 999
        self.lora_rank = args.lora_rank
        self.training = True
    
    def set_step(self, step):
        self.current_step = step
        if hasattr(self.unet, 'set_epoch'):
            self.unet.set_epoch(step)
    
    def _build_color_subspace(self):
        """
        æ„å»ºé¢œè‰²å­ç©ºé—´çš„æ­£äº¤åŸºï¼ˆç”¨äºé¢œè‰²å»åï¼‰
        """
        color_words = [
            "green", "blue", "red", "brown", "yellow", "white", "gray", "grey",
            "black", "purple", "pink", "orange", "cyan", "magenta",
            "light", "dark", "pale", "bright", "deep"
        ]
        
        device = next(self.text_encoder.parameters()).device
        
        # ç¼–ç é¢œè‰²è¯
        with torch.no_grad():
            color_embeds_list = []
            for color in color_words:
                text_input = self.tokenizer(
                    color,
                    padding="max_length",
                    max_length=self.tokenizer.model_max_length,
                    truncation=True,
                    return_tensors="pt"
                ).to(device)
                
                color_embed = self.text_encoder(text_input.input_ids)[0]  # [1, 77, 768]
                color_embed = color_embed.mean(dim=1)  # [1, 768]
                color_embeds_list.append(color_embed)
            
            # å †å æ‰€æœ‰é¢œè‰²embedding: [K, D]
            color_embeds = torch.cat(color_embeds_list, dim=0)
            # å½’ä¸€åŒ–
            color_embeds = F.normalize(color_embeds, dim=-1)
            
            # QRåˆ†è§£æ„å»ºæ­£äº¤åŸº: Q @ R = color_embeds.T
            Q, _ = torch.linalg.qr(color_embeds.T)  # Q: [D, K]
        
        return Q
    
    def _apply_color_debiasing(self, text_embeds, Q):
        """
        å¯¹text embeddingåº”ç”¨é¢œè‰²å­ç©ºé—´å»å
        """
        # æŠ•å½±åˆ°é¢œè‰²å­ç©ºé—´: proj_color = (Q @ Q.T) @ text_embeds.T
        proj_color = Q @ (Q.T @ text_embeds.T)  # [D, B]
        
        # å»é™¤é¢œè‰²åˆ†é‡
        text_debiased = text_embeds.T - proj_color  # [D, B]
        text_debiased = text_debiased.T  # [B, D]
        
        # å½’ä¸€åŒ–åˆ°unit sphere
        text_debiased = F.normalize(text_debiased, dim=-1)
        
        return text_debiased
    
    def select_clip_semantic_label(self, ram_caption, top_k=3, max_ram_tags=12):
        """
        ä½¿ç”¨CLIPå¯¹RAMç”Ÿæˆçš„captionè¿›è¡Œè¯­ä¹‰è¿‡æ»¤ï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰
        Args:
            ram_caption: RAMç”Ÿæˆçš„åŸå§‹captionï¼ˆé€—å·åˆ†éš”çš„æ ‡ç­¾åˆ—è¡¨ï¼‰
            top_k: é€‰æ‹©å‰kä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾ï¼ˆé»˜è®¤3ï¼‰
            max_ram_tags: RAMè¾“å‡ºä¸­ä¿ç•™çš„æœ€å¤§æ ‡ç­¾æ•°ï¼ˆé¿å…è¿‡é•¿å¯¼è‡´æˆªæ–­ï¼‰
        Returns:
            topk_labels: è¿‡æ»¤åçš„æ ‡ç­¾åˆ—è¡¨
        """
        labels = [
            "urban area", "residential zone", "industrial zone", "forest", "grassland",
            "mountain", "river", "lake", "sea", "beach", "harbor", "farmland",
            "road network", "bare soil", "wetland", "desert",
            "bridge", "building", "highway", "stadium",
            "coastline", "cliff", "hill slope", "dense vegetation", "open water",
            "cloudy", "coastal city", "harbor town"
        ]
        
        # é¢„å¤„ç†RAMçš„è¾“å‡ºï¼šRAMæŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½è¾“å‡ºæ ‡ç­¾ï¼Œæˆ‘ä»¬åªä¿ç•™å‰Nä¸ª
        if isinstance(ram_caption, str):
            ram_tags = [tag.strip() for tag in ram_caption.split(',')]
            ram_tags = ram_tags[:max_ram_tags]
            ram_caption_short = ', '.join(ram_tags)
        else:
            ram_caption_short = ram_caption
        
        # å°†RAMè¾“å‡ºä¸æ ‡ç­¾ä¸€å¹¶tokenize
        device = next(self.text_encoder.parameters()).device
        text_inputs = self.tokenizer(
            labels + [ram_caption_short],
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            # æå–æ–‡æœ¬ç‰¹å¾
            text_embeds = self.text_encoder(text_inputs.input_ids)[0]  # [len(labels)+1, 77, 768]
            text_embeds = text_embeds.mean(dim=1)  # å…¨å±€å¹³å‡æ± åŒ– -> [len(labels)+1, 768]
            text_embeds = text_embeds / (text_embeds.norm(dim=-1, keepdim=True) + 1e-6)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼šlabelsä¸ram_captionçš„ç›¸ä¼¼åº¦
            label_embeds = text_embeds[:-1]  # [num_labels, 768]
            caption_embed = text_embeds[-1:]  # [1, 768]
            scores = (label_embeds * caption_embed).sum(dim=-1)  # [num_labels]
            
        # å–å‰ top_k ä¸ªæ ‡ç­¾
        topk_idx = torch.topk(scores, k=min(top_k, len(labels))).indices.cpu().numpy()
        topk_labels = [labels[i] for i in topk_idx]
        
        return topk_labels
    
    def select_clip_semantic_label_with_color_debiasing(self, ram_caption, top_k=5):
        """
        å¯¹RAMç”Ÿæˆçš„captionè¿›è¡Œé¢œè‰²å»åï¼Œç›´æ¥è¿”å›å»ååçš„æ ‡ç­¾
        
        """
        # è§£æRAMè¾“å‡º
        if isinstance(ram_caption, str):
            ram_tags = [tag.strip() for tag in ram_caption.split(',')]
        else:
            ram_tags = ram_caption
        
        # å¦‚æœRAMè¾“å‡ºå°‘äºç­‰äºtop_kä¸ªï¼Œç›´æ¥è¿”å›å…¨éƒ¨
        if len(ram_tags) <= top_k:
            return ram_tags[:top_k]
        
        # ç¼–ç æ‰€æœ‰RAMæ ‡ç­¾
        device = next(self.text_encoder.parameters()).device
        text_inputs = self.tokenizer(
            ram_tags,
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            # æå–æ–‡æœ¬ç‰¹å¾
            text_embeds = self.text_encoder(text_inputs.input_ids)[0]  # [num_tags, 77, 768]
            text_embeds = text_embeds.mean(dim=1)  # å…¨å±€å¹³å‡æ± åŒ– -> [num_tags, 768]
            text_embeds = F.normalize(text_embeds, dim=-1)
            
            Q = self._build_color_subspace()  # [768, K]
            
            tag_embeds_debiased = self._apply_color_debiasing(text_embeds, Q)  # [num_tags, 768]
            
            color_scores = (text_embeds * tag_embeds_debiased).sum(dim=-1)  # [num_tags]
            
            topk_idx = torch.topk(color_scores, k=min(top_k, len(ram_tags))).indices.cpu().numpy()
            
        debiased_labels = [ram_tags[i] for i in topk_idx]
        
        return debiased_labels
    
    def encode_prompt(self, prompt_batch):
        """
        ç¼–ç æ–‡æœ¬æç¤ºä¸ºæ–‡æœ¬åµŒå…¥
        Args:
            prompt_batch: æ–‡æœ¬æç¤ºåˆ—è¡¨
        Returns:
            prompt_embeds: æ–‡æœ¬åµŒå…¥ [B, 77, 768]
        """
        if not self.use_clip or self.tokenizer is None or self.text_encoder is None:
            return None 
        prompt_embeds_list = []
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼Œç¡®è®¤ä¸è®­ç»ƒ
            for caption in prompt_batch:
                text_input_ids = self.tokenizer(
                    caption, 
                    max_length=self.tokenizer.model_max_length,
                    padding="max_length", 
                    truncation=True, 
                    return_tensors="pt"
                ).input_ids
                prompt_embeds = self.text_encoder(
                    text_input_ids.to(self.text_encoder.device)
                )[0]
                prompt_embeds_list.append(prompt_embeds)
        
        if prompt_embeds_list:
            prompt_embeds = torch.concat(prompt_embeds_list, dim=0)
            prompt_embeds = F.layer_norm(prompt_embeds, prompt_embeds.shape[-1:])
            ## ä¿®æ”¹
            if self.current_step % 500 == 0:
                with torch.no_grad():
                    mean_var = prompt_embeds.var(dim=-1).mean().item()
                    mean_norm = prompt_embeds.norm(dim=-1).mean().item()
                    print(f"[CLIP monitor] norm={mean_norm:.3f}, var={mean_var:.5f}")
            return prompt_embeds
        return None
    
    def set_train(self, enable_arconv=False, train_vae=False, train_lora=True, train_controlnet=True, train_clip=False):
        """
        è®¾ç½®è®­ç»ƒæ¨¡å¼
        Args:
            enable_arconv: æ˜¯å¦å¯ç”¨ARConvè®­ç»ƒ
            train_vae: æ˜¯å¦è®­ç»ƒVAEç¼–ç å™¨
            train_lora: æ˜¯å¦è®­ç»ƒLoRAå±‚ï¼ˆé»˜è®¤Trueï¼‰
            train_controlnet: æ˜¯å¦è®­ç»ƒå®Œæ•´ControlNetï¼ˆé»˜è®¤Trueï¼‰
            train_clip: æ˜¯å¦è®­ç»ƒCLIPæ–‡æœ¬ç¼–ç å™¨ï¼ˆé»˜è®¤Falseï¼‰
        """
        self.training = True
        self.unet.train()
        
        trainable_count = {'lora': 0, 'vae': 0, 'arconv': 0, 'controlnet': 0, 'clip': 0, 'gate': 0, 'proj': 0}
        
        # è®¾ç½®UNetå‚æ•°
        for n, p in self.unet.named_parameters():
            if "lora_down" in n or "lora_up" in n:
                p.requires_grad = train_lora
                if train_lora:
                    trainable_count['lora'] += 1
            elif any(key in n.lower() for key in ['arconv', 'adaptive', 'offset', 'modulation', 'kernel_gen']):
                p.requires_grad = enable_arconv
                if enable_arconv:
                    trainable_count['arconv'] += 1
            #
            elif 'gate' in n and 'attn_gate' not in n:  # attn_gateæ˜¯bufferï¼Œä¸è®­ç»ƒ
                p.requires_grad = True
                trainable_count['gate'] += 1
            #
            elif 'text_proj' in n:
                p.requires_grad = True
                trainable_count['proj'] += 1
            else:
                p.requires_grad = False
        
        # è®¾ç½®VAEç¼–ç å™¨å‚æ•°ï¼ˆç‹¬ç«‹ï¼‰
        if self.vae_encoder is not None:
            self.vae_encoder.train()
            for p in self.vae_encoder.parameters():
                p.requires_grad = train_vae
                if train_vae:
                    trainable_count['vae'] += 1
        
        # è®¾ç½®å®Œæ•´ControlNetå‚æ•°ï¼ˆç‹¬ç«‹ï¼‰
        if self.controlnet_full is not None:
            self.controlnet_full.train()
            for p in self.controlnet_full.parameters():
                p.requires_grad = train_controlnet
                if train_controlnet:
                    trainable_count['controlnet'] += 1
        
        # è®¾ç½®CLIPæ–‡æœ¬ç¼–ç å™¨å‚æ•°ï¼ˆç‹¬ç«‹ï¼‰
        if self.use_clip and self.text_encoder is not None:
            if train_clip:
                self.text_encoder.train()
                for p in self.text_encoder.parameters():
                    p.requires_grad = True
                    trainable_count['clip'] += 1
            else:
                self.text_encoder.eval()
                for p in self.text_encoder.parameters():
                    p.requires_grad = False
        
        if not hasattr(self, '_last_train_state') or self._last_train_state != (enable_arconv, train_vae, train_lora, train_controlnet, train_clip):
            arconv_status = "è®­ç»ƒä¸­" if enable_arconv else "å†»ç»“"
            vae_status = "è®­ç»ƒä¸­" if train_vae else "å†»ç»“"
            lora_status = "è®­ç»ƒä¸­" if train_lora else "å†»ç»“"
            controlnet_status = "è®­ç»ƒä¸­" if train_controlnet else "å†»ç»“"
            clip_status = "è®­ç»ƒä¸­" if train_clip else "å†»ç»“"
            print(f"å¯è®­ç»ƒå‚æ•°: LoRA={trainable_count['lora']} ({lora_status}), VAE={trainable_count['vae']} ({vae_status}), ARConv={trainable_count['arconv']} ({arconv_status}), ControlNet={trainable_count['controlnet']} ({controlnet_status}), CLIP={trainable_count['clip']} ({clip_status})")
            # æ˜¾ç¤ºé—¨æ§å’ŒæŠ•å½±å±‚å‚æ•°ï¼ˆå§‹ç»ˆè®­ç»ƒï¼‰
            if trainable_count['gate'] > 0 or trainable_count['proj'] > 0:
                print(f" CLIPç›¸å…³å‚æ•°: Gate={trainable_count['gate']} (å§‹ç»ˆè®­ç»ƒ), Projection={trainable_count['proj']} (å§‹ç»ˆè®­ç»ƒ)")
            self._last_train_state = (enable_arconv, train_vae, train_lora, train_controlnet, train_clip)
    
    def forward(self, lms, pan, ms, gt=None, prompt=None):
        device = lms.device
        batch_size = lms.shape[0]
        
        # VAEç¼–ç å™¨ï¼šæå–å…¨å±€åœºæ™¯ç‰¹å¾ -> scene_token
        vae_features = None
        if self.vae_encoder is not None:
            mu, logvar = self.vae_encoder(pan)
            vae_features = mu  # ä½¿ç”¨muä½œä¸ºscene_token
        
        # CLIPæ–‡æœ¬ç¼–ç å™¨ï¼šæå–æ–‡æœ¬è¯­ä¹‰ç‰¹å¾ -> encoder_hidden_states
        text_embeds = None
        if self.use_clip and self.text_encoder is not None:
            # å¦‚æœå¯ç”¨RAMä¸”æ²¡æœ‰æä¾›promptï¼Œä¸ºå½“å‰batchç”Ÿæˆcaption
            if prompt is None and self.use_ram and self.ram_caption_generator is not None:
                with torch.no_grad():
                    captions = []
                    for i in range(batch_size):
                        pan_i = pan[i]  # ä½¿ç”¨PANå›¾åƒç”Ÿæˆcaption
                        caption_raw = self.ram_caption_generator.generate_caption(pan_i)
                        clip_labels = self.select_clip_semantic_label_with_color_debiasing(caption_raw)
                        caption_filtered = ', '.join(clip_labels)
                        captions.append(caption_filtered)
                    prompt = captions
                    
                    # å®šæœŸç›‘æ§captionå¤šæ ·æ€§
                    if self.current_step % 500 == 0:
                        diversity = len(set(captions)) / max(len(captions), 1)
                        avg_len = sum(len(str(c)) for c in captions) / max(len(captions), 1)
                        print(f"[RAM monitor] batch_size={len(captions)}, unique_rate={diversity:.3f}, avg_len={avg_len:.1f}")
                        print(f"  Sample captions: {captions[:3]}")
                        
            # å¦‚æœä»ç„¶æ²¡æœ‰promptï¼Œä½¿ç”¨é»˜è®¤prompt
            if prompt is None:
                prompt = getattr(self, 'default_prompt', "A high resolution satellite image")
                
            # å°†å•ä¸ªæç¤ºè½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(prompt, str):
                prompt = [prompt] * batch_size
            elif len(prompt) == 1 and batch_size > 1:
                prompt = prompt * batch_size
                
            # ç¼–ç æ–‡æœ¬æç¤º
            text_embeds = self.encode_prompt(prompt)
        
        # ç”Ÿæˆæ—¶é—´æ­¥å’Œå™ªå£°
        if self.training:
            timesteps = torch.randint(100, 999, (batch_size,), device=device, dtype=torch.long)
        else:
            timesteps = torch.full((batch_size,), self.inference_timestep, device=device, dtype=torch.long)
        
        if self.training and gt is not None:
            gt_residual = gt - lms
            noise = torch.randn_like(gt_residual)
            x_t = self.diffusion.q_sample_xt(gt_residual, timesteps, noise=noise)
        else:
            x_t = torch.zeros_like(lms)
        
        # ControlNetï¼šæå–å¤šå°ºåº¦ç©ºé—´ç‰¹å¾ -> control_features
        control_features = None
        if self.controlnet_full is not None:
            control_features = self.controlnet_full(pan, x_t, timesteps)
            
            # åœ¨forwardä¸­ä¹Ÿç›‘æ§ ControlNet ç‰¹å¾ï¼ˆæ¯500æ­¥ï¼‰
            if not self.training and hasattr(self, 'current_step') and self.current_step % 500 == 0:
                print(f"\n[ControlNet ç‰¹å¾ç›‘æ§ - Forward]")
                print(f"{'å±‚çº§':<10} {'Mean':<12} {'Std':<12} {'Abs Mean':<12} {'Shape'}")
                print("-" * 70)
                for level_name, features in control_features.items():
                    mean_val = features.mean().item()
                    std_val = features.std().item()
                    abs_mean_val = features.abs().mean().item()
                    shape_str = 'x'.join(map(str, features.shape[1:]))
                    print(f"{level_name:<10} {mean_val:>11.6f} {std_val:>11.6f} {abs_mean_val:>11.6f} [{shape_str}]")
        
        # UNetå‰å‘ä¼ æ’­ï¼ˆå¯ä»¥åŒæ—¶ä½¿ç”¨scene_tokenã€control_featureså’Œtext_embedsï¼‰
        residual_pred = self.unet.forward_impl(
            lms, pan, ms, x_t, timesteps, 
            scene_token=vae_features,      # VAEçš„å…¨å±€ç‰¹å¾
            control_features=control_features,  # ControlNetçš„ç©ºé—´ç‰¹å¾
            encoder_hidden_states=text_embeds,  # CLIPçš„æ–‡æœ¬ç‰¹å¾
            epoch=self.current_step
        )
        
        if torch.isnan(residual_pred).any():
            residual_pred = torch.nan_to_num(residual_pred, nan=0.0)
        
        if self.args.predict_xstart:
            residual = residual_pred
        else:
            residual = self.diffusion._predict_xstart_from_eps(x_t, timesteps, residual_pred)
        
        output = lms + residual
        output = output.clamp(0, 1)
        
        return output, residual_pred, vae_features
    
    def l1_loss(self, lms, pan, ms, gt, prompt=None):
        batch_size = lms.shape[0]
        device = lms.device
        
        # VAEç¼–ç å™¨ï¼šæå–å…¨å±€åœºæ™¯ç‰¹å¾
        vae_features = None
        mu = None
        logvar = None
        if self.vae_encoder is not None:
            mu, logvar = self.vae_encoder(pan)
            vae_features = mu
        
        # CLIPæ–‡æœ¬ç¼–ç å™¨ï¼šæå–æ–‡æœ¬è¯­ä¹‰ç‰¹å¾
        text_embeds = None
        if self.use_clip and self.text_encoder is not None:
            # å¦‚æœå¯ç”¨RAMä¸”æ²¡æœ‰æä¾›promptï¼Œä¸ºå½“å‰batchç”Ÿæˆcaption
            if prompt is None and self.use_ram and self.ram_caption_generator is not None:
                with torch.no_grad():
                    captions = []
                    for i in range(batch_size):
                        pan_i = pan[i]  # ä½¿ç”¨PANå›¾åƒç”Ÿæˆcaption
                        caption_raw = self.ram_caption_generator.generate_caption(pan_i)
                        clip_labels = self.select_clip_semantic_label_with_color_debiasing(caption_raw)
                        caption_filtered = ', '.join(clip_labels)
                        captions.append(caption_filtered)
                    prompt = captions
            
            # å¦‚æœä»ç„¶æ²¡æœ‰promptï¼Œä½¿ç”¨é»˜è®¤prompt
            if prompt is None:
                prompt = getattr(self, 'default_prompt', "A high resolution satellite image")
            
            # å°†å•ä¸ªæç¤ºè½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(prompt, str):
                prompt = [prompt] * batch_size
            elif len(prompt) == 1 and batch_size > 1:
                prompt = prompt * batch_size
                
            # ç¼–ç æ–‡æœ¬æç¤º
            text_embeds = self.encode_prompt(prompt)
        
        timesteps = torch.randint(100, 999, (batch_size,), device=device, dtype=torch.long)
        
        gt_residual = gt - lms
        noise = torch.randn_like(gt_residual)
        x_t = self.diffusion.q_sample_xt(gt_residual, timesteps, noise=noise)
        
        # ControlNetï¼šæå–å¤šå°ºåº¦ç©ºé—´ç‰¹å¾
        control_features = None
        if self.controlnet_full is not None:
            control_features = self.controlnet_full(pan, x_t, timesteps)
        
        # UNetå‰å‘ä¼ æ’­
        residual_pred = self.unet.forward_impl(
            lms, pan, ms, x_t, timesteps, 
            scene_token=vae_features,      # VAEçš„å…¨å±€ç‰¹å¾
            control_features=control_features,  # ControlNetçš„ç©ºé—´ç‰¹å¾
            encoder_hidden_states=text_embeds,  # CLIPçš„æ–‡æœ¬ç‰¹å¾
            epoch=self.current_step
        )
        
        if self.args.predict_xstart:
            residual_final = residual_pred
        else:
            residual_final = self.diffusion._predict_xstart_from_eps(x_t, timesteps, residual_pred)
        
        # è®¡ç®—L1é‡å»ºæŸå¤±
        l1_loss = F.l1_loss(residual_final, gt_residual, reduction="mean")
        
        # åˆå§‹åŒ–æ€»æŸå¤±ä¸ºL1æŸå¤±
        total_loss = l1_loss
        
        # æŸå¤±æ—¥å¿—
        loss_dict = {
            'l1_loss': l1_loss.item(),
            'total_loss': total_loss.item(),
        }
        
        # æ·»åŠ KLæ•£åº¦æŸå¤±
        kl_loss = 0.0
        if self.use_kl_loss and mu is not None and logvar is not None:
            # KLæ•£åº¦: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size
            total_loss = total_loss + self.lambda_kl * kl_loss
            loss_dict['kl_loss'] = kl_loss.item()
            loss_dict['total_loss'] = total_loss.item()
        
            # æ·»åŠ æ„ŸçŸ¥æŸå¤±
            perceptual_loss = 0.0
            if self.use_perceptual_loss and hasattr(self, 'perceptual_loss'):
                try:
     
                    pred_image = lms + residual_final
          
                    pred_image = pred_image.clamp(0, 1)
                    gt = gt.clamp(0, 1)
                    
          
                    perceptual_loss = self.perceptual_loss(pred_image, gt)
                    
             
                    if perceptual_loss > 5.0:
                        perceptual_loss = torch.log(perceptual_loss + 1.0)
                        
     
                    weighted_perceptual_loss = self.lambda_perceptual * perceptual_loss
                    total_loss = total_loss + weighted_perceptual_loss
                    loss_dict['perceptual_loss'] = perceptual_loss.item()
                    loss_dict['weighted_perceptual_loss'] = weighted_perceptual_loss.item()
                    loss_dict['total_loss'] = total_loss.item()
                except Exception as e:
                    print(f"è®¡ç®—æ„ŸçŸ¥æŸå¤±æ—¶å‡ºé”™: {e}")
                    # å‡ºé”™æ—¶ä¸æ·»åŠ æ„ŸçŸ¥æŸå¤±
                    loss_dict['perceptual_loss_error'] = str(e)
        if text_embeds is not None:
            clip_var = text_embeds.var(dim=-1).mean()
            clip_loss = 1e-3 * (clip_var - 1.0).pow(2)   # è®©æ–¹å·®æ¥è¿‘ 1
            total_loss = total_loss + clip_loss
            loss_dict['clip_loss'] = clip_loss.item()
        # æ¯100æ­¥æ‰“å°æŸå¤±è¯¦æƒ…
        if hasattr(self, 'current_step') and self.current_step % 500 == 0:
            print(f"\nğŸ“Š æŸå¤±è¯¦æƒ… (æ­¥æ•° {self.current_step}):")
            print(f"   L1æŸå¤±: {l1_loss.item():.6f}")
            
            if self.use_kl_loss and 'kl_loss' in loss_dict:
                kl_value = loss_dict['kl_loss']
                print(f"   KLæ•£åº¦æŸå¤±: {kl_value:.6f} (Î»={self.lambda_kl})")
                
            if self.use_perceptual_loss and 'perceptual_loss' in loss_dict:
                p_value = loss_dict['perceptual_loss']
                wp_value = loss_dict.get('weighted_perceptual_loss', p_value * self.lambda_perceptual)
                print(f"   æ„ŸçŸ¥æŸå¤±: {p_value:.6f} (åŸå§‹)")
                print(f"   åŠ æƒæ„ŸçŸ¥æŸå¤±: {wp_value:.6f} (Î»={self.lambda_perceptual})")
                
            if 'cliap_loss' in loss_dict:
                print(f"   clip_loss   : {loss_dict['clip_loss']}")
                
            print(f"   æ€»æŸå¤±: {total_loss.item():.6f}")
            
            
            if text_embeds is not None:
                mean_norm = text_embeds.norm(dim=-1).mean().item()
                cos_sim = F.cosine_similarity(text_embeds[:-1], text_embeds[1:]).mean().item()
                print(f"\n[CLIP monitor] mean_norm={mean_norm:.3f}, cos_sim={cos_sim:.3f}")
            
            # ç›‘æ§ ControlNet æ³¨å…¥ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
            if control_features is not None:
                print(f"\n[ControlNet ç‰¹å¾ç›‘æ§]")
                print(f"{'å±‚çº§':<10} {'Mean':<12} {'Std':<12} {'Abs Mean':<12} {'Shape'}")
                print("-" * 70)
                for level_name, features in control_features.items():
                    mean_val = features.mean().item()
                    std_val = features.std().item()
                    abs_mean_val = features.abs().mean().item()
                    shape_str = 'x'.join(map(str, features.shape[1:]))  # ä¸æ˜¾ç¤ºbatchç»´åº¦
                    print(f"{level_name:<10} {mean_val:>11.6f} {std_val:>11.6f} {abs_mean_val:>11.6f} [{shape_str}]")
                
                # è®¡ç®—æ•´ä½“ç»Ÿè®¡
                all_means = [f.mean().item() for f in control_features.values()]
                all_stds = [f.std().item() for f in control_features.values()]
                all_abs_means = [f.abs().mean().item() for f in control_features.values()]
                print("-" * 70)
                print(f"{'Overall':<10} {sum(all_means)/len(all_means):>11.6f} {sum(all_stds)/len(all_stds):>11.6f} {sum(all_abs_means)/len(all_abs_means):>11.6f} [avg]")
                print("")
        
        return total_loss, vae_features, loss_dict
    
    def save_model(self, save_path):
        state_dict = {
            'lora_target_modules': self.lora_target_modules,
            'lora_rank': self.lora_rank,
            'unet_state_dict': {},
            'vae_state_dict': {},
            'controlnet_state_dict': {},
            'clip_state_dict': {},
        }
        
        # ä¿å­˜LoRAå‚æ•° + é—¨æ§å‚æ•° + CLIPæŠ•å½±å±‚
        saved_counts = {'lora': 0, 'text_gate': 0, 'scene_gate': 0, 'control_gate': 0, 'proj': 0}
        print(f"\n{'='*70}")
        print(f"ä¿å­˜UNetå‚æ•°:")
        print(f"{'='*70}")
        for name, param in self.unet.named_parameters():
            # LoRAå‚æ•°
            if 'lora' in name:
                state_dict['unet_state_dict'][name] = param.cpu()
                saved_counts['lora'] += 1
            # é—¨æ§å‚æ•°ï¼ˆtext_gate, scene_gate, control_gateç­‰ï¼‰
            elif 'gate' in name:
                state_dict['unet_state_dict'][name] = param.cpu()
                if 'text_gate' in name:
                    saved_counts['text_gate'] += 1
                    print(f"  CLIPé—¨æ§: {name} = {param.item():.6f}")
                elif 'scene_gate' in name:
                    saved_counts['scene_gate'] += 1
                    print(f"  Sceneé—¨æ§: {name} = {param.item():.6f}")
                elif 'control_gate' in name:
                    saved_counts['control_gate'] += 1
                    print(f"  Controlé—¨æ§: {name} = {param.item():.6f}")
            elif 'text_proj' in name:
                state_dict['unet_state_dict'][name] = param.cpu()
                saved_counts['proj'] += 1
                print(f"  CLIPæŠ•å½±å±‚: {name}, shape={param.shape}")
        
        print(f"\n{'='*70}")
        print(f"ä¿å­˜ç»Ÿè®¡:")
        print(f"{'='*70}")
        print(f"  âœ… LoRAå‚æ•°: {saved_counts['lora']} ä¸ª")
        total_gates = saved_counts['text_gate'] + saved_counts['scene_gate'] + saved_counts['control_gate']
        if total_gates > 0:
            print(f"  âœ… é—¨æ§å‚æ•°: {total_gates} ä¸ª (Text={saved_counts['text_gate']}, Scene={saved_counts['scene_gate']}, Control={saved_counts['control_gate']})")
        if saved_counts['proj'] > 0:
            print(f"  âœ… æŠ•å½±å±‚å‚æ•°: {saved_counts['proj']} ä¸ª")
        print(f"  ğŸ“¦ æ€»å‚æ•°: {len(state_dict['unet_state_dict'])} ä¸ª")
        print(f"{'='*70}")
        
        # ä¿å­˜VAEå‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.vae_encoder is not None:
            state_dict['vae_state_dict'] = self.vae_encoder.state_dict()
        
        # ä¿å­˜å®Œæ•´ControlNetå‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.controlnet_full is not None:
            state_dict['controlnet_state_dict'] = self.controlnet_full.state_dict()
            # æ˜¾ç¤ºControlNetçš„gateå‚æ•°å€¼
            print(f"\nControlNetå‚æ•°:")
            controlnet_gate_count = 0
            for name, param in self.controlnet_full.named_parameters():
                if 'control_gate' in name:
                    controlnet_gate_count += 1
                    print(f"   ControlNeté—¨æ§: {name} = {param.item():.6f}")
            print(f" ControlNetæ€»å‚æ•°: {len(state_dict['controlnet_state_dict'])}, å…¶ä¸­gateå‚æ•°: {controlnet_gate_count}")
            
        # ä¿å­˜CLIPç›¸å…³å‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_clip:
            state_dict['use_clip'] = True
            state_dict['default_prompt'] = getattr(self, 'default_prompt', "A high resolution satellite image")
            
            # å¦‚æœCLIPæ¨¡å‹è¢«å¾®è°ƒäº†ï¼Œä¿å­˜å…¶çŠ¶æ€
            if self.text_encoder is not None and any(p.requires_grad for p in self.text_encoder.parameters()):
                state_dict['clip_state_dict'] = self.text_encoder.state_dict()
        
        torch.save(state_dict, save_path)


class SSDiff_VAE_test(nn.Module):
    """æµ‹è¯•/æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒVAEå’ŒControlNet"""
    def __init__(self, args, use_multi_gpu=False):
        super().__init__()
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._is_data_parallel = False  # å§‹ç»ˆå•å¡
        print(f"å½“å‰è¿è¡Œè®¾å¤‡: {self.device}")

        print(args.use_clip)
        if args.use_distillation:
            original_respacing = args.timestep_respacing
            args.timestep_respacing = ""
        
        self.model, self.diffusion = create_model_and_diffusion(
            **args_to_dict(args, model_and_diffusion_defaults().keys())
        )
        
        if args.use_distillation:
            args.timestep_respacing = original_respacing
        
        # åŠ è½½æ¨¡å‹æƒé‡
        if hasattr(args, 'model_path') and args.model_path:
            state_dict = torch.load(args.model_path, map_location='cpu')
            
            if args.use_distillation and 'unet_state_dict' in state_dict:
                if hasattr(args, 'pretrained_ssdiff_path'):
                    base_state = torch.load(args.pretrained_ssdiff_path, map_location='cpu')
                    missing_keys, _ = self.model.load_state_dict(base_state, strict=False)
                
                if 'lora_target_modules' in state_dict:
                    if 'lora_rank' in state_dict:
                        args.lora_rank = state_dict['lora_rank']
                    else:
                        args.lora_rank = 4
                    
                    self.model, _ = initialize_ssdiff_unet_with_lora(
                        args, pretrained_path=args.pretrained_ssdiff_path
                    )
                    
                    # åŠ è½½LoRAã€é—¨æ§å‚æ•°å’ŒCLIPæŠ•å½±å±‚
                    loaded_counts = {'lora': 0, 'text_gate': 0, 'scene_gate': 0, 'control_gate': 0, 'proj': 0}
                    print(f"\n{'='*70}")
                    print(f"åŠ è½½UNetå‚æ•°:")
                    print(f"{'='*70}")
                    for name, param in self.model.named_parameters():
                        if name in state_dict['unet_state_dict']:
                            param.data.copy_(state_dict['unet_state_dict'][name])
                            if 'lora' in name:
                                loaded_counts['lora'] += 1
                            elif 'text_gate' in name:
                                loaded_counts['text_gate'] += 1
                                print(f"  CLIPé—¨æ§: {name} = {param.item():.6f}")
                            elif 'scene_gate' in name:
                                loaded_counts['scene_gate'] += 1
                                print(f"  Sceneé—¨æ§: {name} = {param.item():.6f}")
                            elif 'control_gate' in name:
                                loaded_counts['control_gate'] += 1
                                print(f"  Controlé—¨æ§: {name} = {param.item():.6f}")
                            elif 'text_proj' in name:
                                loaded_counts['proj'] += 1
                                print(f"  CLIPæŠ•å½±å±‚: {name}, shape={param.shape}")
                    
                    print(f"\n{'='*70}")
                    print(f"åŠ è½½ç»Ÿè®¡:")
                    print(f"{'='*70}")
                    print(f"  âœ… LoRAå‚æ•°: {loaded_counts['lora']} ä¸ª")
                    total_gates = loaded_counts['text_gate'] + loaded_counts['scene_gate'] + loaded_counts['control_gate']
                    if total_gates > 0:
                        print(f"  âœ… é—¨æ§å‚æ•°: {total_gates} ä¸ª (Text={loaded_counts['text_gate']}, Scene={loaded_counts['scene_gate']}, Control={loaded_counts['control_gate']})")
                    if loaded_counts['proj'] > 0:
                        print(f"  âœ… æŠ•å½±å±‚å‚æ•°: {loaded_counts['proj']} ä¸ª")
                    print(f"{'='*70}\n")
            else:
                missing_keys, _ = self.model.load_state_dict(state_dict, strict=False)
        
        #  ä¸»æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        self.model.to(self.device)
        self.model.set_epoch(10001)
        self.model.eval()
        
# ç›´æ¥åˆå§‹åŒ–å¤§å‹æ¨¡å—ï¼ˆå•GPUä¸éœ€è¦å»¶è¿Ÿï¼‰
        self.vae_encoder = None
        self.controlnet_full = None

# åˆå§‹åŒ–VAEç¼–ç å™¨
        if getattr(args, "use_vae", False):
            print("åˆå§‹åŒ–VAEç¼–ç å™¨...")
            self.vae_encoder = VAEEncoder(
                input_channels=1,
            latent_dim=getattr(args, "vae_latent_dim", 256)
            ).to(self.device).eval()
            
            if hasattr(args, "model_path") and args.model_path:
                state_dict = torch.load(args.model_path, map_location="cpu")
                if "vae_state_dict" in state_dict:
                    self.vae_encoder.load_state_dict(state_dict["vae_state_dict"])
                    print("âœ“ å·²åŠ è½½VAEæƒé‡")
                    
            if hasattr(args, "model_path") and args.model_path:
                state_dict = torch.load(args.model_path, map_location="cpu")
                if "vae_state_dict" in state_dict:
                    self.vae_encoder.load_state_dict(state_dict["vae_state_dict"])
                print("âœ“ å·²åŠ è½½VAEæƒé‡")

        # åˆå§‹åŒ–ControlNet
        if getattr(args, "use_controlnet", False):
            print("\nåˆå§‹åŒ–ControlNet...")
            # åœ¨æµ‹è¯•æ—¶ï¼Œverbose=Falseé¿å…è¾“å‡ºè¿‡å¤šåˆå§‹åŒ–ä¿¡æ¯
            # å› ä¸ºæƒé‡ä¼šä»checkpointåŠ è½½è€Œä¸æ˜¯ä½¿ç”¨åˆå§‹åŒ–çš„æƒé‡
            self.controlnet_full = ControlNet(
                unet_model=self.model,
                ms_dim=getattr(args, "ms_channels", 8),
                pan_dim=1,
                copy_weights=True,  # ä¿æŒå¤åˆ¶æƒé‡ï¼ˆä½œä¸ºbackupï¼‰
                verbose=False  # æµ‹è¯•æ—¶ä¸è¾“å‡ºè¯¦ç»†çš„å¤åˆ¶ä¿¡æ¯
            ).to(self.device).eval()
            if hasattr(args, "model_path") and args.model_path:
                state_dict = torch.load(args.model_path, map_location="cpu")
                if "controlnet_state_dict" in state_dict:
                    self.controlnet_full.load_state_dict(state_dict["controlnet_state_dict"])
                    print("âœ“ å·²ä»checkpointåŠ è½½ControlNetè®­ç»ƒæƒé‡")
                    # æ˜¾ç¤ºåŠ è½½çš„gateå‚æ•°å€¼
                    controlnet_gate_count = 0
                    for name, param in self.controlnet_full.named_parameters():
                        if 'control_gate' in name:
                            controlnet_gate_count += 1
                            print(f" ControlNeté—¨æ§: {name} = {param.item():.6f}")
                    if controlnet_gate_count > 0:
                        print(f"  ğŸ‰ æˆåŠŸåŠ è½½ {controlnet_gate_count} ä¸ªControlNeté—¨æ§å‚æ•°")
        # åˆå§‹åŒ–CLIPæ–‡æœ¬ç¼–ç å™¨ï¼ˆç‹¬ç«‹ï¼‰
        self.tokenizer = None
        self.text_encoder = None
        self.use_clip = False
        self.use_ram = False  #  æ–°å¢ï¼šRAMæ”¯æŒ
        self.ram_caption_generator = None
        if hasattr(args, 'use_clip') and args.use_clip:
            # ä½¿ç”¨SD21Baseä½œä¸ºCLIPåŸºç¡€æ¨¡å‹è·¯å¾„
            clip_base_path = getattr(args, 'clip_model_path', None)
            if clip_base_path is None:
                # é»˜è®¤ä½¿ç”¨SD21Baseè·¯å¾„
                import os
                script_dir = os.path.dirname(os.path.abspath(__file__))
                clip_base_path = os.path.join(script_dir, "model", "SD21Base")
            
            print(f"æ­£åœ¨ä»SD21BaseåŠ è½½CLIPæ¨¡å‹: {clip_base_path}")
            try:
                # ä»SD21BaseåŠ è½½tokenizerå’Œtext_encoderï¼ˆé¢„è®­ç»ƒæƒé‡ï¼‰
                self.tokenizer = AutoTokenizer.from_pretrained(
                    clip_base_path, subfolder="tokenizer"
                )
                self.text_encoder = CLIPTextModel.from_pretrained(
                    clip_base_path, subfolder="text_encoder"
                ).to(self.device)
                self.text_encoder.eval()
                
                self.use_clip = True
                # å°è¯•ä»checkpointåŠ è½½ä¿å­˜çš„prompté…ç½®ï¼ˆä¸åŠ è½½æƒé‡ï¼Œå› ä¸ºCLIPåœ¨è®­ç»ƒæ—¶æ˜¯å†»ç»“çš„ï¼‰
                
                print(f"  - æ³¨æ„: CLIPåœ¨è®­ç»ƒæ—¶æ˜¯å†»ç»“çš„ï¼Œä½¿ç”¨SD21Baseé¢„è®­ç»ƒæƒé‡")
                    
            except Exception as e:
                print(f"âš  CLIPæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                self.use_clip = False
                self.tokenizer = None
                self.text_encoder = None
        
        #  RAM Captionç”Ÿæˆå™¨ï¼ˆç”¨äºä»PANå›¾åƒè‡ªåŠ¨ç”Ÿæˆcaptionï¼‰
        if hasattr(args, 'use_ram') and args.use_ram:
            self.use_ram = True
            ram_model_path = getattr(args, 'ram_model_path', None)
            
            try:
                # =============================
                # åˆå§‹åŒ– RAM caption ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰å°è£…ï¼‰
                # =============================
                device = self.device

                # åˆå§‹åŒ–ç”Ÿæˆå™¨
                self.ram_caption_generator = RAMCaptionGenerator(
                    model_path=ram_model_path,
                    device=device
                )

                print(f"âœ“ å·²å¯ç”¨RAM Captionç”Ÿæˆå™¨ï¼ˆSwin-Large æ¨¡å‹ï¼‰")
                print(f"  - è‡ªåŠ¨ä»PANå›¾åƒç”Ÿæˆcaption")
                print(f"  - æ¨¡å‹è·¯å¾„: {ram_model_path}")

            except Exception as e:
                print(f"âš  RAMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.use_ram = False
                self.ram_caption_generator = None
    
    def _build_color_subspace(self):
        """
        æ„å»ºé¢œè‰²å­ç©ºé—´çš„æ­£äº¤åŸºï¼ˆç”¨äºé¢œè‰²å»åï¼‰
        Returns:
            Q: é¢œè‰²å­ç©ºé—´çš„æ­£äº¤åŸº [D, K]ï¼Œå…¶ä¸­Kæ˜¯é¢œè‰²è¯æ•°é‡
        """
        color_words = [
            "green", "blue", "red", "brown", "yellow", "white", "gray", "grey",
            "black", "purple", "pink", "orange", "cyan", "magenta",
            "light", "dark", "pale", "bright", "deep"
        ]
        
        device = next(self.text_encoder.parameters()).device
        
        # ç¼–ç é¢œè‰²è¯
        with torch.no_grad():
            color_embeds_list = []
            for color in color_words:
                text_input = self.tokenizer(
                    color,
                    padding="max_length",
                    max_length=self.tokenizer.model_max_length,
                    truncation=True,
                    return_tensors="pt"
                ).to(device)
                
                color_embed = self.text_encoder(text_input.input_ids)[0]  # [1, 77, 768]
                color_embed = color_embed.mean(dim=1)  # [1, 768]
                color_embeds_list.append(color_embed)
            
            # å †å æ‰€æœ‰é¢œè‰²embedding: [K, D]
            color_embeds = torch.cat(color_embeds_list, dim=0)
            # å½’ä¸€åŒ–
            color_embeds = F.normalize(color_embeds, dim=-1)
            
            # QRåˆ†è§£æ„å»ºæ­£äº¤åŸº: Q @ R = color_embeds.T
            Q, _ = torch.linalg.qr(color_embeds.T)  # Q: [D, K]
        
        return Q
    
    def _apply_color_debiasing(self, text_embeds, Q):
        """
        å¯¹text embeddingåº”ç”¨é¢œè‰²å­ç©ºé—´å»å
        Args:
            text_embeds: CLIP text embeddings [B, D]
            Q: é¢œè‰²å­ç©ºé—´çš„æ­£äº¤åŸº [D, K]
        Returns:
            debiased_embeds: å»é™¤é¢œè‰²åˆ†é‡åçš„embeddings [B, D]
        """
        # æŠ•å½±åˆ°é¢œè‰²å­ç©ºé—´: proj_color = (Q @ Q.T) @ text_embeds.T
        proj_color = Q @ (Q.T @ text_embeds.T)  # [D, B]
        
        # å»é™¤é¢œè‰²åˆ†é‡
        text_debiased = text_embeds.T - proj_color  # [D, B]
        text_debiased = text_debiased.T  # [B, D]
        
        # å½’ä¸€åŒ–åˆ°unit sphere
        text_debiased = F.normalize(text_debiased, dim=-1)
        
        return text_debiased
    
    def select_clip_semantic_label(self, ram_caption, top_k=3, max_ram_tags=12):
        """
        ä½¿ç”¨CLIPå¯¹RAMç”Ÿæˆçš„captionè¿›è¡Œè¯­ä¹‰è¿‡æ»¤ï¼ˆæ ‡å‡†ç‰ˆæœ¬ï¼‰
        Args:
            ram_caption: RAMç”Ÿæˆçš„åŸå§‹captionï¼ˆé€—å·åˆ†éš”çš„æ ‡ç­¾åˆ—è¡¨ï¼‰
            top_k: é€‰æ‹©å‰kä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾ï¼ˆé»˜è®¤3ï¼‰
            max_ram_tags: RAMè¾“å‡ºä¸­ä¿ç•™çš„æœ€å¤§æ ‡ç­¾æ•°ï¼ˆé¿å…è¿‡é•¿å¯¼è‡´æˆªæ–­ï¼‰
        Returns:
            topk_labels: è¿‡æ»¤åçš„æ ‡ç­¾åˆ—è¡¨
        """
        labels = [
            "urban area", "residential zone", "industrial zone", "forest", "grassland",
            "mountain", "river", "lake", "sea", "beach", "harbor", "farmland",
            "road network", "bare soil", "wetland", "desert",
            "bridge", "building", "highway", "stadium",
            "coastline", "cliff", "hill slope", "dense vegetation", "open water",
            "cloudy", "coastal city", "harbor town"
        ]
        
        # é¢„å¤„ç†RAMçš„è¾“å‡ºï¼šRAMæŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½è¾“å‡ºæ ‡ç­¾ï¼Œæˆ‘ä»¬åªä¿ç•™å‰Nä¸ª
        if isinstance(ram_caption, str):
            ram_tags = [tag.strip() for tag in ram_caption.split(',')]
            ram_tags = ram_tags[:max_ram_tags]
            ram_caption_short = ', '.join(ram_tags)
        else:
            ram_caption_short = ram_caption
        
        # å°†RAMè¾“å‡ºä¸æ ‡ç­¾ä¸€å¹¶tokenize
        device = self.device
        text_inputs = self.tokenizer(
            labels + [ram_caption_short],
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            # æå–æ–‡æœ¬ç‰¹å¾
            text_embeds = self.text_encoder(text_inputs.input_ids)[0]  # [len(labels)+1, 77, 768]
            text_embeds = text_embeds.mean(dim=1)  # å…¨å±€å¹³å‡æ± åŒ– -> [len(labels)+1, 768]
            text_embeds = text_embeds / (text_embeds.norm(dim=-1, keepdim=True) + 1e-6)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼šlabelsä¸ram_captionçš„ç›¸ä¼¼åº¦
            label_embeds = text_embeds[:-1]  # [num_labels, 768]
            caption_embed = text_embeds[-1:]  # [1, 768]
            scores = (label_embeds * caption_embed).sum(dim=-1)  # [num_labels]
            
        # å–å‰ top_k ä¸ªæ ‡ç­¾
        topk_idx = torch.topk(scores, k=min(top_k, len(labels))).indices.cpu().numpy()
        topk_labels = [labels[i] for i in topk_idx]
        
        return topk_labels
    
    def select_clip_semantic_label_with_color_debiasing(self, ram_caption, top_k=5):
        """
        å¯¹RAMç”Ÿæˆçš„captionè¿›è¡Œé¢œè‰²å»åï¼Œç›´æ¥è¿”å›å»ååçš„æ ‡ç­¾
        
        å®ç°åŸç†ï¼ˆColor Debiasingï¼‰ï¼š
        1. æ„å»ºé¢œè‰²è¯çš„CLIP embeddingæ„æˆçš„é¢œè‰²å­ç©ºé—´
        2. é€šè¿‡QRåˆ†è§£å¾—åˆ°é¢œè‰²å­ç©ºé—´çš„æ­£äº¤åŸº Q
        3. å¯¹RAMæ¯ä¸ªæ ‡ç­¾çš„embeddingå»é™¤å…¶åœ¨é¢œè‰²å­ç©ºé—´çš„æŠ•å½±ï¼še' = e - Q @ (Q.T @ e)
        4. æ ¹æ®å»ååçš„embeddingä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä¿ç•™æœ€ä¸ç›¸å…³é¢œè‰²çš„å‰kä¸ªæ ‡ç­¾
        5. è¿”å›å»ååçš„æ ‡ç­¾ï¼ˆä¿æŒåŸå§‹æ–‡æœ¬ï¼‰
        
        Args:
            ram_caption: RAMç”Ÿæˆçš„åŸå§‹captionï¼ˆé€—å·åˆ†éš”çš„æ ‡ç­¾åˆ—è¡¨ï¼‰
            top_k: è¿”å›å‰kä¸ªæ ‡ç­¾ï¼ˆé»˜è®¤5ï¼‰
        Returns:
            debiased_labels: å»ååçš„å‰kä¸ªæ ‡ç­¾åˆ—è¡¨
        """
        # è§£æRAMè¾“å‡º
        if isinstance(ram_caption, str):
            ram_tags = [tag.strip() for tag in ram_caption.split(',')]
        else:
            ram_tags = ram_caption
        
        # å¦‚æœRAMè¾“å‡ºå°‘äºç­‰äºtop_kä¸ªï¼Œç›´æ¥è¿”å›å…¨éƒ¨
        if len(ram_tags) <= top_k:
            return ram_tags[:top_k]
        
        # ç¼–ç æ‰€æœ‰RAMæ ‡ç­¾
        device = self.device
        text_inputs = self.tokenizer(
            ram_tags,
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            # æå–æ–‡æœ¬ç‰¹å¾
            text_embeds = self.text_encoder(text_inputs.input_ids)[0]  # [num_tags, 77, 768]
            text_embeds = text_embeds.mean(dim=1)  # å…¨å±€å¹³å‡æ± åŒ– -> [num_tags, 768]
            text_embeds = F.normalize(text_embeds, dim=-1)
            
            # æ„å»ºé¢œè‰²å­ç©ºé—´
            Q = self._build_color_subspace()  # [768, K]
            
            # å¯¹æ¯ä¸ªæ ‡ç­¾è¿›è¡Œé¢œè‰²å»å
            tag_embeds_debiased = self._apply_color_debiasing(text_embeds, Q)  # [num_tags, 768]
            
            # è®¡ç®—æ¯ä¸ªæ ‡ç­¾å»åå‰åçš„ç›¸ä¼¼åº¦ï¼ˆç›¸ä¼¼åº¦è¶Šä½è¯´æ˜é¢œè‰²æˆåˆ†è¶Šå¤šï¼‰
            color_scores = (text_embeds * tag_embeds_debiased).sum(dim=-1)  # [num_tags]
            
            # é€‰æ‹©é¢œè‰²æˆåˆ†æœ€å°‘çš„å‰kä¸ªæ ‡ç­¾ï¼ˆç›¸ä¼¼åº¦æœ€é«˜çš„ï¼‰
            # æˆ–è€…ç›´æ¥æŒ‰åŸå§‹é¡ºåºè¿”å›å‰kä¸ª
            topk_idx = torch.topk(color_scores, k=min(top_k, len(ram_tags))).indices.cpu().numpy()
            
        # è¿”å›å»ååçš„æ ‡ç­¾ï¼ˆä¿æŒåŸå§‹æ–‡æœ¬ï¼‰
        debiased_labels = [ram_tags[i] for i in topk_idx]
        
        return debiased_labels
        
    def encode_prompt(self, prompt_batch):
        """
        ç¼–ç æ–‡æœ¬æç¤ºä¸ºæ–‡æœ¬åµŒå…¥
        Args:
            prompt_batch: æ–‡æœ¬æç¤ºåˆ—è¡¨
        Returns:
            prompt_embeds: æ–‡æœ¬åµŒå…¥ [B, 77, 768]
        """
        print("[DEBUG: encode_prompt]")
        if hasattr(self, "ram_caption_generator") and self.ram_caption_generator is not None:
            print("RAM Caption Generator å·²åŠ è½½ï¼Œå½“å‰æ‰¹æ¬¡è¾“å…¥:")
            for i, p in enumerate(prompt_batch[:3]):
                print(f"  [{i}] prompt: {p}")
        if not self.use_clip or self.tokenizer is None or self.text_encoder is None:
            print("[DEBUG: encode_prompt]")
            print("  self.use_clip:", self.use_clip)
            print("  self.tokenizer:", type(self.tokenizer))
            print("  self.text_encoder:", type(self.text_encoder))
            print("  self.ram_caption_generator:", type(self.ram_caption_generator))
            return None
            
        prompt_embeds_list = []
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼Œç¡®è®¤ä¸è®­ç»ƒ
            for caption in prompt_batch:
                text_input_ids = self.tokenizer(
                    caption, 
                    max_length=self.tokenizer.model_max_length,
                    padding="max_length", 
                    truncation=True, 
                    return_tensors="pt"
                ).input_ids
                prompt_embeds = self.text_encoder(
                    text_input_ids.to(self.text_encoder.device)
                )[0]
                prompt_embeds_list.append(prompt_embeds)
        
        if prompt_embeds_list:
            prompt_embeds = torch.concat(prompt_embeds_list, dim=0)
            prompt_embeds = F.layer_norm(prompt_embeds, prompt_embeds.shape[-1:])
            return prompt_embeds
        return None
    
    @torch.no_grad()
    def forward(self, lms, pan, ms, prompt=None):
        self.model.training = False
        self.model.eval()
        
        lms = lms.to(self.device)
        pan = pan.to(self.device)
        ms = ms.to(self.device)
        
        # å¤„ç†æ–‡æœ¬æç¤º
        text_embeds = None
        batch_size = lms.shape[0]
        if self.use_clip and self.text_encoder is not None:
            # å¦‚æœå¯ç”¨RAMï¼Œä¸ºå½“å‰batchçš„æ¯å¼ å›¾åƒç”Ÿæˆcaption
            if self.use_ram and self.ram_caption_generator is not None:
                with torch.no_grad():
                    captions = []
                    for i in range(batch_size):
                        pan_i = pan[i]  # ä½¿ç”¨PANå›¾åƒç”Ÿæˆcaption
                        caption_raw = self.ram_caption_generator.generate_caption(pan_i)
                        clip_labels = self.select_clip_semantic_label(caption_raw)
                        caption_filtered = ', '.join(clip_labels)
                        captions.append(caption_filtered)
                    prompt = captions
                    print(f"[DEBUG] ä¸ºå½“å‰batchç”Ÿæˆçš„captions: {prompt[:3] if len(prompt) > 3 else prompt}")
            elif prompt is None:
                # å¦‚æœæ²¡æœ‰æä¾›promptä¸”æ²¡æœ‰å¯ç”¨RAMï¼Œä½¿ç”¨é»˜è®¤prompt
                prompt = [self.default_prompt] * batch_size
                
            # ç¼–ç æ–‡æœ¬æç¤º
            text_embeds = self.encode_prompt(prompt)
        
        model_kwargs = {"lms": lms, "pan": pan, "ms": ms, "encoder_hidden_states": text_embeds}
        
        if self.args.use_distillation:
            first_patch = [True]
            
            def direct_forward_fn(model_output, t, lms_input, **kwargs):
                if self.args.predict_xstart:
                    pred_xstart = model_output
                else:
                    x_t = kwargs.get('noise', None)
                    if x_t is None:
                        x_t = torch.zeros_like(lms_input)
                    
                    if x_t.shape != lms_input.shape:
                        x_t = torch.zeros_like(lms_input)
                    
                    if isinstance(t, torch.Tensor):
                        if t.dim() == 0:
                            t_batch = t.to(device=lms_input.device, dtype=torch.long).expand(lms_input.shape[0])
                        else:
                            t_batch = t.to(device=lms_input.device, dtype=torch.long)
                    else:
                        t_batch = torch.full((lms_input.shape[0],), 999, device=lms_input.device, dtype=torch.long)
                    pred_xstart = self.diffusion._predict_xstart_from_eps(x_t, t_batch, model_output)
                
                output = lms_input + pred_xstart
                return {"sample": output, "pred_xstart": pred_xstart}
            
            xt = torch.zeros_like(lms)
            
            if hasattr(self.args, 'test_with_noise') and self.args.test_with_noise:
                noise = torch.randn_like(lms)
                t_full = torch.full((lms.shape[0],), 99, device=self.device, dtype=torch.long)
                xt = self.diffusion.q_sample_xt(torch.zeros_like(lms), t_full, noise=noise)
            
            # VAEç¼–ç å™¨ï¼šæå–å…¨å±€åœºæ™¯ç‰¹å¾ï¼ˆè½»é‡çº§ï¼Œå¯ä»¥åœ¨æ•´å›¾ä¸Šè¿è¡Œï¼‰
            vae_features = None
            if self.vae_encoder is not None:
                mu, logvar = self.vae_encoder(pan)
                vae_features = mu
            
            batch_size = lms.shape[0]
            timesteps = torch.full((batch_size,), 99, device=self.device, dtype=torch.long)
            
            #  ControlNetï¼šä¸åœ¨è¿™é‡Œæå–ç‰¹å¾ï¼Œè€Œæ˜¯åœ¨forward_chop_distillå†…éƒ¨é€patchæå–
            # è¿™æ ·å¯ä»¥é¿å…å¯¹æ•´å›¾è®¡ç®—ControlNetå¯¼è‡´OOM
            # control_features = None
            # if self.controlnet_full is not None:
            #     control_features = self.controlnet_full(pan, xt, timesteps)
            
            #  å°†controlnet_fullä¼ é€’ç»™forward_chop_distillï¼Œè®©å®ƒå†…éƒ¨é€patchè®¡ç®—
            output = self.model.forward_chop_distill(
                lms, pan, ms, xt,
                sample_fn=direct_forward_fn,
                scene_token=vae_features,        # VAEçš„å…¨å±€ç‰¹å¾
                controlnet=self.controlnet_full,  #  ä¼ é€’ControlNetå¯¹è±¡è€Œä¸æ˜¯ç‰¹å¾
                encoder_hidden_states=text_embeds,  # CLIPçš„æ–‡æœ¬ç‰¹å¾
                noise=xt,
            )
        else:
            sample_fn = (
                self.diffusion.p_sample_loop 
                if not self.args.use_ddim 
                else self.diffusion.ddim_sample_loop
            )
            
            output = sample_fn(
                self.model,
                shape=ms.shape,
                model_kwargs=model_kwargs,
                clip_denoised=self.args.clip_denoised,
                progress=False
            )
        
        output = output.clamp(0, 1)
        
        return output
