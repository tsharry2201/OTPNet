"""
SSDiffä¸€æ­¥è’¸é¦è®­ç»ƒè„šæœ¬
æ¨¡ä»¿OSEDiffçš„è®­ç»ƒèŒƒå¼
"""
import os
import sys
import argparse
import datetime
import socket
import numpy as np
import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
from pathlib import Path
import wandb

# æ·»åŠ SSDiffè·¯å¾„
sys.path.append('/data2/user/zelilin/ARConv_SSDiff/SSDiff_main')

from accelerate import Accelerator
from accelerate.utils import set_seed, ProjectConfiguration
from accelerate import DistributedDataParallelKwargs
from diffusers.optimization import get_scheduler

from ssdiff_distill import SSDiff_gen, SSDiff_reg
from configs.option_DPM_pansharpening import parser_args as ssdiff_parser_args
from pancollection.common.psdata import PansharpeningSession as DataSession


def parse_args(input_args=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Train SSDiff Distillation')
    
    # åŸºç¡€è·¯å¾„
    parser.add_argument("--pretrained_ssdiff_path", type=str, required=True,
                       help="é¢„è®­ç»ƒSSDiffæ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_dir", type=str, required=True,
                       help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="experiments/ssdiff_distill",
                       help="è¾“å‡ºç›®å½•")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--seed", type=int, default=123)
    parser.add_argument("--train_batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--max_train_steps", type=int, default=50000)
    parser.add_argument("--num_training_epochs", type=int, default=10000)
    parser.add_argument("--checkpointing_steps", type=int, default=500)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    parser.add_argument("--lr_scheduler", type=str, default="constant",
                       choices=["linear", "cosine", "cosine_with_restarts", 
                               "polynomial", "constant", "constant_with_warmup"])
    parser.add_argument("--lr_warmup_steps", type=int, default=500)
    parser.add_argument("--lr_num_cycles", type=int, default=1)
    parser.add_argument("--lr_power", type=float, default=1.0)
    
    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument("--adam_beta1", type=float, default=0.9)
    parser.add_argument("--adam_beta2", type=float, default=0.999)
    parser.add_argument("--adam_weight_decay", type=float, default=1e-2)
    parser.add_argument("--adam_epsilon", type=float, default=1e-08)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    
    # æŸå¤±æƒé‡
    parser.add_argument("--lambda_l2", type=float, default=1.0,
                       help="L2é‡å»ºæŸå¤±æƒé‡")
    parser.add_argument("--lambda_vsd", type=float, default=1.0,
                       help="åˆ†å¸ƒåŒ¹é…æŸå¤±æƒé‡")
    parser.add_argument("--lambda_vsd_lora", type=float, default=1.0,
                       help="æ‰©æ•£æŸå¤±æƒé‡")
    
    # LoRAé…ç½®
    parser.add_argument("--lora_rank", type=int, default=4)
    
    # æ··åˆç²¾åº¦å’ŒåŠ é€Ÿ
    parser.add_argument("--mixed_precision", type=str, default="fp16",
                       choices=["no", "fp16", "bf16"])
    parser.add_argument("--enable_xformers", action="store_true")
    parser.add_argument("--gradient_checkpointing", action="store_true")
    parser.add_argument("--allow_tf32", action="store_true")
    parser.add_argument("--set_grads_to_none", action="store_true")
    
    # æ—¥å¿—å’ŒæŠ¥å‘Š
    parser.add_argument("--logging_dir", type=str, default="logs")
    parser.add_argument("--report_to", type=str, default="tensorboard")
    parser.add_argument("--tracker_project_name", type=str, default="train_ssdiff_distill")
    parser.add_argument("--dataloader_num_workers", type=int, default=0)
    
    # SSDiffç‰¹å®šå‚æ•°
    parser.add_argument("--ms_dim", type=int, default=8, help="å¤šå…‰è°±é€šé“æ•°")
    parser.add_argument("--pan_dim", type=int, default=1, help="å…¨è‰²é€šé“æ•°")
    parser.add_argument("--image_size", type=int, default=64, help="Patchå¤§å°")
    
    # æ¢å¤è®­ç»ƒ
    parser.add_argument("--resume_from_checkpoint", type=str, default=None)
    parser.add_argument("--resume_step", type=int, default=0)
    
    if input_args is not None:
        args = parser.parse_args(input_args)
    else:
        args = parser.parse_args()
    
    return args


def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # åˆå§‹åŒ–Accelerator
    logging_dir = Path(args.output_dir, args.logging_dir)
    accelerator_project_config = ProjectConfiguration(
        project_dir=args.output_dir, 
        logging_dir=logging_dir
    )
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        kwargs_handlers=[ddp_kwargs],
    )
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_main_process:
        os.makedirs(os.path.join(args.output_dir, "checkpoints"), exist_ok=True)
        os.makedirs(os.path.join(args.output_dir, "eval"), exist_ok=True)
    
    # è·å–SSDiffçš„é…ç½®
    ssdiff_args = ssdiff_parser_args()
    # åˆå¹¶å‚æ•°
    for key, value in vars(args).items():
        # å¼ºåˆ¶æ·»åŠ æ‰€æœ‰æ–°å‚æ•°ï¼Œä¸ç®¡ssdiff_argsæ˜¯å¦å·²æœ‰
        setattr(ssdiff_args, key, value)
    
    # åˆ›å»ºæ¨¡å‹
    print("Creating SSDiff distillation models...")
    model_gen = SSDiff_gen(ssdiff_args)
    model_gen.set_train()
    
    model_reg = SSDiff_reg(ssdiff_args, accelerator)
    model_reg.set_train()
    
    print("âœ… Models created successfully!")
    
    # è®¾ç½®ä¼˜åŒ–å™¨ - ç”Ÿæˆå™¨
    layers_to_opt = []
    for n, p in model_gen.unet.named_parameters():
        if ("lora_down" in n or "lora_up" in n) and p.requires_grad:
            layers_to_opt.append(p)
    
    print(f"Optimizing {len(layers_to_opt)} LoRA parameters in generator")
    
    optimizer = torch.optim.AdamW(
        layers_to_opt,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )
    
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        num_training_steps=args.max_train_steps * accelerator.num_processes,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )
    
    # è®¾ç½®ä¼˜åŒ–å™¨ - æ­£åˆ™åŒ–å™¨
    layers_to_opt_reg = []
    for n, p in model_reg.unet_update.named_parameters():
        if ("lora_down" in n or "lora_up" in n) and p.requires_grad:
            layers_to_opt_reg.append(p)
    
    print(f"Optimizing {len(layers_to_opt_reg)} LoRA parameters in regularizer")
    
    optimizer_reg = torch.optim.AdamW(
        layers_to_opt_reg,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )
    
    lr_scheduler_reg = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer_reg,
        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        num_training_steps=args.max_train_steps * accelerator.num_processes,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("Creating dataloaders...")
    session = DataSession(ssdiff_args)
    # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•åç§°å’Œå‚æ•°æ ¼å¼
    train_dataloader, _, _ = session.get_dataloader(ssdiff_args.dataset['train'], False, None)
    
    print(f"Train batches: {len(train_dataloader)}")
    
    # Prepare everything with accelerator
    model_gen, model_reg, optimizer, optimizer_reg, train_dataloader, \
    lr_scheduler, lr_scheduler_reg = accelerator.prepare(
        model_gen, model_reg, optimizer, optimizer_reg, train_dataloader,
        lr_scheduler, lr_scheduler_reg
    )
    
    # åˆå§‹åŒ–trackers
    if accelerator.is_main_process:
        tracker_config = dict(vars(args))
        accelerator.init_trackers(args.tracker_project_name, config=tracker_config)
    
    # æ¢å¤checkpoint
    global_step = 0
    resume_step = 0
    
    if args.resume_from_checkpoint is not None:
        print(f"ğŸ”„ Resuming from checkpoint: {args.resume_from_checkpoint}")
        try:
            ckpt = torch.load(args.resume_from_checkpoint, map_location='cpu')
            
            unwrapped_model_gen = accelerator.unwrap_model(model_gen)
            if 'unet_state_dict' in ckpt:
                for name, param in unwrapped_model_gen.unet.named_parameters():
                    if 'lora' in name and name in ckpt['unet_state_dict']:
                        param.data.copy_(ckpt['unet_state_dict'][name])
            
            resume_step = args.resume_step
            global_step = resume_step
            print(f"âœ… Resumed from step {resume_step}")
        except Exception as e:
            print(f"âŒ Failed to resume: {e}")
            resume_step = 0
            global_step = 0
    
    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(
        range(resume_step, args.max_train_steps),
        initial=resume_step,
        desc="Steps",
        disable=not accelerator.is_local_main_process,
    )
    
    for epoch in range(args.num_training_epochs):
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(model_gen, model_reg):
                # è·å–æ•°æ®
                # æ•°æ®è¯´æ˜ï¼š
                # - pan: é«˜åˆ†è¾¨ç‡å…¨è‰²å›¾åƒ [B, 1, 64, 64]
                # - lms: ä¸Šé‡‡æ ·åçš„ä½åˆ†è¾¨ç‡å¤šå…‰è°± [B, 8, 64, 64]  
                # - ms: åŸå§‹ä½åˆ†è¾¨ç‡å¤šå…‰è°± [B, 8, 16, 16]
                # - gt: ç›®æ ‡é«˜åˆ†è¾¨ç‡å¤šå…‰è°± [B, H, W, 8] æˆ– [B, 8, 64, 64]
                pan = batch['pan']
                lms = batch['lms']
                ms = batch['ms']
                gt = batch['gt']
                
                # è°ƒè¯•ï¼šæ‰“å°æ•°æ®ç»´åº¦å’ŒèŒƒå›´ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
                if global_step == 0:
                    print(f"Data shapes - pan: {pan.shape}, lms: {lms.shape}, ms: {ms.shape}, gt: {gt.shape}")
                    print(f"Data ranges BEFORE norm - pan: [{pan.min():.2f}, {pan.max():.2f}], lms: [{lms.min():.2f}, {lms.max():.2f}], ms: [{ms.min():.2f}, {ms.max():.2f}], gt: [{gt.min():.2f}, {gt.max():.2f}]")
                
                # è°ƒæ•´GTç»´åº¦ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢
                # æ•°æ®é›†ä¸­çš„gtå¯èƒ½æ˜¯ [B, 8, H, W] æˆ– [B, H, W, 8]
                # åªæœ‰å½“gtçš„ç¬¬äºŒä¸ªç»´åº¦å¤§äºç¬¬ä¸€ä¸ªç»´åº¦æ—¶æ‰éœ€è¦è½¬æ¢
                if gt.shape[1] > gt.shape[2]:  # å¦‚æœæ˜¯ [B, H, W, C] æ ¼å¼
                    import einops
                    gt = einops.rearrange(gt, 'b h w c -> b c h w')
                    if global_step == 0:
                        print(f"GT rearranged to: {gt.shape}")
                else:
                    if global_step == 0:
                        print(f"GT already in correct format: {gt.shape}")
                
                # æ³¨æ„ï¼šæ•°æ®åŠ è½½å™¨å·²ç»å°†æ•°æ®å½’ä¸€åŒ–åˆ°[0, 1]ï¼Œä¸éœ€è¦å†æ¬¡å½’ä¸€åŒ–ï¼
                # è°ƒè¯•ï¼šæ‰“å°æœ€ç»ˆæ•°æ®èŒƒå›´ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
                if global_step == 0:
                    print(f"Final data ranges - pan: [{pan.min():.4f}, {pan.max():.4f}], lms: [{lms.min():.4f}, {lms.max():.4f}], ms: [{ms.min():.4f}, {ms.max():.4f}], gt: [{gt.min():.4f}, {gt.max():.4f}]")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
                if torch.isnan(lms).any() or torch.isinf(lms).any():
                    print(f"Warning: lms contains NaN or Inf, skipping batch")
                    continue
                if torch.isnan(pan).any() or torch.isinf(pan).any():
                    print(f"Warning: pan contains NaN or Inf, skipping batch")
                    continue
                if torch.isnan(ms).any() or torch.isinf(ms).any():
                    print(f"Warning: ms contains NaN or Inf, skipping batch")
                    continue
                if torch.isnan(gt).any() or torch.isinf(gt).any():
                    print(f"Warning: gt contains NaN or Inf, skipping batch")
                    continue
                
                # å‰å‘ä¼ æ’­ï¼ˆç°åœ¨è¿”å›outputå’Œresidualï¼‰
                try:
                    output_pred, residual_pred = model_gen(lms, pan, ms, gt)
                except RuntimeError as e:
                    print(f"Error in forward pass: {e}")
                    print(f"Shapes - lms: {lms.shape}, pan: {pan.shape}, ms: {ms.shape}, gt: {gt.shape}")
                    raise
                
                # è°ƒè¯•ï¼šæ‰“å°è¾“å‡ºç»´åº¦ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
                if global_step == 0:
                    print(f"Model output - output_pred: {output_pred.shape}, residual_pred: {residual_pred.shape}")
                    print(f"Before residual calc - gt: {gt.shape}, lms: {lms.shape}")
                
                # è®¡ç®—çœŸå®æ®‹å·®
                gt_residual = gt - lms
                
                # è®¡ç®—æ®‹å·®é‡å»ºæŸå¤±ï¼ˆä½¿ç”¨L1ï¼Œä¸åŸå§‹SSDiffä¸€è‡´ï¼‰
                loss_l2 = F.l1_loss(
                    residual_pred.float(), 
                    gt_residual.float(), 
                    reduction="mean"
                ) * args.lambda_l2
                
                loss = loss_l2
                
                # è®¡ç®—åˆ†å¸ƒåŒ¹é…æŸå¤±ï¼ˆä½¿ç”¨æœ€ç»ˆè¾“å‡ºoutput_predï¼‰
                if torch.cuda.device_count() > 1:
                    loss_vsd = model_reg.module.distribution_matching_loss(
                        lms, pan, ms, output_pred
                    ) * args.lambda_vsd
                else:
                    loss_vsd = model_reg.distribution_matching_loss(
                        lms, pan, ms, output_pred
                    ) * args.lambda_vsd
                
                loss = loss + loss_vsd
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(layers_to_opt, args.max_grad_norm)
                
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=args.set_grads_to_none)
                
                # è®¡ç®—æ‰©æ•£æŸå¤±ï¼ˆæ­£åˆ™åŒ–å™¨ï¼‰
                if torch.cuda.device_count() > 1:
                    loss_diff = model_reg.module.diff_loss(
                        lms, pan, ms, gt
                    ) * args.lambda_vsd_lora
                else:
                    loss_diff = model_reg.diff_loss(
                        lms, pan, ms, gt
                    ) * args.lambda_vsd_lora
                
                accelerator.backward(loss_diff)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(
                        model_reg.parameters(), 
                        args.max_grad_norm
                    )
                
                optimizer_reg.step()
                lr_scheduler_reg.step()
                optimizer_reg.zero_grad(set_to_none=args.set_grads_to_none)
            
            # æ›´æ–°è¿›åº¦
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                
                if accelerator.is_main_process:
                    # è®°å½•æ—¥å¿—
                    logs = {
                        "loss_l2": loss_l2.detach().item(),
                        "loss_vsd": loss_vsd.detach().item(),
                        "loss_diff": loss_diff.detach().item(),
                        "loss_total": loss.detach().item(),
                    }
                    progress_bar.set_postfix(**logs)
                    
                    # ä¿å­˜checkpoint
                    if global_step % args.checkpointing_steps == 1:
                        outf = os.path.join(
                            args.output_dir, 
                            "checkpoints", 
                            f"model_{global_step}.pkl"
                        )
                        accelerator.unwrap_model(model_gen).save_model(outf)
                        print(f"ğŸ’¾ Checkpoint saved at step {global_step}")
                    
                    # è®°å½•åˆ°wandb/tensorboard
                    if global_step % 10 == 0:
                        wandb_logs = {
                            "train/loss_total": loss.item(),
                            "train/loss_l2": loss_l2.item(),
                            "train/loss_vsd": loss_vsd.item(),
                            "train/loss_diff": loss_diff.item(),
                            "train/step": global_step,
                            "train/learning_rate": args.learning_rate,
                            "train/epoch": epoch,
                        }
                        wandb.log(wandb_logs)
                    
                    accelerator.log(logs, step=global_step)
                
                # æ—©åœ
                if global_step >= args.max_train_steps:
                    print(f"\nğŸ Training completed! Reached max steps: {args.max_train_steps}")
                    return
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
        if global_step >= args.max_train_steps:
            wandb.finish()
            break
    
    print("ğŸ‰ Training finished!")


if __name__ == "__main__":
    args = parse_args()
    
    # åˆå§‹åŒ–wandb
    run_dir = os.path.join("runs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    os.makedirs(run_dir, exist_ok=True)
    
    wandb.init(
        config=vars(args),
        project="ssdiff-distillation",
        entity="tszharry-xi-an-jiaotong-university-",  # è®¾ç½®æ‚¨çš„wandb entity
        notes=socket.gethostname(),
        name=f"distill_{args.lora_rank}",
        dir=run_dir,
        job_type="training",
        mode="offline",  # æ”¹ä¸º"online"ä»¥åŒæ­¥åˆ°wandbæœåŠ¡å™¨
        reinit=True
    )
    
    main(args)

