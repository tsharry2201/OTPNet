"""
SSDiffä¸€æ­¥è’¸é¦æ¨¡å‹
æ¨¡ä»¿OSEDiffçš„è®­ç»ƒèŒƒå¼ï¼Œå°†å¤šæ­¥SSDiffè’¸é¦ä¸ºå•æ­¥æ¨¡å‹
ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨è½»é‡çº§Scene Tokenæ›¿ä»£CLIPï¼Œæä¾›åœºæ™¯æ¡ä»¶ä¿¡æ¯
"""
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from peft import LoraConfig
import copy

# æ·»åŠ SSDiffè·¯å¾„
sys.path.append('/data2/user/zelilin/ARConv_SSDiff/SSDiff_main')
from utils.script_util import create_model_and_diffusion, args_to_dict, model_and_diffusion_defaults
from model.ARConv import ARConv


def _extract_effective_weight(conv_layer):
    """
    æå–Convå±‚çš„æœ‰æ•ˆæƒé‡
    
    - å¦‚æœæ˜¯æ™®é€šConv2dï¼šç›´æ¥è¿”å›æƒé‡
    - å¦‚æœæ˜¯LoRALayerï¼šè¿”å›èåˆæƒé‡ï¼ˆbase + LoRA effectï¼‰
    
    Args:
        conv_layer: Conv2dæˆ–LoRALayer
    
    Returns:
        weight: æœ‰æ•ˆçš„å·ç§¯æƒé‡ [outc, inc, h, w]
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯LoRALayerï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰lora_downå±æ€§ï¼‰
    if hasattr(conv_layer, 'base_layer') and hasattr(conv_layer, 'lora_down'):
        print(f"   æ£€æµ‹åˆ°LoRALayerï¼Œè®¡ç®—èåˆæƒé‡...")
        # è·å–baseæƒé‡
        base_weight = conv_layer.base_layer.weight.data.clone()  # [outc, inc, 3, 3]
        
        # è®¡ç®—LoRA delta
        # LoRA: delta = lora_up(lora_down(I)) * scaling
        # å¯¹äºå·ç§¯ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ç­‰æ•ˆçš„æƒé‡ä¿®æ­£
        lora_down_weight = conv_layer.lora_down.weight.data  # [rank, inc, 3, 3]
        lora_up_weight = conv_layer.lora_up.weight.data      # [outc, rank, 1, 1]
        scaling = conv_layer.scaling
        
        # è®¡ç®—LoRA delta weight
        # ç®€åŒ–ï¼šå¯¹äºkernel_size=3çš„convï¼ŒLoRAçš„å½±å“å¯ä»¥è¿‘ä¼¼ä¸ºæƒé‡å åŠ 
        # æ›´ç²¾ç¡®çš„åšæ³•ï¼šlora_delta = conv(lora_up_weight, lora_down_weight)
        # ä½†ç”±äºlora_upæ˜¯1x1å·ç§¯ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åšçŸ©é˜µä¹˜æ³•
        rank, inc, kh, kw = lora_down_weight.shape
        outc, rank2, _, _ = lora_up_weight.shape
        
        # Reshape for matmul: [outc, rank] @ [rank, inc*kh*kw]
        lora_down_flat = lora_down_weight.reshape(rank, inc * kh * kw)  # [rank, inc*9]
        lora_up_flat = lora_up_weight.squeeze(-1).squeeze(-1)  # [outc, rank]
        
        # Delta weight: [outc, inc*kh*kw]
        lora_delta_flat = torch.matmul(lora_up_flat, lora_down_flat) * scaling
        lora_delta = lora_delta_flat.reshape(outc, inc, kh, kw)
        
        # èåˆæƒé‡
        fused_weight = base_weight + lora_delta
        
        print(f"      Baseæƒé‡èŒƒå›´: [{base_weight.min():.4f}, {base_weight.max():.4f}]")
        print(f"      LoRA deltaèŒƒå›´: [{lora_delta.min():.4f}, {lora_delta.max():.4f}]")
        print(f"      èåˆæƒé‡èŒƒå›´: [{fused_weight.min():.4f}, {fused_weight.max():.4f}]")
        
        return fused_weight
    else:
        # æ™®é€šConv2d
        return conv_layer.weight.data.clone()


def switch_conv2d_to_arconv_with_interpolation(model, args):
    """
    åŠ¨æ€åˆ‡æ¢ï¼šå°†æ¨¡å‹ä¸­çš„Conv2dï¼ˆå«LoRAï¼‰æ›¿æ¢ä¸ºARConvï¼Œä½¿ç”¨æ’å€¼æ‰©å±•åˆå§‹åŒ–
    
    é€‚ç”¨åœºæ™¯ï¼š
    - å‰Næ­¥ï¼šStudentä½¿ç”¨Conv2dè®­ç»ƒï¼ˆç»§æ‰¿é¢„è®­ç»ƒæƒé‡+LoRAï¼‰
    - Næ­¥åï¼šåˆ‡æ¢åˆ°ARConvï¼Œå°†Conv2dæƒé‡é€šè¿‡æ’å€¼æ‰©å±•åˆ°ARConvçš„9ä¸ªå·ç§¯æ ¸
    
    Args:
        model: Student UNetæ¨¡å‹
        args: é…ç½®å‚æ•°
    """
    print("\n" + "="*80)
    print("ğŸ”„ å¼€å§‹åˆ‡æ¢ï¼šConv2d â†’ ARConvï¼ˆæ’å€¼æ‰©å±•åˆå§‹åŒ–ï¼‰")
    print("="*80)
    
    # é€’å½’æ›¿æ¢æ‰€æœ‰ResBlockä¸­çš„Conv2dä¸ºARConv
    def replace_conv_in_module(module, module_name=""):
        from model.SSNet import ResBlock
        
        if isinstance(module, ResBlock):
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ARConv
            if isinstance(module.conv0, ARConv):
                print(f"â­ï¸  {module_name}: å·²ç»æ˜¯ARConvï¼Œè·³è¿‡")
                return
            
            # ğŸ”¥ æå–æœ‰æ•ˆæƒé‡ï¼ˆèåˆConv2d base + LoRA deltaï¼‰
            conv0_weight = _extract_effective_weight(module.conv0)  # [outc, inc, 3, 3]
            conv1_weight = _extract_effective_weight(module.conv1)  # [outc, inc, 3, 3]
            
            # åˆ›å»ºæ–°çš„ARConv
            in_channels = conv0_weight.shape[1]      # inc
            hidden_channels = conv0_weight.shape[0]   # outc of conv0
            out_channels = conv1_weight.shape[0]      # outc of conv1
            
            new_conv0 = ARConv(in_channels, hidden_channels, 3, 1, 1)
            new_conv1 = ARConv(hidden_channels, out_channels, 3, 1, 1)
            
            # ä½¿ç”¨æ’å€¼æ‰©å±•åˆå§‹åŒ–ARConvçš„9ä¸ªå·ç§¯æ ¸
            _init_arconv_from_conv2d_interpolation(new_conv0, conv0_weight)
            _init_arconv_from_conv2d_interpolation(new_conv1, conv1_weight)
            
            # æ›¿æ¢
            module.conv0 = new_conv0
            module.conv1 = new_conv1
            module.use_arconv = True
            
            # ğŸ”¥ ç¡®ä¿arconv_hw_rangeæ˜¯åˆ—è¡¨æ ¼å¼
            if hasattr(args, 'arconv_hw_range'):
                hw_range = args.arconv_hw_range
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                if isinstance(hw_range, str):
                    hw_range = eval(hw_range)
                module.arconv_hw_range = hw_range
            else:
                module.arconv_hw_range = [1, 9]  # é»˜è®¤å€¼
            
            print(f"âœ… {module_name}: Conv2d+LoRA â†’ ARConv (æ’å€¼æ‰©å±•ï¼Œhw_range={module.arconv_hw_range})")
    
        # é€’å½’å¤„ç†å­æ¨¡å—
        for name, child in module.named_children():
            replace_conv_in_module(child, f"{module_name}.{name}" if module_name else name)
    
    replace_conv_in_module(model, "unet")
    
    print("="*80)
    print("ğŸ‰ åˆ‡æ¢å®Œæˆï¼ARConvå·²ä»Conv2d+LoRAæƒé‡åˆå§‹åŒ–")
    print("="*80 + "\n")


def _init_arconv_from_conv2d_interpolation(arconv_model, conv2d_weight):
    """
    ğŸ”¥ æ›´ä¸¥æ ¼çš„ARConvæ’ç­‰èµ·æ­¥åˆå§‹åŒ–
    
    ç­–ç•¥ï¼š
    - åˆ‡æ¢ç¬é—´ä»…ä¿ç•™3Ã—3è·¯å¾„ï¼šæ··åˆæƒé‡å¯¹3Ã—3ç½®1ï¼Œå…¶å®ƒæ ¸ç½®0
    - offsetå…¨0ï¼šæ— ç©ºé—´åç§»
    - modulationåˆå€¼1.0ï¼ˆè€Œä¸æ˜¯tanhâ‰ˆ0.96ï¼‰ï¼Œé¿å…å¹…å€¼æ”¶ç¼©
    
    Args:
        arconv_model: ARConvæ¨¡å—
        conv2d_weight: Conv2dæƒé‡ [outc, inc, 3, 3]
    """
    kernel_sizes = [(3,3), (3,5), (5,3), (3,7), (7,3), (5,5), (5,7), (7,5), (7,7)]
    
    # ğŸ”¥ æ–°ç­–ç•¥ï¼šå…ˆç”¨æ’å€¼åˆå§‹åŒ–æ‰€æœ‰kernelï¼Œä½†ä»…æ¿€æ´»3Ã—3
    for i, (h, w) in enumerate(kernel_sizes):
        if h == 3 and w == 3:
            # 3x3ç›´æ¥å¤åˆ¶
            arconv_model.convs[i].weight.data = conv2d_weight.clone()
        else:
            # å…¶ä»–å°ºå¯¸ï¼šä½¿ç”¨æ’å€¼æ‰©å±•ï¼ˆä¸ºåç»­è®­ç»ƒå‡†å¤‡ï¼‰
            expanded_weight = F.interpolate(
                conv2d_weight, 
                size=(h, w), 
                mode='bilinear', 
                align_corners=True
            )
            arconv_model.convs[i].weight.data = expanded_weight
    
    # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šæ›´ä¸¥æ ¼çš„"æ’ç­‰èµ·æ­¥"
    
    # 1. m_convï¼ˆmodulationï¼‰ï¼šåˆå§‹åŒ–ä¸º1.0ï¼ˆæ— è°ƒåˆ¶ï¼‰
    #    out = x_offset * m + biasï¼Œm=1æ—¶ç›¸å½“äº out = x_offset + bias
    #    ä¿®æ”¹ï¼šä¸ä½¿ç”¨tanhâ‰ˆ0.96ï¼ˆä¼šæ”¶ç¼©å¹…å€¼ï¼‰ï¼Œç›´æ¥åˆå§‹åŒ–è¾“å‡ºä¸ºæ¥è¿‘0ï¼ˆç»tanhåæ¥è¿‘1ï¼‰
    nn.init.zeros_(arconv_model.m_conv[6].weight)
    if arconv_model.m_conv[6].bias is not None:
        # ä¿®æ”¹ï¼šä½¿ç”¨æ›´å¤§çš„biasè®©tanhè¾“å‡ºæ›´æ¥è¿‘1
        nn.init.constant_(arconv_model.m_conv[6].bias, 3.0)  # Tanh(3.0) â‰ˆ 0.995
    
    # 2. b_convï¼ˆbiasï¼‰ï¼šåˆå§‹åŒ–ä¸º0ï¼ˆæ— é¢å¤–biasï¼‰
    nn.init.zeros_(arconv_model.b_conv[6].weight)
    if arconv_model.b_conv[6].bias is not None:
        nn.init.zeros_(arconv_model.b_conv[6].bias)
    
    # 3. p_convï¼ˆoffset positionï¼‰ï¼šåˆå§‹åŒ–ä¸º0ï¼ˆæ— åç§»ï¼‰
    nn.init.zeros_(arconv_model.p_conv[4].weight)
    if arconv_model.p_conv[4].bias is not None:
        nn.init.zeros_(arconv_model.p_conv[4].bias)
    
    # 4. l_convå’Œw_convï¼ˆkernel size selectionï¼‰ï¼š
    #    ğŸ”¥ ä¿®æ”¹ï¼šåˆå§‹åŒ–ä¸ºæ›´æ¥è¿‘3çš„å€¼ï¼ˆSigmoidè¾“å‡ºâ‰ˆ0ï¼‰
    #    l = Sigmoid(output) * (hw_range[1] - 1) + 1
    #    Sigmoid(-5.0) â‰ˆ 0.007ï¼Œåˆ™ l â‰ˆ 0.007*4+1 â‰ˆ 1ï¼ˆå¯¹äºhw_range=[1,5]ï¼‰
    #    ä½†æˆ‘ä»¬å¸Œæœ›åˆå§‹ä¸º3ï¼Œæ‰€ä»¥Sigmoidåº”è¯¥â‰ˆ0.5ï¼Œå³bias=0
    nn.init.zeros_(arconv_model.l_conv[4].weight)
    if arconv_model.l_conv[4].bias is not None:
        # å¯¹äºhw_range=[1,5]ï¼šSigmoid(0)=0.5 â†’ 0.5*4+1=3
        nn.init.constant_(arconv_model.l_conv[4].bias, 0.0)
    
    nn.init.zeros_(arconv_model.w_conv[4].weight)
    if arconv_model.w_conv[4].bias is not None:
        nn.init.constant_(arconv_model.w_conv[4].bias, 0.0)
    
    print("      ğŸ¯ ARConvæ’ç­‰èµ·æ­¥åˆå§‹åŒ–ç­–ç•¥:")
    print("         - å·ç§¯æ ¸: ä»Conv2dæ’å€¼æ‰©å±•")
    print("         - Modulation: åˆå§‹åŒ–ä¸ºâ‰ˆ1.0ï¼ˆæ— å¹…å€¼æ”¶ç¼©ï¼‰")
    print("         - Bias: åˆå§‹åŒ–ä¸º0ï¼ˆæ— åç§»ï¼‰")
    print("         - Offset: åˆå§‹åŒ–ä¸º0ï¼ˆæ— ç©ºé—´åç§»ï¼‰")
    print("         - Kernel size: åˆå§‹åŒ–ä¸º3x3ï¼ˆç¨³å®šèµ·ç‚¹ï¼Œhw_range=[1,5]ï¼‰")
    

def initialize_ssdiff_unet_with_lora(args, pretrained_path=None):
    """
    åˆå§‹åŒ–SSDiffçš„UNetå¹¶æ·»åŠ LoRAå±‚
    
    Args:
        args: é…ç½®å‚æ•°
        pretrained_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    
    Returns:
        unet: å¸¦LoRAçš„UNetæ¨¡å‹
        lora_target_modules: LoRAç›®æ ‡æ¨¡å—åˆ—è¡¨
    """
    # åˆ›å»ºStudentæ¨¡å‹ï¼ˆé…ç½®è·Ÿéšargsï¼‰
    student_args_dict = args_to_dict(args, model_and_diffusion_defaults().keys())
    student_args_dict['use_arconv'] = args.use_arconv  # ğŸ”¥ Studentçš„ARConvè®¾ç½®è·Ÿéšargs
    student_args_dict['use_scene_token'] = args.use_scene_token  # ğŸ”¥ Studentçš„Scene Tokenè®¾ç½®è·Ÿéšargs
    model, _ = create_model_and_diffusion(**student_args_dict)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if pretrained_path is not None and os.path.exists(pretrained_path):
        print(f"Loading pretrained SSDiff from: {pretrained_path}")
        state_dict = torch.load(pretrained_path, map_location='cpu')
        # ä½¿ç”¨strict=Falseï¼Œå› ä¸ºæˆ‘ä»¬æ·»åŠ äº†scene_embedå±‚
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"âš ï¸  ç¼ºå¤±çš„é”®ï¼ˆæ–°æ·»åŠ çš„å±‚ï¼Œå°†éšæœºåˆå§‹åŒ–ï¼‰: {len(missing_keys)} ä¸ª")
            # æ‰“å°scene_embedç›¸å…³çš„ç¼ºå¤±é”®
            scene_keys = [k for k in missing_keys if 'scene_embed' in k]
            if scene_keys:
                print(f"   Scene embedå±‚: {len(scene_keys)} ä¸ªå‚æ•°")
        print("âœ… Pretrained SSDiff loaded successfully!")
    
    # å†»ç»“æ‰€æœ‰å‚æ•°
    model.requires_grad_(False)
    model.train()
    
    # æ‰¾åˆ°æ‰€æœ‰å¯ä»¥æ·»åŠ LoRAçš„å±‚
    lora_target_modules = []
    for name, module in model.named_modules():
        # ä¸ºConv2då’ŒLinearå±‚æ·»åŠ LoRA
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            # è·³è¿‡è¾“å…¥è¾“å‡ºå±‚å’Œä¸€äº›ç‰¹æ®Šå±‚
            if any(skip in name for skip in ['time_embed', 'label_emb']):
                continue
            lora_target_modules.append(name)
    
    # åªä¿ç•™éƒ¨åˆ†å…³é”®å±‚ï¼ˆé¿å…è¿‡åº¦å‚æ•°åŒ–ï¼‰
    # ä¼˜å…ˆé€‰æ‹©attentionå’Œæ®‹å·®å—ä¸­çš„å±‚
    filtered_modules = []
    for name in lora_target_modules:
        if any(key in name for key in ['attn', 'in_layers', 'out_layers', 'skip_connection']):
            filtered_modules.append(name)
    
    if len(filtered_modules) == 0:
        filtered_modules = lora_target_modules  # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œä½¿ç”¨å…¨éƒ¨
    
    print(f"Found {len(filtered_modules)} modules for LoRA")
    print(f"Sample modules: {filtered_modules[:5]}")
    
    # æ‰‹åŠ¨ä¸ºConv2då±‚æ·»åŠ LoRAå‚æ•°
    # ä½¿ç”¨ç®€å•çš„LoRAå®ç°ï¼Œé¿å…PEFTåº“çš„å¤æ‚æ€§
    class LoRALayer(nn.Module):
        def __init__(self, base_layer, rank=4, alpha=8):
            super().__init__()
            self.base_layer = base_layer
            self.rank = rank
            self.alpha = alpha
            self.scaling = alpha / rank
            
            # ä¸ºConv2dåˆ›å»ºLoRAå‚æ•°
            if isinstance(base_layer, nn.Conv2d):
                self.lora_down = nn.Conv2d(
                    base_layer.in_channels, 
                    rank, 
                    kernel_size=base_layer.kernel_size,
                    stride=base_layer.stride,
                    padding=base_layer.padding,
                    bias=False
                )
                self.lora_up = nn.Conv2d(
                    rank, 
                    base_layer.out_channels, 
                    kernel_size=1,
                    stride=1,
                    padding=0,
                    bias=False
                )
                # åˆå§‹åŒ–
                nn.init.kaiming_uniform_(self.lora_down.weight, a=1)
                nn.init.zeros_(self.lora_up.weight)
        
        def forward(self, x):
            base_out = self.base_layer(x)
            if hasattr(self, 'lora_down'):
                # ç¡®ä¿ LoRA å±‚ä½¿ç”¨ä¸è¾“å…¥ç›¸åŒçš„ dtype
                x_dtype = x.dtype
                lora_down_out = self.lora_down(x.to(self.lora_down.weight.dtype))
                lora_out = self.lora_up(lora_down_out) * self.scaling
                # è½¬æ¢å›åŸå§‹ dtype
                lora_out = lora_out.to(x_dtype)
                return base_out + lora_out
            return base_out
    
    # ä¸ºé€‰å®šçš„æ¨¡å—æ·»åŠ LoRA
    lora_count = 0
    for name, module in model.named_modules():
        if name in filtered_modules and isinstance(module, nn.Conv2d):
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]
            parent = dict(model.named_modules())[parent_name] if parent_name else model
            
            # åˆ›å»ºLoRAåŒ…è£…å±‚
            lora_layer = LoRALayer(module, rank=args.lora_rank, alpha=args.lora_rank * 2)
            setattr(parent, child_name, lora_layer)
            lora_count += 1
    
    print(f"âœ… Added LoRA to {lora_count} Conv2d layers")
    return model, filtered_modules


class SceneTokenExtractor(nn.Module):
    """
    ğŸ”¥ è½»é‡çº§åœºæ™¯ç‰¹å¾æå–å™¨
    ä»PANå›¾åƒä¸­è‡ªåŠ¨æå–åœºæ™¯æ¡ä»¶tokenï¼Œæ›¿ä»£CLIP
    
    ä¼˜åŠ¿ï¼š
    1. ä»»åŠ¡ç‰¹å®šï¼šç›´æ¥å­¦ä¹ é¥æ„Ÿå›¾åƒçš„åœºæ™¯ç‰¹å¾
    2. è½»é‡çº§ï¼šåªæœ‰~100Kå‚æ•°ï¼Œè®¡ç®—å¼€é”€æå°
    3. ç«¯åˆ°ç«¯å¯è®­ç»ƒï¼šä¸è’¸é¦ä»»åŠ¡è”åˆä¼˜åŒ–
    4. æ— éœ€é¢å¤–æ ‡æ³¨ï¼šè‡ªåŠ¨ä»å›¾åƒæå–
    """
    def __init__(self, input_channels=1, token_dim=256):
        super().__init__()
        self.token_dim = token_dim
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        self.encoder = nn.Sequential(
            # Stage 1: 64x64 -> 32x32
            nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            
            # Stage 2: 32x32 -> 16x16
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, 128),
            nn.SiLU(),
            
            # Stage 3: 16x16 -> 8x8
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, 256),
            nn.SiLU(),
            
            # Global pooling
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            
            # Project to token space
            nn.Linear(256, token_dim),
            nn.LayerNorm(token_dim)
        )
        
        # åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, pan):
        """
        Args:
            pan: [B, 1, H, W] å…¨è‰²å›¾åƒ
        
        Returns:
            scene_token: [B, token_dim] åœºæ™¯ç‰¹å¾å‘é‡
        """
        return self.encoder(pan)


class SSDiff_gen(nn.Module):
    """
    å•æ­¥ç”Ÿæˆå™¨æ¨¡å‹
    ç±»ä¼¼äºOSEDiff_genï¼Œä½†é’ˆå¯¹SSDiffçš„å…¨æ™¯é”åŒ–ä»»åŠ¡
    ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨è½»é‡çº§Scene Tokenæä¾›åœºæ™¯æ¡ä»¶ä¿¡æ¯
    """
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.current_step = 0  # ğŸ”¥ è¿½è¸ªå½“å‰è®­ç»ƒæ­¥æ•°ï¼ˆç”¨äºARConvçš„epochå‚æ•°ï¼‰
        
        # ğŸ”¥ åˆå§‹åŒ–Scene Tokenæå–å™¨ï¼ˆè½»é‡çº§ï¼Œå¯è®­ç»ƒï¼‰
        if hasattr(args, 'use_scene_token') and args.use_scene_token:
            token_dim = getattr(args, 'scene_token_dim', 256)
            print(f"ğŸ¨ Initializing Scene Token Extractor (dim={token_dim})...")
            self.scene_extractor = SceneTokenExtractor(
                input_channels=1,  # PANé€šé“æ•°
                token_dim=token_dim
            )
            print("âœ… Scene Token Extractor initialized")
        else:
            self.scene_extractor = None
            print("â„¹ï¸  Scene Token disabled")
        
        # ğŸ”¥ OSEDiffé£æ ¼ä¿®æ”¹ï¼šæ·»åŠ Teacheræ¨¡å‹ï¼ˆå†»ç»“ï¼‰
        # Teacherï¼šåŸå§‹é¢„è®­ç»ƒçš„SSDiffï¼ˆæ— ARConvï¼Œæ— Scene Tokenï¼Œæ— LoRAï¼‰
        print("ğŸ“š Loading Teacher model (frozen, no ARConv, no Scene Token)...")
        # Teacherä¸ä½¿ç”¨ARConvå’ŒScene Tokenï¼ˆå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰ï¼‰
        teacher_args_dict = args_to_dict(args, model_and_diffusion_defaults().keys())
        teacher_args_dict['use_arconv'] = False  # ğŸ”¥ Teacherä¸ä½¿ç”¨ARConv
        teacher_args_dict['use_scene_token'] = False  # ğŸ”¥ Teacherä¸ä½¿ç”¨Scene Token
        self.unet_teacher, _ = create_model_and_diffusion(**teacher_args_dict)
        # åŠ è½½teacheræƒé‡ï¼ˆä½¿ç”¨strict=Falseï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰scene_embedå±‚ï¼‰
        teacher_state = torch.load(args.pretrained_ssdiff_path, map_location='cpu')
        missing_keys, unexpected_keys = self.unet_teacher.load_state_dict(teacher_state, strict=False)
        
        # æ‰“å°ç¼ºå¤±å’Œå¤šä½™çš„é”®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if missing_keys:
            print(f"âš ï¸  Teacherç¼ºå¤±çš„é”®ï¼ˆè¿™äº›æ˜¯æ–°æ·»åŠ çš„ï¼Œæ­£å¸¸ï¼‰: {missing_keys[:5]}...")  # åªæ‰“å°å‰5ä¸ª
        if unexpected_keys:
            print(f"âš ï¸  Teacherå¤šä½™çš„é”®: {unexpected_keys}")
        
        # å†»ç»“teacher
        self.unet_teacher.eval()
        for p in self.unet_teacher.parameters():
            p.requires_grad = False
        print("âœ… Teacher model loaded and frozen")
        
        # Studentï¼šæ·»åŠ LoRAçš„å¯è®­ç»ƒæ¨¡å‹
        print("ğŸ“ Loading Student model (with LoRA)...")
        self.unet, self.lora_target_modules = initialize_ssdiff_unet_with_lora(
            args, 
            pretrained_path=args.pretrained_ssdiff_path
        )
        print("âœ… Student model loaded with LoRA")
        
        # åˆ›å»ºdiffusionï¼ˆç”¨äºå•æ­¥å»å™ªï¼Œä½¿ç”¨é¡¶éƒ¨å·²å¯¼å…¥çš„å‡½æ•°ï¼‰
        _, self.diffusion = create_model_and_diffusion(
            **args_to_dict(args, model_and_diffusion_defaults().keys())
        )
        
        # æ‰“å°predict_xstartè®¾ç½®ï¼ˆè®­ç»ƒæ—¶ï¼‰
        print(f"ğŸ” [SSDiff_gen è®­ç»ƒ] predict_xstart = {args.predict_xstart}")
        
        # ğŸ”¥ OSEDiffé£æ ¼ä¿®æ”¹ï¼šä¸å†å›ºå®štimestepï¼Œè®­ç»ƒæ—¶åŠ¨æ€ç”Ÿæˆ
        # æ¨ç†æ—¶ä½¿ç”¨t=999ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨éšæœºtimestep [100, 999)
        self.inference_timestep = 999
        
        self.lora_rank = args.lora_rank
        self.training = True  # æ·»åŠ trainingæ ‡å¿—
    
    def set_step(self, step):
        """è®¾ç½®å½“å‰è®­ç»ƒæ­¥æ•°ï¼ˆç”¨äºARConvçš„epochå‚æ•°å’Œæ¸è¿›å¼è®­ç»ƒï¼‰"""
        self.current_step = step
        # å°†stepä¼ é€’ç»™unetï¼ˆARConvä¼šæ ¹æ®epochåˆ¤æ–­æ˜¯å¦å›ºå®šï¼‰
        if hasattr(self.unet, 'set_epoch'):
            self.unet.set_epoch(step)
    
    def set_train(self, enable_arconv=False, freeze_scene_token=False):
        """
        è®¾ç½®è®­ç»ƒæ¨¡å¼ï¼ŒLoRAå±‚ã€Scene Extractorå’ŒARConvå¯è®­ç»ƒ
        
        Args:
            enable_arconv: æ˜¯å¦å¯ç”¨ARConvè®­ç»ƒï¼ˆæ¸è¿›å¼è§£å†»ç”¨ï¼‰
                          æ³¨æ„ï¼šfixstepåARConvçš„offset/modulationä¼šå›ºå®šï¼Œ
                          ä½†å·ç§¯æƒé‡ä»ç»§ç»­è®­ç»ƒ
            freeze_scene_token: æ˜¯å¦å†»ç»“Scene Token Extractorï¼ˆå‰300æ­¥å†»ç»“ï¼‰
        """
        self.training = True
        self.unet.train()
        
        trainable_count = {'lora': 0, 'scene': 0, 'scene_gate': 0, 'arconv': 0}
        
        # UNetçš„å‚æ•°è®¾ç½®
        for n, p in self.unet.named_parameters():
            # LoRAå±‚å§‹ç»ˆå¯è®­ç»ƒ
            if "lora_down" in n or "lora_up" in n:
                p.requires_grad = True
                trainable_count['lora'] += 1
            # ğŸ”¥ Scene Gateå‚æ•°ï¼ˆé—¨æ§Î±ï¼‰å§‹ç»ˆå¯è®­ç»ƒ
            elif "scene_gate" in n:
                p.requires_grad = True
                trainable_count['scene_gate'] += 1
            # ğŸ”¥ ARConvæ ¹æ®é˜¶æ®µå†³å®šæ˜¯å¦å¯è®­ç»ƒ
            # åŒ…æ‹¬ï¼šoffset_conv, modulation_conv, weightç­‰æ‰€æœ‰å‚æ•°
            elif any(key in n.lower() for key in ['arconv', 'adaptive', 'offset', 'modulation', 'kernel_gen']):
                p.requires_grad = enable_arconv
                if enable_arconv:
                    trainable_count['arconv'] += 1
            else:
                p.requires_grad = False
        
        # ğŸ”¥ Scene Extractorï¼šå‰300æ­¥å†»ç»“ï¼Œä¹‹åè§£å†»ï¼ˆå­¦ä¹ ç‡ä¸ºä¸»å¹²çš„0.1xï¼‰
        if self.scene_extractor is not None:
            self.scene_extractor.train()
            for p in self.scene_extractor.parameters():
                p.requires_grad = not freeze_scene_token
                if not freeze_scene_token:
                    trainable_count['scene'] += 1
        
        # åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æˆ–çŠ¶æ€æ”¹å˜æ—¶æ‰“å°
        if not hasattr(self, '_last_arconv_state') or self._last_arconv_state != enable_arconv:
            status = "è®­ç»ƒä¸­ï¼ˆoffset+modulation+weightï¼‰" if enable_arconv and self.current_step < 4000 else \
                     "è®­ç»ƒä¸­ï¼ˆä»…weightï¼Œoffset/modulationå·²å›ºå®šï¼‰" if enable_arconv else "å†»ç»“"
            scene_status = "å†»ç»“" if freeze_scene_token else "è®­ç»ƒä¸­"
            print(f"ğŸ”¥ å¯è®­ç»ƒå‚æ•°: LoRA={trainable_count['lora']}, "
                  f"Scene={trainable_count['scene']} ({scene_status}), SceneGate={trainable_count['scene_gate']}, "
                  f"ARConv={trainable_count['arconv']} ({status})")
            self._last_arconv_state = enable_arconv
    
    def forward(self, lms, pan, ms, gt=None):
        """
        å‰å‘ä¼ æ’­ï¼ˆæ®‹å·®å­¦ä¹ ç‰ˆæœ¬ï¼Œä¸åŸSSDiffå¯¹é½ï¼‰
        ğŸ”¥ ä¼˜åŒ–ï¼šæ”¯æŒScene Token conditioning
        
        Args:
            lms: ä½åˆ†è¾¨ç‡å¤šå…‰è°±å›¾åƒ [B, 8, H, W]
            pan: å…¨è‰²å›¾åƒ [B, 1, H, W]
            ms: ä¸Šé‡‡æ ·çš„å¤šå…‰è°±å›¾åƒ [B, 8, H, W]
            gt: Ground truth (è®­ç»ƒæ—¶ä½¿ç”¨) [B, 8, H, W]
        
        Returns:
            output: é¢„æµ‹çš„é«˜åˆ†è¾¨ç‡å¤šå…‰è°±å›¾åƒ [B, 8, H, W]
            residual_pred: UNeté¢„æµ‹çš„æ®‹å·®
            scene_token: Scene token features (å¦‚æœå¯ç”¨)
        """
        device = lms.device
        batch_size = lms.shape[0]
        
        # ğŸ”¥ æå–scene token (ä»PANæå–)
        scene_token = None
        if self.scene_extractor is not None:
            scene_token = self.scene_extractor(pan)  # [B, token_dim]
        
        # ğŸ”¥ è®­ç»ƒæ—¶ä½¿ç”¨éšæœºtimestep
        if self.training:
            # è®­ç»ƒï¼šéšæœºtimestep [100, 999)ï¼Œè¦†ç›–å¤šç§å™ªå£°æ°´å¹³
            timesteps = torch.randint(100, 999, (batch_size,), device=device, dtype=torch.long)
        else:
            # æ¨ç†ï¼šå›ºå®št=999å•æ­¥
            timesteps = torch.full((batch_size,), self.inference_timestep, device=device, dtype=torch.long)
        
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä¸åŸSSDiffå®Œå…¨ä¸€è‡´ï¼Œä½¿ç”¨q_sample_xt
        if self.training and gt is not None:
            # è®­ç»ƒæ—¶ï¼šè®¡ç®—çœŸæ®‹å·®ï¼Œä½¿ç”¨åŸSSDiffçš„æ‰©æ•£è¿‡ç¨‹
            gt_residual = gt - lms  # çœŸæ®‹å·®
            
            # ä½¿ç”¨åŸSSDiffçš„q_sample_xtè¿›è¡ŒåŠ å™ªï¼ˆä¸åŸSSDiffå®Œå…¨ä¸€è‡´ï¼‰
            noise = torch.randn_like(gt_residual)
            x_t = self.diffusion.q_sample_xt(gt_residual, timesteps, noise=noise)
        else:
            # æ¨ç†æ—¶ï¼šè¾“å…¥ä¸º0ï¼ˆåˆå§‹å™ªå£°æ®‹å·®çš„è¿‘ä¼¼ï¼‰
            x_t = torch.zeros_like(lms)
        
        # UNeté¢„æµ‹æ®‹å·®ï¼ˆä½¿ç”¨forward_implæ–¹æ³•ï¼‰
        # forward_impl(self, lms, pan, ms, x_t, timesteps, scene_token, epoch)
        # å‚æ•°è¯´æ˜ï¼šlms(64x64), pan(64x64), ms(16x16ä¼šè¢«upsample), x_t(å™ªå£°æ®‹å·®, 64x64)
        # æ³¨æ„ï¼šx_tç°åœ¨æ˜¯å™ªå£°æ®‹å·®ï¼Œä¸åŸSSDiffçš„è¾“å…¥ä¸€è‡´
        # epochå‚æ•°ç”¨äºARConvåˆ¤æ–­æ˜¯å¦å›ºå®šå·ç§¯æ ¸
        residual_pred = self.unet.forward_impl(lms, pan, ms, x_t, timesteps, scene_token=scene_token, epoch=self.current_step)
        
        # æ£€æŸ¥ UNet è¾“å‡º
        if torch.isnan(residual_pred).any():
            print(f"[SSDiff_gen] residual_pred contains NaN after forward_impl!")
            print(f"  Input ranges - lms: [{lms.min():.4f}, {lms.max():.4f}], x_t: [{x_t.min():.4f}, {x_t.max():.4f}]")
            # å°† NaN æ›¿æ¢ä¸º 0
            residual_pred = torch.nan_to_num(residual_pred, nan=0.0)
        
        # å•æ­¥å»å™ªå¾—åˆ°æ®‹å·®
        if self.args.predict_xstart:
            # å¦‚æœæ¨¡å‹é¢„æµ‹x0ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆè¿™é‡Œx0å°±æ˜¯æ®‹å·®ï¼‰
            residual = residual_pred
        else:
            # ä»å™ªå£°é¢„æµ‹è®¡ç®—x0ï¼ˆæ®‹å·®ï¼‰
            residual = self.diffusion._predict_xstart_from_eps(x_t, timesteps, residual_pred)
        
        # æœ€ç»ˆè¾“å‡º = LMS + æ®‹å·®
        output = lms + residual
        
        # æ£€æŸ¥æœ€ç»ˆè¾“å‡º
        if torch.isnan(output).any():
            print(f"[SSDiff_gen] output contains NaN!")
            output = torch.nan_to_num(output, nan=0.0)
        
        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        output = output.clamp(0, 1)
        
        return output, residual_pred, scene_token
    
    def distribution_matching_loss(self, lms, pan, ms, gt):
        """
        ğŸ”¥ OSEDiffé£æ ¼çš„VSD (Variational Score Distillation) æŸå¤±
        ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨Scene Tokenå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§
        
        æ ¸å¿ƒæ€æƒ³ï¼šStudentå­¦ä¹ Teacheråœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„é¢„æµ‹åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦GT
        
        Args:
            lms: ä½åˆ†è¾¨ç‡å¤šå…‰è°±å›¾åƒ [B, 8, H, W]
            pan: å…¨è‰²å›¾åƒ [B, 1, H, W]
            ms: ä¸Šé‡‡æ ·çš„å¤šå…‰è°±å›¾åƒ [B, 8, H, W]
            gt: Ground truth [B, 8, H, W]
        
        Returns:
            loss_vsd: VSDæŸå¤±æ ‡é‡
            scene_token: Scene token features (å¦‚æœå¯ç”¨)
        """
        batch_size = lms.shape[0]
        device = lms.device
        
        # ğŸ”¥ æå–scene token (ä»PANæå–)
        scene_token = None
        if self.scene_extractor is not None:
            scene_token = self.scene_extractor(pan)
        
        # éšæœºtimestepï¼ˆä¸forwardä¸­è®­ç»ƒæ—¶çš„èŒƒå›´ä¸€è‡´ï¼‰
        timesteps = torch.randint(100, 999, (batch_size,), device=device, dtype=torch.long)
        
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨åŸSSDiffçš„q_sample_xtï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
        gt_residual = gt - lms  # è®¡ç®—çœŸæ®‹å·®
        noise = torch.randn_like(gt_residual)
        # ä½¿ç”¨åŸSSDiffçš„æ‰©æ•£è¿‡ç¨‹è¿›è¡ŒåŠ å™ª
        x_t = self.diffusion.q_sample_xt(gt_residual, timesteps, noise=noise)
        
        with torch.no_grad():
            # Teacheré¢„æµ‹ï¼ˆå†»ç»“ï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼‰- Teacherä¸ä½¿ç”¨scene tokenï¼Œä¹Ÿä¸éœ€è¦epoch
            residual_teacher = self.unet_teacher.forward_impl(lms, pan, ms, x_t, timesteps, scene_token=None, epoch=0)
            
            if self.args.predict_xstart:
                residual_teacher_final = residual_teacher
            else:
                residual_teacher_final = self.diffusion._predict_xstart_from_eps(x_t, timesteps, residual_teacher)
            output_teacher = (lms + residual_teacher_final).clamp(0, 1)
        
        # Studenté¢„æµ‹ï¼ˆéœ€è¦æ¢¯åº¦ï¼‰- Studentä½¿ç”¨scene tokenå’Œå½“å‰step
        residual_student = self.unet.forward_impl(lms, pan, ms, x_t, timesteps, scene_token=scene_token, epoch=self.current_step)
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å°Studenté…ç½®
        if not hasattr(self, '_debug_student_printed'):
            print(f"   - use_arconv: {self.unet.resblock0.use_arconv}")
            print(f"   - use_scene_token: {hasattr(self.unet, 'scene_embed') and self.unet.scene_embed is not None}")
            self._debug_student_printed = True
        if self.args.predict_xstart:
            residual_student_final = residual_student
        else:
            residual_student_final = self.diffusion._predict_xstart_from_eps(x_t, timesteps, residual_student)
        output_student = (lms + residual_student_final).clamp(0, 1)
        
        # VSDæŸå¤±è®¡ç®—ï¼ˆOSEDiffé£æ ¼çš„å®ç°ï¼‰
        # åŠ æƒå› å­ï¼šåŸºäºGTå’ŒTeacheré¢„æµ‹çš„å·®å¼‚
        weighting_factor = torch.abs(gt - output_teacher).mean(dim=[1, 2, 3], keepdim=True) + 1e-8
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å°å…³é”®å€¼
        if self.current_step % 100 == 0:
            student_teacher_diff = torch.abs(output_student - output_teacher).mean().item()
            gt_teacher_diff = torch.abs(gt - output_teacher).mean().item()
            print(f"   - |Student - Teacher|: {student_teacher_diff:.6f}")
            print(f"   - |GT - Teacher|: {gt_teacher_diff:.6f}")
            print(f"   - weighting_factor: {weighting_factor.mean().item():.6f}")
        
        # æ¢¯åº¦ï¼šStudentå’ŒTeacherçš„å·®å¼‚ï¼Œç»è¿‡weightingæ ‡å‡†åŒ–
        grad = (output_student - output_teacher) / weighting_factor
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å°gradå€¼
        if self.current_step % 100 == 0:
            grad_mean = torch.abs(grad).mean().item()
            grad_max = torch.abs(grad).max().item()
            print(f"   - grad_mean: {grad_mean:.6f}, grad_max: {grad_max:.6f}")
        
        # VSDæŸå¤±ï¼šè®©Studentæ¥è¿‘Teacherçš„åˆ†å¸ƒ
        # ä½¿ç”¨stop_gradientæŠ€å·§ï¼šgt - gradä½œä¸ºtargetï¼Œä½†gradä¸ä¼ æ¢¯åº¦
        loss_vsd = F.mse_loss(gt, (gt - grad).detach(), reduction="mean")
        
        return loss_vsd, scene_token
    
    def save_model(self, save_path):
        """ä¿å­˜æ¨¡å‹ï¼ˆLoRAæƒé‡ + ARConvæƒé‡ + Scene Extractoræƒé‡ + Scene Gateï¼‰"""
        state_dict = {
            'lora_target_modules': self.lora_target_modules,
            'lora_rank': self.lora_rank,
            'unet_state_dict': {},
            'scene_extractor_state_dict': None,
            'use_arconv': self.args.use_arconv if hasattr(self.args, 'use_arconv') else False,
            'use_scene_token': self.scene_extractor is not None,
        }
        
        # ä¿å­˜LoRAå‚æ•°ã€ARConvå‚æ•°å’ŒScene Gate
        saved_counts = {'lora': 0, 'arconv': 0, 'scene_gate': 0}
        
        for name, param in self.unet.named_parameters():
            # ä¿å­˜LoRAæƒé‡
            if 'lora' in name:
                state_dict['unet_state_dict'][name] = param.cpu()
                saved_counts['lora'] += 1
            # ğŸ”¥ ä¿å­˜Scene Gateï¼ˆåœºæ™¯tokençš„é—¨æ§å‚æ•°ï¼‰
            elif 'scene_gate' in name:
                state_dict['unet_state_dict'][name] = param.cpu()
                saved_counts['scene_gate'] += 1
            # ğŸ”¥ ä¿å­˜ARConvç›¸å…³æƒé‡ï¼ˆæ‰€æœ‰ä¸è‡ªé€‚åº”å·ç§¯ç›¸å…³çš„å‚æ•°ï¼‰
            elif any(key in name.lower() for key in ['arconv', 'adaptive', 'offset', 'modulation', 'p_conv', 'l_conv', 'w_conv', 'm_conv', 'b_conv', 'convs.']):
                state_dict['unet_state_dict'][name] = param.cpu()
                saved_counts['arconv'] += 1
        
        # ğŸ”¥ ä¿å­˜Scene Extractoræƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.scene_extractor is not None:
            state_dict['scene_extractor_state_dict'] = self.scene_extractor.state_dict()
        
        # æ‰“å°ä¿å­˜ç»Ÿè®¡
        print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜ç»Ÿè®¡:")
        print(f"   LoRAå‚æ•°: {saved_counts['lora']}")
        print(f"   ARConvå‚æ•°: {saved_counts['arconv']} {'âœ…' if saved_counts['arconv'] > 0 else 'âš ï¸ æœªæ‰¾åˆ°ARConvå‚æ•°ï¼'}")
        print(f"   Scene Gateå‚æ•°: {saved_counts['scene_gate']}")
        if self.scene_extractor is not None:
            print(f"   Scene Extractor: âœ… å·²ä¿å­˜")
        print(f"   æ€»å‚æ•°: {len(state_dict['unet_state_dict'])}")
        
        torch.save(state_dict, save_path)
        print(f"ğŸ’¾ Model saved to: {save_path}\n")


class SSDiff_reg(nn.Module):
    """
    æ­£åˆ™åŒ–æ¨¡å‹
    ç±»ä¼¼äºOSEDiff_regï¼ŒåŒ…å«å›ºå®šçš„UNetå’Œå¯æ›´æ–°çš„UNet
    ğŸ”¥ ä¼˜åŒ–ï¼šé›†æˆScene Token Extractor
    """
    def __init__(self, args, accelerator):
        super().__init__()
        self.args = args
        
        # ğŸ”¥ åˆå§‹åŒ–Scene Tokenæå–å™¨
        if hasattr(args, 'use_scene_token') and args.use_scene_token:
            token_dim = getattr(args, 'scene_token_dim', 256)
            print(f"ğŸ¨ [SSDiff_reg] Initializing Scene Token Extractor (dim={token_dim})...")
            self.scene_extractor = SceneTokenExtractor(
                input_channels=1,  # PANé€šé“æ•°
                token_dim=token_dim
            )
            self.scene_extractor.to(accelerator.device)
            print("âœ… [SSDiff_reg] Scene Token Extractor initialized")
        else:
            self.scene_extractor = None
        
        # åˆ›å»ºå›ºå®šçš„UNetï¼ˆä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œæ— ARConvï¼Œæ— Scene Tokenï¼‰
        fix_args_dict = args_to_dict(args, model_and_diffusion_defaults().keys())
        fix_args_dict['use_arconv'] = False  # ğŸ”¥ unet_fixä¸ä½¿ç”¨ARConvï¼ˆå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰ï¼‰
        fix_args_dict['use_scene_token'] = False  # ğŸ”¥ unet_fixä¸ä½¿ç”¨Scene Tokenï¼ˆå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰ï¼‰
        self.unet_fix, _ = create_model_and_diffusion(**fix_args_dict)
        if args.pretrained_ssdiff_path and os.path.exists(args.pretrained_ssdiff_path):
            state_dict = torch.load(args.pretrained_ssdiff_path, map_location='cpu')
            # ä½¿ç”¨strict=Falseï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰scene_embedå±‚
            missing_keys, _ = self.unet_fix.load_state_dict(state_dict, strict=False)
            if missing_keys:
                print(f"âš ï¸  unet_fixç¼ºå¤±çš„é”®ï¼ˆæ–°æ·»åŠ çš„å±‚ï¼‰: {len(missing_keys)} ä¸ª")
        self.unet_fix.requires_grad_(False)
        self.unet_fix.eval()
        
        # åˆ›å»ºå¯æ›´æ–°çš„UNetï¼ˆå¸¦LoRAï¼‰
        self.unet_update, self.lora_target_modules = initialize_ssdiff_unet_with_lora(
            args,
            pretrained_path=args.pretrained_ssdiff_path
        )
        
        # åˆ›å»ºdiffusionï¼ˆä½¿ç”¨é¡¶éƒ¨å·²å¯¼å…¥çš„å‡½æ•°ï¼‰
        _, self.diffusion = create_model_and_diffusion(
            **args_to_dict(args, model_and_diffusion_defaults().keys())
        )
        
        # æ‰“å°predict_xstartè®¾ç½®ï¼ˆæ­£åˆ™åŒ–è®­ç»ƒï¼‰
        print(f"ğŸ” [SSDiff_reg è®­ç»ƒ] predict_xstart = {args.predict_xstart}")
        
        # è®¾ç½®æƒé‡ç±»å‹
        weight_dtype = torch.float32
        if accelerator.mixed_precision == "fp16":
            weight_dtype = torch.float16
        elif accelerator.mixed_precision == "bf16":
            weight_dtype = torch.bfloat16
        self.weight_dtype = weight_dtype
    
    def set_train(self):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        self.unet_update.train()
        for n, p in self.unet_update.named_parameters():
            if "lora_down" in n or "lora_up" in n:
                p.requires_grad = True
            else:
                p.requires_grad = False
        
        # Scene Extractorå¯è®­ç»ƒ
        if self.scene_extractor is not None:
            self.scene_extractor.train()
            for p in self.scene_extractor.parameters():
                p.requires_grad = True
    
    def diff_loss(self, lms, pan, ms, gt):
        """
        æ‰©æ•£æŸå¤±ï¼šè®©LoRAå­¦ä¹ æ®‹å·®ï¼ˆä¸åŸå§‹SSDiffä¿æŒä¸€è‡´ï¼‰
        æ”¹è¿›æ–¹æ¡ˆï¼šä»MSå¼€å§‹é¢„æµ‹æ®‹å·®ï¼ˆGT - LMSï¼‰
        ä½¿ç”¨Scene Tokenæä¾›é¢å¤–çš„åœºæ™¯æ¡ä»¶ä¿¡æ¯
        """
        device = gt.device
        bsz = gt.shape[0]
        
        # æå–scene token
        scene_token = None
        if self.scene_extractor is not None:
            scene_token = self.scene_extractor(pan)
        
        # è®¡ç®—çœŸå®æ®‹å·®
        gt_residual = gt - lms
        
        # å›ºå®šåœ¨timestep=999ï¼ˆä¸ä¸»æ¨¡å‹ä¸€è‡´ï¼‰
        timesteps = torch.full((bsz,), 999, device=device, dtype=torch.long)
        
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨åŸSSDiffçš„q_sample_xtï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
        noise = torch.randn_like(gt_residual)
        # ä½¿ç”¨åŸSSDiffçš„æ‰©æ•£è¿‡ç¨‹è¿›è¡ŒåŠ å™ª
        x_t = self.diffusion.q_sample_xt(gt_residual, timesteps, noise=noise)
        
        # é¢„æµ‹æ®‹å·®ï¼ˆä½¿ç”¨forward_implæ–¹æ³•ï¼ŒåŠ å…¥scene_tokenï¼‰
        # å‚æ•°ï¼šlms(64x64), pan(64x64), ms(16x16ä¼šè¢«upsample), x_t(64x64), scene_token
        residual_pred = self.unet_update.forward_impl(lms, pan, ms, x_t, timesteps, scene_token=scene_token)
        
        # è®¡ç®—æŸå¤±ï¼šé¢„æµ‹æ®‹å·®ä¸çœŸå®æ®‹å·®çš„å·®å¼‚ï¼ˆä½¿ç”¨L1ï¼Œä¸åŸå§‹SSDiffä¸€è‡´ï¼‰
        if self.args.predict_xstart:
            # å¦‚æœé¢„æµ‹x0ï¼Œè¿™é‡Œx0å°±æ˜¯æ®‹å·®
            loss = F.l1_loss(residual_pred.float(), gt_residual.float(), reduction="mean")
        else:
            # å¦‚æœé¢„æµ‹å™ªå£°ï¼Œéœ€è¦è½¬æ¢ä¸ºx0ï¼ˆæ®‹å·®ï¼‰å†æ¯”è¾ƒ
            residual_pred_x0 = self.diffusion._predict_xstart_from_eps(x_t, timesteps, residual_pred)
            loss = F.l1_loss(residual_pred_x0.float(), gt_residual.float(), reduction="mean")
        
        return loss
    
    def distribution_matching_loss(self, lms, pan, ms, x_pred):
        """
        åˆ†å¸ƒåŒ¹é…æŸå¤±ï¼šè®©å•æ­¥æ¨¡å‹çš„è¾“å‡ºæ¥è¿‘å¤šæ­¥SSDiffçš„åˆ†å¸ƒ
        ä½¿ç”¨Scene Tokenæä¾›é¢å¤–çš„åœºæ™¯æ¡ä»¶ä¿¡æ¯
        
        Args:
            lms, pan, ms: æ¡ä»¶è¾“å…¥
            x_pred: å­¦ç”Ÿæ¨¡å‹ï¼ˆå•æ­¥ï¼‰çš„é¢„æµ‹
        
        Returns:
            loss: åˆ†å¸ƒåŒ¹é…æŸå¤±
        """
        device = x_pred.device
        bsz = x_pred.shape[0]
        
        # æå–scene token
        scene_token = None
        if self.scene_extractor is not None:
            scene_token = self.scene_extractor(pan)
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®
        if torch.isnan(lms).any():
            print(f"[distribution_matching_loss] lms contains NaN!")
        if torch.isnan(pan).any():
            print(f"[distribution_matching_loss] pan contains NaN!")
        if torch.isnan(ms).any():
            print(f"[distribution_matching_loss] ms contains NaN!")
        if torch.isnan(x_pred).any():
            print(f"[distribution_matching_loss] x_pred contains NaN!")
        
        # éšæœºé‡‡æ ·ä¸­é—´æ—¶é—´æ­¥
        timesteps = torch.randint(20, 980, (bsz,), device=device).long()
        
        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¯¹æ®‹å·®æ·»åŠ å™ªå£°ï¼ˆä¸åŸSSDiffä¸€è‡´ï¼‰
        x_pred_residual = x_pred - lms  # é¢„æµ‹çš„æ®‹å·®
        noise = torch.randn_like(x_pred_residual)
        # ä½¿ç”¨diffusionçš„q_sample_xtå¯¹æ®‹å·®æ·»åŠ å™ªå£°
        noisy_x = self.diffusion.q_sample_xt(x_pred_residual, timesteps, noise=noise)
        
        if torch.isnan(noisy_x).any():
            print(f"[distribution_matching_loss] noisy_x contains NaN after q_sample_xt!")
        
        with torch.no_grad():
            # å­¦ç”Ÿæ¨¡å‹ï¼ˆå¯æ›´æ–°ï¼‰çš„é¢„æµ‹ï¼ˆä½¿ç”¨forward_impl + scene_tokenï¼‰
            # å‚æ•°ï¼šlms(64x64), pan(64x64), ms(16x16ä¼šè¢«upsample), noisy_x(64x64), scene_token
            noise_pred_update = self.unet_update.forward_impl(lms, pan, ms, noisy_x, timesteps, scene_token=scene_token)
            x0_pred_update = self.diffusion._predict_xstart_from_eps(
                noisy_x, timesteps, noise_pred_update
            )
            
            # æ•™å¸ˆæ¨¡å‹ï¼ˆå›ºå®šï¼‰çš„é¢„æµ‹ï¼ˆä½¿ç”¨forward_implï¼Œä¸ä½¿ç”¨scene_tokenï¼‰
            # å‚æ•°ï¼šlms(64x64), pan(64x64), ms(16x16ä¼šè¢«upsample), noisy_x(64x64)
            noise_pred_fix = self.unet_fix.forward_impl(
                lms.to(self.weight_dtype), 
                pan.to(self.weight_dtype),
                ms.to(self.weight_dtype),
                noisy_x.to(self.weight_dtype), 
                timesteps,
                scene_token=None
            )
            x0_pred_fix = self.diffusion._predict_xstart_from_eps(
                noisy_x, timesteps, noise_pred_fix.float()
            )
        
        weighting_factor = torch.abs(x_pred - x0_pred_fix).mean(
            dim=[1, 2, 3], keepdim=True
        ) + 1e-5
        # è®¡ç®—æ¢¯åº¦
        grad = (x0_pred_update - x0_pred_fix) / weighting_factor
        
        # VSDæŸå¤±
        loss = F.mse_loss(x_pred, (x_pred - grad).detach())
        
        return loss


class SSDiff_test(nn.Module):
    """
    æµ‹è¯•/æ¨ç†æ¨¡å‹
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. åŸå§‹SSDiffå¤šæ­¥é‡‡æ ·ï¼ˆuse_distillation=Falseï¼‰
    2. è’¸é¦åçš„å•æ­¥é‡‡æ ·ï¼ˆuse_distillation=Trueï¼‰
    ğŸ”¥ ä¼˜åŒ–ï¼šæ”¯æŒScene Token conditioningï¼ˆå¯é€‰ï¼‰
    """
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ğŸ”¥ åˆå§‹åŒ–Scene Token Extractorï¼ˆå¯é€‰ï¼‰
        if hasattr(args, 'use_scene_token') and args.use_scene_token:
            token_dim = getattr(args, 'scene_token_dim', 256)
            print(f"ğŸ¨ [SSDiff_test] Initializing Scene Token Extractor (dim={token_dim})...")
            self.scene_extractor = SceneTokenExtractor(
                input_channels=1,  # PANé€šé“æ•°
                token_dim=token_dim
            )
            self.scene_extractor.to(self.device)
            self.scene_extractor.eval()
            print("âœ… [SSDiff_test] Scene Token Extractor initialized")
        else:
            self.scene_extractor = None
        
        # å¯¹äºè’¸é¦æ¨¡å¼ï¼Œä¸ä½¿ç”¨SpacedDiffusionï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´çš„1000æ­¥ç©ºé—´
        if args.use_distillation:
            # ä¸´æ—¶ç§»é™¤ timestep_respacingï¼Œä½¿ç”¨å®Œæ•´çš„ Gaussian Diffusion
            original_respacing = args.timestep_respacing
            args.timestep_respacing = ""  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸ä½¿ç”¨spacing
            print(f"ğŸ“Š Using full 1000-step space for distillation (è®­ç»ƒæ—¶ä½¿ç”¨timestep=999)")
        
        # åˆ›å»ºæ¨¡å‹å’Œdiffusion
        self.model, self.diffusion = create_model_and_diffusion(
            **args_to_dict(args, model_and_diffusion_defaults().keys())
        )
        
        # æ¢å¤åŸå§‹è®¾ç½®
        if args.use_distillation:
            args.timestep_respacing = original_respacing
            
        # æ‰“å°predict_xstartè®¾ç½®ï¼ˆæµ‹è¯•æ—¶ï¼‰
        print(f"ğŸ” [SSDiff_test æµ‹è¯•] predict_xstart = {args.predict_xstart}")
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if hasattr(args, 'model_path') and args.model_path:
            print(f"Loading model from: {args.model_path}")
            state_dict = torch.load(args.model_path, map_location='cpu')
            
            if args.use_distillation and 'unet_state_dict' in state_dict:
                # è’¸é¦æ¨¡å‹ï¼šå…ˆåŠ è½½åŸºç¡€æƒé‡ï¼Œå†åŠ è½½LoRA
                if hasattr(args, 'pretrained_ssdiff_path'):
                    base_state = torch.load(args.pretrained_ssdiff_path, map_location='cpu')
                    # ä½¿ç”¨strict=Falseï¼Œå› ä¸ºæˆ‘ä»¬æ·»åŠ äº†scene_embedå±‚
                    missing_keys, _ = self.model.load_state_dict(base_state, strict=False)
                    if missing_keys and len(missing_keys) > 0:
                        print(f"âš ï¸  ç¼ºå¤± {len(missing_keys)} ä¸ªé”®ï¼ˆscene_embedç­‰æ–°å±‚ï¼‰")
                    print("âœ… Loaded base SSDiff weights")
                
                # æ·»åŠ LoRAå±‚
                if 'lora_target_modules' in state_dict:
                    # ä»checkpointä¸­è¯»å–lora_rank
                    if 'lora_rank' in state_dict:
                        args.lora_rank = state_dict['lora_rank']
                        print(f"ğŸ“Š Using lora_rank={args.lora_rank} from checkpoint")
                    else:
                        # é»˜è®¤å€¼
                        args.lora_rank = 4
                        print(f"âš ï¸  lora_rank not found in checkpoint, using default: {args.lora_rank}")
                    
                    # æ·»åŠ LoRAå±‚å¹¶åŠ è½½æƒé‡ï¼ˆåœ¨åŸºç¡€SSDiffæƒé‡ä¹‹ä¸Šï¼‰
                    self.model, _ = initialize_ssdiff_unet_with_lora(
                        args, pretrained_path=args.pretrained_ssdiff_path
                    )
                    
                    # åŠ è½½LoRAæƒé‡ã€ARConvæƒé‡å’ŒScene Gate
                    loaded_counts = {'lora': 0, 'arconv': 0, 'scene_gate': 0}
                    
                    for name, param in self.model.named_parameters():
                        if name in state_dict['unet_state_dict']:
                            param.data.copy_(state_dict['unet_state_dict'][name])
                            if 'lora' in name:
                                loaded_counts['lora'] += 1
                            elif 'scene_gate' in name:
                                loaded_counts['scene_gate'] += 1
                            elif any(key in name.lower() for key in ['arconv', 'adaptive', 'offset', 'modulation', 'p_conv', 'l_conv', 'w_conv', 'm_conv', 'b_conv', 'convs.']):
                                loaded_counts['arconv'] += 1
                    
                    print(f"\nâœ… æ¨¡å‹åŠ è½½ç»Ÿè®¡:")
                    print(f"   LoRAå‚æ•°: {loaded_counts['lora']}")
                    print(f"   ARConvå‚æ•°: {loaded_counts['arconv']} {'âœ…' if loaded_counts['arconv'] > 0 else 'âš ï¸ æœªæ‰¾åˆ°ï¼'}")
                    print(f"   Scene Gateå‚æ•°: {loaded_counts['scene_gate']}")
                    
                    if args.use_arconv and loaded_counts['arconv'] == 0:
                        print(f"âš ï¸  è­¦å‘Šï¼šæ¨¡å‹ä½¿ç”¨ARConvä½†checkpointä¸­æ²¡æœ‰ARConvæƒé‡")
                        print(f"    å°†ä½¿ç”¨æ’å€¼åˆå§‹åŒ–ï¼ˆä»Conv2dæƒé‡æ‰©å±•ï¼‰")
                
                # ğŸ”¥ åŠ è½½Scene Extractoræƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'scene_extractor_state_dict' in state_dict and state_dict['scene_extractor_state_dict'] is not None:
                    if self.scene_extractor is not None:
                        self.scene_extractor.load_state_dict(state_dict['scene_extractor_state_dict'])
                        print("âœ… Loaded Scene Extractor weights")
                    else:
                        print("âš ï¸  Checkpoint has Scene Extractor weights but model doesn't use scene_token")
                elif self.scene_extractor is not None:
                    print("âš ï¸  Model uses scene_token but checkpoint doesn't have Scene Extractor weights (will use random init)")
            else:
                # åŸå§‹SSDiffæ¨¡å‹
                # ä½¿ç”¨strict=Falseï¼Œå› ä¸ºæˆ‘ä»¬æ·»åŠ äº†scene_embedå±‚
                missing_keys, _ = self.model.load_state_dict(state_dict, strict=False)
                if missing_keys and len(missing_keys) > 0:
                    print(f"âš ï¸  ç¼ºå¤± {len(missing_keys)} ä¸ªé”®ï¼ˆscene_embedç­‰æ–°å±‚ï¼‰")
                print("âœ… Loaded original SSDiff weights")
        
        # ğŸ”¥ ç®€åŒ–æ¨¡å‹åŠ è½½ï¼Œé¿å…å¤æ‚çš„æƒé‡ç±»å‹è®¾ç½®
        self.model.to(self.device)
        
        # è®¾ç½® epoch > 1000ï¼Œç¡®ä¿ ARConv ä½¿ç”¨è®­ç»ƒå¥½çš„å›ºå®šå·ç§¯æ ¸ï¼ˆreserved_NXYï¼‰
        self.model.set_epoch(10001)
        print("âœ… Set epoch to 10001 for using fixed ARConv kernel size")
        
        self.model.eval()
    
    @torch.no_grad()
    def forward(self, lms, pan, ms):
        """
        æ¨ç†å‰å‘ä¼ æ’­
        
        Args:
            lms: ä½åˆ†è¾¨ç‡å¤šå…‰è°± [B, 8, H, W]
            pan: å…¨è‰²å›¾åƒ [B, 1, H, W]  
            ms: ä¸Šé‡‡æ ·å¤šå…‰è°± [B, 8, H, W]
        
        Returns:
            output: é”åŒ–åçš„å¤šå…‰è°±å›¾åƒ [B, 8, H, W]
        """
        # ğŸ”¥ ç¡®ä¿æ¨ç†æ¨¡å¼
        self.model.training = False
        self.model.eval()
        
        # æ•°æ®ç±»å‹è½¬æ¢ï¼ˆå»æ‰æ··åˆç²¾åº¦ï¼‰
        lms = lms.to(self.device)
        pan = pan.to(self.device)
        ms = ms.to(self.device)
        
        model_kwargs = {"lms": lms, "pan": pan, "ms": ms}
        
        if self.args.use_distillation:
            # å•æ­¥è’¸é¦æ¨¡å¼ - ç›´æ¥è°ƒç”¨forward_implï¼Œä¸ä½¿ç”¨é‡‡æ ·è¿‡ç¨‹
            # ä¸è®­ç»ƒæ—¶çš„é€»è¾‘å®Œå…¨ä¸€è‡´ï¼šç›´æ¥å‰å‘ä¼ æ’­ï¼Œæ— éœ€ddim_sample
            
            # ä½¿ç”¨forward_chopå¤„ç†å¤§å›¾åƒ
            # éœ€è¦å®šä¹‰ä¸€ä¸ªç®€å•çš„åŒ…è£…å‡½æ•°ï¼Œä¸ä½¿ç”¨é‡‡æ ·
            
            # ç”¨äºè®°å½•ç¬¬ä¸€ä¸ªpatchçš„è°ƒè¯•ä¿¡æ¯
            first_patch = [True]
            
            def direct_forward_fn(model_output, t, lms_input, **kwargs):
                # è®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨forward_implçš„è¾“å‡ºä½œä¸ºæ®‹å·®
                # model_outputæ˜¯forward_implçš„è¾“å‡ºï¼ˆæ®‹å·®é¢„æµ‹ï¼‰
                # lms_inputæ˜¯åˆ‡åˆ†åçš„patchï¼ˆ64x64ï¼‰
                
                # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
               
                # å¦‚æœpredict_xstart=Trueï¼Œmodel_outputå°±æ˜¯é¢„æµ‹çš„x0ï¼ˆæ®‹å·®ï¼‰
                # å¦åˆ™éœ€è¦ä»epsè½¬æ¢ä¸ºx0
                if self.args.predict_xstart:
                    pred_xstart = model_output
                else:
                    # ä»kwargsç¨³å¥è·å–å½“å‰patchå¯¹åº”çš„x_tï¼›è‹¥æ— åˆ™ç”¨0
                    x_t = kwargs.get('noise', None)
                    if x_t is None:
                        x_t = torch.zeros_like(lms_input)
                    
                    # ç¡®ä¿x_tä¸lms_inputå½¢çŠ¶åŒ¹é…
                    if x_t.shape != lms_input.shape:
                        print(f"âš ï¸ å½¢çŠ¶ä¸åŒ¹é…! x_t: {x_t.shape}, lms: {lms_input.shape}")
                        # é‡æ–°åˆ›å»ºä¸å½“å‰patchåŒ¹é…çš„x_t
                        x_t = torch.zeros_like(lms_input)
                    
                    # æ„é€ ä¸å½“å‰patchæ‰¹å¤§å°åŒ¹é…çš„t_batchï¼ˆä¼˜å…ˆä½¿ç”¨å›è°ƒä¼ å…¥çš„tï¼‰
                    if isinstance(t, torch.Tensor):
                        if t.dim() == 0:
                            t_batch = t.to(device=lms_input.device, dtype=torch.long).expand(lms_input.shape[0])
                        else:
                            t_batch = t.to(device=lms_input.device, dtype=torch.long)
                    else:
                        t_batch = torch.full((lms_input.shape[0],), 999, device=lms_input.device, dtype=torch.long)
                    pred_xstart = self.diffusion._predict_xstart_from_eps(x_t, t_batch, model_output)
                
                # æœ€ç»ˆè¾“å‡º = lms_patch + æ®‹å·®
                output = lms_input + pred_xstart
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                print(f"   LMS patch: [{lms_input.min():.4f}, {lms_input.max():.4f}], mean={lms_input.mean():.4f}")
                print(f"   Model output (residual): [{model_output.min():.4f}, {model_output.max():.4f}], mean={model_output.mean():.4f}")
                print(f"   Predicted residual: [{pred_xstart.min():.4f}, {pred_xstart.max():.4f}], mean={pred_xstart.mean():.4f}")
                print(f"   Output before clamp: [{output.min():.4f}, {output.max():.4f}], mean={output.mean():.4f}")
                
                return {"sample": output, "pred_xstart": pred_xstart}
            
            # ä¸è®­ç»ƒ/æ¨ç†é€»è¾‘ä¿æŒä¸€è‡´ï¼šå•æ­¥è’¸é¦æ¨ç†æ—¶ä½¿ç”¨ x_t = 0
            xt = torch.zeros_like(lms)
            
            # å†æ¬¡ç¡®è®¤æµ‹è¯•æ—¶predict_xstartè®¾ç½®
            print(f"ğŸ” [SSDiff_test å•æ­¥è’¸é¦æ¨ç†] predict_xstart = {self.args.predict_xstart}")
            
            # å¯é€‰ï¼šæ·»åŠ å¯¹æ¯”å®éªŒ - ä½¿ç”¨å¸¦å™ªå£°çš„x_tï¼ˆä¸è®­ç»ƒæ›´æ¥è¿‘ï¼‰
            if hasattr(self.args, 'test_with_noise') and self.args.test_with_noise:
                print("ğŸ”¬ ä½¿ç”¨å¸¦å™ªå£°çš„x_tè¿›è¡Œæµ‹è¯•ï¼ˆæ›´æ¥è¿‘è®­ç»ƒåˆ†å¸ƒï¼‰")
                # å¯¹æ•´å¼ å›¾ç”Ÿæˆä¸€è‡´çš„å™ªå£°ï¼Œé¿å…patchè¾¹ç•Œé—®é¢˜
                noise = torch.randn_like(lms)
                # æ—¶é—´æ­¥å›ºå®št=999
                t_full = torch.full((lms.shape[0],), 99, device=self.device, dtype=torch.long)
                # å¯¹é›¶æ®‹å·®åŠ å™ª
                xt = self.diffusion.q_sample_xt(torch.zeros_like(lms), t_full, noise=noise)
                print(f"   å¸¦å™ªx_tèŒƒå›´: [{xt.min():.4f}, {xt.max():.4f}], å‡å€¼={xt.mean():.4f}, æ ‡å‡†å·®={xt.std():.4f}")
                        
            # ğŸ”¥ æå–scene tokenï¼ˆå¦‚æœå¯ç”¨ï¼‰
            scene_token = None
            if self.scene_extractor is not None:
                with torch.no_grad():
                    scene_token = self.scene_extractor(pan)
                    print(f"âœ… Scene tokenæå–å®Œæˆ: {scene_token.shape}")
            
            # ğŸ”¥ ç»Ÿä¸€ä½¿ç”¨ forward_chop å¤„ç†ï¼ˆæ”¯æŒå¤§å›¾åƒpatchåˆ‡åˆ†ï¼‰
            batch_size = lms.shape[0]
            timesteps = torch.full((batch_size,), 99, device=self.device, dtype=torch.long)
            
            print(f"ä½¿ç”¨è‡ªå®šä¹‰çš„ forward_chop_distill è¿›è¡Œå•æ­¥è’¸é¦æ¨ç†")
            
            # ğŸ”¥ ä½¿ç”¨æˆ‘ä»¬åœ¨ SSNet.py ä¸­æ–°æ·»åŠ çš„ forward_chop_distill æ–¹æ³•
            # è¿™ä¸ªæ–¹æ³•ç›´æ¥å¤„ç†è¾“å…¥ï¼Œä¸éœ€è¦ç»è¿‡å¤æ‚çš„ module.py é€»è¾‘
            output = self.model.forward_chop_distill(
                lms, pan, ms, xt,
                sample_fn=direct_forward_fn,
                scene_token=scene_token,
                noise=xt,
            )
        else:
            # åŸå§‹å¤šæ­¥é‡‡æ ·æ¨¡å¼
            sample_fn = (
                self.diffusion.p_sample_loop 
                if not self.args.use_ddim 
                else self.diffusion.ddim_sample_loop
            )
            
            output = sample_fn(
                self.model,
                shape=ms.shape,
                model_kwargs=model_kwargs,
                clip_denoised=self.args.clip_denoised,
                progress=False
            )
        
        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        output = output.clamp(0, 1)
        
        return output